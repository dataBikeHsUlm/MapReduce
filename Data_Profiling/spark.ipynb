{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://vpn-082-252.hs-ulm.de:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1554832590601)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+---------+-----------------+---------+------+-----------+-------+-----+----------+----+------------+-------------+\n",
      "|StartStation Id|EndStation Id|Frequency|    Daily Weather|  Weekday|Season|Day & Night|Holiday|Month|      Date|Year|Passed_Years|Passed_Months|\n",
      "+---------------+-------------+---------+-----------------+---------+------+-----------+-------+-----+----------+----+------------+-------------+\n",
      "|            836|          749|        1|partly-cloudy-day| Thursday|Autumn|      night|  False|    9|2018-09-06|2018|           1|            7|\n",
      "|            831|          638|        5|        clear-day|  Tuesday|Summer|      night|  False|    7|2018-07-24|2018|           1|            9|\n",
      "|            831|          264|        1|partly-cloudy-day|Wednesday|Summer|        day|  False|    6|2018-06-06|2018|           1|           10|\n",
      "|            831|          372|        2|        clear-day|Wednesday|Spring|        day|  False|    4|2018-04-18|2018|           1|           12|\n",
      "|            831|           41|        1|partly-cloudy-day|Wednesday|Summer|        day|  False|    8|2018-08-15|2018|           1|            8|\n",
      "+---------------+-------------+---------+-----------------+---------+------+-----------+-------+-----+----------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.sql.functions._\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@707e4ccd\n",
       "df: org.apache.spark.sql.DataFrame = [_c0: string, StartStation Id: string ... 12 more fields]\n",
       "dfs: org.apache.spark.sql.DataFrame = [StartStation Id: string, EndStation Id: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "import org.apache.spark.sql.functions._ \n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"raw_files/features.csv\")\n",
    "val dfs = df.drop(df.col(\"_c0\"))\n",
    "dfs.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [StartStation Id: string, EndStation Id: string ... 11 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [StartStation Id: string, EndStation Id: string ... 11 more fields]\n",
       "training_df: org.apache.spark.sql.DataFrame = [label: double, Daily Weather: string ... 1 more field]\n",
       "train_set: org.apache.spark.sql.DataFrame = [label: double, Daily Weather: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val Array(training, test) = dfs.randomSplit(Array[Double](0.7, 0.3), 18)\n",
    "\n",
    "val training_df = (training.select(training(\"Frequency\").as(\"label\"),$\"Daily Weather\",$\"Day & Night\" )).withColumn(\"label\", 'label cast DoubleType)\n",
    "val train_set = training_df.na.drop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------+\n",
      "|label|    Daily Weather|Day & Night|\n",
      "+-----+-----------------+-----------+\n",
      "|  3.0|partly-cloudy-day|      night|\n",
      "| 19.0|partly-cloudy-day|      night|\n",
      "|  8.0|partly-cloudy-day|        day|\n",
      "|  3.0|partly-cloudy-day|      night|\n",
      "|228.0|partly-cloudy-day|        day|\n",
      "+-----+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_set.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{OneHotEncoder, VectorIndexer, VectorAssembler, StringIndexer}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "DayPerIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_20772f3ff977\n",
       "DayWthIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_228bb5e0ae81\n",
       "LabelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_0f6008498d7a\n",
       "DayPerEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_9cda62599b4b\n",
       "DayWthEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_af755f2e23e6\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f18b7c236ab8\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, VectorIndexer, VectorAssembler, StringIndexer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// converting strings into numerical values\n",
    "//val FreqIndexer = new StringIndexer().setInputCol(\"Frequency\").setOutputCol(\"Freq\")\n",
    "\n",
    "val DayPerIndexer = new StringIndexer().setInputCol(\"Day & Night\").setOutputCol(\"DayPer\")\n",
    "val DayWthIndexer = new StringIndexer().setInputCol(\"Daily Weather\").setOutputCol(\"DayWth\")\n",
    "\n",
    "val LabelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"DayWth\")\n",
    "\n",
    "//val indexed = indexer.transform(df)\n",
    "\n",
    "// converting numerical values into one hot encoding (0 or 1)\n",
    "//val FreqEncoder = new OneHotEncoder().setInputCol(\"Freq\").setOutputCol(\"FreqVec\")\n",
    "val DayPerEncoder = new OneHotEncoder().setInputCol(\"DayPer\").setOutputCol(\"DayPerVec\")\n",
    "val DayWthEncoder = new OneHotEncoder().setInputCol(\"DayWth\").setOutputCol(\"DayWthVec\")\n",
    "\n",
    "// (Label, features)\n",
    "var assembler= (new VectorAssembler()\n",
    "                .setInputCols(Array( \"DayWthVec\", \"DayPerVec\")).setOutputCol(\"features\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "//val encoded = encoder.transform(indexed)\n",
    "//encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_1aa307bf02f8\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_de511f9d4751\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array( DayPerIndexer,DayPerEncoder, DayWthIndexer, DayWthEncoder, assembler))\n",
    "//val pipeline = new Pipeline().setStages(Array( DayPerIndexer,DayPerEncoder, DayWthIndexer, DayWthEncoder, assembler, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+-----------+------+-------------+------+-------------+-------------------+\n",
      "|label|    Daily Weather|Day & Night|DayPer|    DayPerVec|DayWth|    DayWthVec|           features|\n",
      "+-----+-----------------+-----------+------+-------------+------+-------------+-------------------+\n",
      "|  3.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "| 19.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "|  8.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "|  3.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "|228.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "|  1.0|        clear-day|        day|   0.0|(1,[0],[1.0])|   1.0|(6,[1],[1.0])|(7,[1,6],[1.0,1.0])|\n",
      "| 11.0|        clear-day|        day|   0.0|(1,[0],[1.0])|   1.0|(6,[1],[1.0])|(7,[1,6],[1.0,1.0])|\n",
      "| 47.0|             rain|        day|   0.0|(1,[0],[1.0])|   2.0|(6,[2],[1.0])|(7,[2,6],[1.0,1.0])|\n",
      "|  3.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "|227.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "|190.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "| 38.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "|  5.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "| 80.0|        clear-day|        day|   0.0|(1,[0],[1.0])|   1.0|(6,[1],[1.0])|(7,[1,6],[1.0,1.0])|\n",
      "|155.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "| 52.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "|  2.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "|  1.0|partly-cloudy-day|      night|   1.0|    (1,[],[])|   0.0|(6,[0],[1.0])|      (7,[0],[1.0])|\n",
      "|126.0|             rain|        day|   0.0|(1,[0],[1.0])|   2.0|(6,[2],[1.0])|(7,[2,6],[1.0,1.0])|\n",
      "|  3.0|partly-cloudy-day|        day|   0.0|(1,[0],[1.0])|   0.0|(6,[0],[1.0])|(7,[0,6],[1.0,1.0])|\n",
      "+-----+-----------------+-----------+------+-------------+------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipe: org.apache.spark.sql.DataFrame = [label: double, Daily Weather: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val model = pipeline.fit(train_set)\n",
    "var pipe=pipeline.fit(train_set).transform(train_set)\n",
    "pipe.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//val ss = (training.select(training(\"Frequency\").as(\"label\"),$\"Daily Weather\",$\"Day & Night\" )).withColumn(\"label\", 'label cast DoubleType)\n",
    "\n",
    "//val model = pipeline.fit(train_set)\n",
    "//val results= model.transform(test)\n",
    "//var s= pipe.drop(pipe.col(\"Day & Night\",\"Daily Weather\"))\n",
    "//s= s.drop(s.col(\"Daily Weather\"))\n",
    "//s= s.drop(s.col(\"DayPer\"))\n",
    "//s= s.drop(s.col(\"DayWth\"))\n",
    "\n",
    "//s.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val lrModel = lr.fit(pipe)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
