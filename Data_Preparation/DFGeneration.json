{"paragraphs":[{"text":"%md\n###DataFrame Generation###\nProblem: resolve the data on hourly level\nIdea: use a sliding window and move the window according to the wished length of hours\n- Get rid of the json format in the weather hourly data\n- Prepare and add hourly usage of a given station\n- Merge weather and usage to one DF\n- Resolve `null` values\n","user":"admin","dateUpdated":"2019-06-08T03:29:40+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>DataFrame Generation</h3>\n<p>Problem: resolve the data on hourly level\n<br  />Idea: use a sliding window and move the window according to the wished length of hours</p>\n<ul>\n<li>Get rid of the json format in the weather hourly data</li>\n<li>Prepare and add hourly usage of a given station</li>\n<li>Merge weather and usage to one DF</li>\n<li>Resolve <code>null</code> values</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1558311442059_-537161341","id":"20190520-021722_1715877619","dateCreated":"2019-05-20T02:17:22+0200","dateStarted":"2019-06-08T03:29:40+0200","dateFinished":"2019-06-08T03:29:40+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:168"},{"text":"%sh\nd_name=/user/hadoop/\nf_name=\"test\"\nhdfs dfs -count $d_name\na=0\nfor i in $(hdfs dfs -ls $d_name)\ndo\n let \"a += 1\"\n #echo $a\n echo \"Räume auf...\"\n hdfs dfs -rm -r $d_name$f_name$a\ndone\necho $d_name\n","user":"admin","dateUpdated":"2019-06-08T03:29:40+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"          28          245        14366764783 /user/hadoop\nRäume auf...\nrm: `/user/hadoop/test1': No such file or directory\nRäume auf...\nrm: `/user/hadoop/test2': No such file or directory\nRäume auf...\nrm: `/user/hadoop/test3': No such file or directory\nRäume auf...\nrm: `/user/hadoop/test4': No such file or directory\nRäume auf...\nrm: `/user/hadoop/test5': No such file or directory\nRäume auf...\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1558341692370_-1255032650","id":"20190520-104132_920406303","dateCreated":"2019-05-20T10:41:32+0200","dateStarted":"2019-06-08T03:29:41+0200","dateFinished":"2019-06-08T03:30:14+0200","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:169"},{"text":"%sh\n\n# Remove old dataset if already exists in hadoop /user/hadoop directory\nd_name=/user/hadoop/usage-stats/\n\nhdfs dfs -test -d $d_name\n    if [ $? != 0 ]\n            then\n                echo \"Move folder to HDFS...\"\n                hdfs dfs -put /home/hadoop/TFL_Cycling_Data_raw/usage-stats /user/hadoop/usage-stats\n                echo \"Folder successfully uploaded to HDFS\"\n                hdfs dfs -ls $d_name\n            else\n                echo \"Directory already present in HDFS\"\n                echo \"Move folder to trash\"\n                hdfs dfs -rm -r -skipTrash $d_name\n                echo \"$d_name\" \"has been deleted\"\n    fi","user":"admin","dateUpdated":"2019-05-20T10:29:50+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Move folder to HDFS...\nFolder successfully uploaded to HDFS\nFound 188 items\n-rw-r--r--   3 zeppelin hdfs   34451153 2019-05-20 10:29 /user/hadoop/usage-stats/01aJourneyDataExtract10Jan16-23Jan16.csv\n-rw-r--r--   3 zeppelin hdfs   35486081 2019-05-20 10:29 /user/hadoop/usage-stats/01b Journey Data Extract 24Jan16-06Feb16.csv\n-rw-r--r--   3 zeppelin hdfs   35486081 2019-05-20 10:29 /user/hadoop/usage-stats/01bJourneyDataExtract24Jan16-06Feb16.csv\n-rw-r--r--   3 zeppelin hdfs   33287674 2019-05-20 10:29 /user/hadoop/usage-stats/02aJourneyDataExtract07Fe16-20Feb2016.csv\n-rw-r--r--   3 zeppelin hdfs   34882871 2019-05-20 10:29 /user/hadoop/usage-stats/02bJourneyDataExtract21Feb16-05Mar2016.csv\n-rw-r--r--   3 zeppelin hdfs   67717329 2019-05-20 10:29 /user/hadoop/usage-stats/03JourneyDataExtract06Mar2016-31Mar2016.csv\n-rw-r--r--   3 zeppelin hdfs   94572397 2019-05-20 10:29 /user/hadoop/usage-stats/04JourneyDataExtract01Apr2016-30Apr2016.csv\n-rw-r--r--   3 zeppelin hdfs   71655072 2019-05-20 10:30 /user/hadoop/usage-stats/05JourneyDataExtract01May2016-17May2016.csv\n-rw-r--r--   3 zeppelin hdfs   27903078 2019-05-20 10:30 /user/hadoop/usage-stats/06JourneyDataExtract18May2016-24May2016.csv\n-rw-r--r--   3 zeppelin hdfs   25920903 2019-05-20 10:30 /user/hadoop/usage-stats/07JourneyDataExtract25May2016-31May2016.csv\n-rw-r--r--   3 zeppelin hdfs   28771414 2019-05-20 10:30 /user/hadoop/usage-stats/08JourneyDataExtract01Jun2016-07Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   27477331 2019-05-20 10:30 /user/hadoop/usage-stats/09JourneyDataExtract08Jun2016-14Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   17945922 2019-05-20 10:30 /user/hadoop/usage-stats/100JourneyDataExtract07Mar2018-13Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   17008201 2019-05-20 10:30 /user/hadoop/usage-stats/101JourneyDataExtract14Mar2018-20Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   20611024 2019-05-20 10:30 /user/hadoop/usage-stats/102JourneyDataExtract21Mar2018-27Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   11698458 2019-05-20 10:30 /user/hadoop/usage-stats/103JourneyDataExtract28Mar2018-03Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   19569924 2019-05-20 10:30 /user/hadoop/usage-stats/104JourneyDataExtract04Apr2018-10Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   23764410 2019-05-20 10:30 /user/hadoop/usage-stats/105JourneyDataExtract11Apr2018-17Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   33599248 2019-05-20 10:30 /user/hadoop/usage-stats/106JourneyDataExtract18Apr2018-24Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   20983000 2019-05-20 10:30 /user/hadoop/usage-stats/107JourneyDataExtract25Apr2018-01May2018.csv\n-rw-r--r--   3 zeppelin hdfs   31624193 2019-05-20 10:30 /user/hadoop/usage-stats/108JourneyDataExtract02May2018-08May2018.csv\n-rw-r--r--   3 zeppelin hdfs   29317958 2019-05-20 10:30 /user/hadoop/usage-stats/109JourneyDataExtract09May2018-15May2018.csv\n-rw-r--r--   3 zeppelin hdfs   27103628 2019-05-20 10:30 /user/hadoop/usage-stats/10JourneyDataExtract15Jun2016-21Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   53052751 2019-05-20 10:30 /user/hadoop/usage-stats/10a-Journey-Data-Extract-20Sep15-03Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   49626229 2019-05-20 10:30 /user/hadoop/usage-stats/10b-Journey-Data-Extract-04Oct15-17Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   32103278 2019-05-20 10:30 /user/hadoop/usage-stats/110JourneyDataExtract16May2018-22May2018.csv\n-rw-r--r--   3 zeppelin hdfs   28111400 2019-05-20 10:30 /user/hadoop/usage-stats/111JourneyDataExtract23May2018-29May2018.csv\n-rw-r--r--   3 zeppelin hdfs   31383816 2019-05-20 10:30 /user/hadoop/usage-stats/112JourneyDataExtract30May2018-05June2018.csv\n-rw-r--r--   3 zeppelin hdfs   32989219 2019-05-20 10:30 /user/hadoop/usage-stats/113JourneyDataExtract06June2018-12June2018.csv\n-rw-r--r--   3 zeppelin hdfs   31084740 2019-05-20 10:30 /user/hadoop/usage-stats/114JourneyDataExtract13June2018-19June2018.csv\n-rw-r--r--   3 zeppelin hdfs   35340649 2019-05-20 10:30 /user/hadoop/usage-stats/115JourneyDataExtract20June2018-26June2018.csv\n-rw-r--r--   3 zeppelin hdfs   35471699 2019-05-20 10:30 /user/hadoop/usage-stats/116JourneyDataExtract27June2018-03July2018.csv\n-rw-r--r--   3 zeppelin hdfs   35034559 2019-05-20 10:30 /user/hadoop/usage-stats/117JourneyDataExtract04July2018-10July2018.csv\n-rw-r--r--   3 zeppelin hdfs   34344069 2019-05-20 10:30 /user/hadoop/usage-stats/118JourneyDataExtract11July2018-17July2018.csv\n-rw-r--r--   3 zeppelin hdfs   34922018 2019-05-20 10:30 /user/hadoop/usage-stats/119JourneyDataExtract18July2018-24July2018.csv\n-rw-r--r--   3 zeppelin hdfs   27282325 2019-05-20 10:30 /user/hadoop/usage-stats/11JourneyDataExtract22Jun2016-28Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   44970052 2019-05-20 10:30 /user/hadoop/usage-stats/11a-Journey-Data-Extract-18Oct15-31Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   41144797 2019-05-20 10:30 /user/hadoop/usage-stats/11b-Journey-Data-Extract-01Nov15-14Nov15.csv\n-rw-r--r--   3 zeppelin hdfs   31772001 2019-05-20 10:30 /user/hadoop/usage-stats/120JourneyDataExtract25July2018-31July2018.csv\n-rw-r--r--   3 zeppelin hdfs   35021999 2019-05-20 10:30 /user/hadoop/usage-stats/121JourneyDataExtract01Aug2018-07Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   27192201 2019-05-20 10:30 /user/hadoop/usage-stats/122JourneyDataExtract08Aug2018-14Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   29937814 2019-05-20 10:30 /user/hadoop/usage-stats/123JourneyDataExtract15Aug2018-21Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   23412636 2019-05-20 10:30 /user/hadoop/usage-stats/124JourneyDataExtract22Aug2018-28Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   29254055 2019-05-20 10:30 /user/hadoop/usage-stats/125JourneyDataExtract29Aug2018-04Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   28857237 2019-05-20 10:30 /user/hadoop/usage-stats/126JourneyDataExtract05Sep2018-11Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   30181508 2019-05-20 10:30 /user/hadoop/usage-stats/127JourneyDataExtract12Sep2018-18Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   24268772 2019-05-20 10:30 /user/hadoop/usage-stats/128JourneyDataExtract19Sep2018-25Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   29482432 2019-05-20 10:30 /user/hadoop/usage-stats/129JourneyDataExtract26Sep2018-02Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   29674143 2019-05-20 10:30 /user/hadoop/usage-stats/12JourneyDataExtract29Jun2016-05Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   35179761 2019-05-20 10:30 /user/hadoop/usage-stats/12aJourneyDataExtract15Nov15-27Nov15.csv\n-rw-r--r--   3 zeppelin hdfs   39630539 2019-05-20 10:30 /user/hadoop/usage-stats/12bJourneyDataExtract28Nov15-12Dec15.csv\n-rw-r--r--   3 zeppelin hdfs   27050318 2019-05-20 10:30 /user/hadoop/usage-stats/130JourneyDataExtract03Oct2018-09Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   26875110 2019-05-20 10:30 /user/hadoop/usage-stats/131JourneyDataExtract10Oct2018-16Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   28228103 2019-05-20 10:30 /user/hadoop/usage-stats/132JourneyDataExtract17Oct2018-23Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   22841542 2019-05-20 10:30 /user/hadoop/usage-stats/133JourneyDataExtract24Oct2018-30Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   24277034 2019-05-20 10:30 /user/hadoop/usage-stats/134JourneyDataExtract31Oct2018-06Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   21479754 2019-05-20 10:30 /user/hadoop/usage-stats/135JourneyDataExtract07Nov2018-13Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   18968325 2019-05-20 10:30 /user/hadoop/usage-stats/136JourneyDataExtract14Nov2018-20Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   18804556 2019-05-20 10:30 /user/hadoop/usage-stats/137JourneyDataExtract21Nov2018-27Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   19409452 2019-05-20 10:30 /user/hadoop/usage-stats/138JourneyDataExtract28Nov2018-04Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   17720079 2019-05-20 10:30 /user/hadoop/usage-stats/139JourneyDataExtract05Dec2018-11Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   29899103 2019-05-20 10:30 /user/hadoop/usage-stats/13JourneyDataExtract06Jul2016-12Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   28981434 2019-05-20 10:30 /user/hadoop/usage-stats/13aJourneyDataExtract13Dec15-24Dec15.csv\n-rw-r--r--   3 zeppelin hdfs   30064333 2019-05-20 10:30 /user/hadoop/usage-stats/13bJourneyDataExtract25Dec15-09Jan16.csv\n-rw-r--r--   3 zeppelin hdfs   17156314 2019-05-20 10:30 /user/hadoop/usage-stats/140JourneyDataExtract12Dec2018-18Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   15629698 2019-05-20 10:30 /user/hadoop/usage-stats/141JourneyDataExtract19Dec2018-25Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   11287217 2019-05-20 10:30 /user/hadoop/usage-stats/142JourneyDataExtract26Dec2018-01Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   17278475 2019-05-20 10:30 /user/hadoop/usage-stats/143JourneyDataExtract02Jan2019-08Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   20381535 2019-05-20 10:30 /user/hadoop/usage-stats/144JourneyDataExtract09Jan2019-15Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   19152163 2019-05-20 10:30 /user/hadoop/usage-stats/145JourneyDataExtract16Jan2019-22Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   18065288 2019-05-20 10:30 /user/hadoop/usage-stats/146JourneyDataExtract23Jan2019-29Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   16157228 2019-05-20 10:30 /user/hadoop/usage-stats/147JourneyDataExtract30Jan2019-05Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   18602268 2019-05-20 10:30 /user/hadoop/usage-stats/148JourneyDataExtract06Feb2019-12Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   22287567 2019-05-20 10:30 /user/hadoop/usage-stats/149JourneyDataExtract13Feb2019-19Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   34977660 2019-05-20 10:30 /user/hadoop/usage-stats/14JourneyDataExtract13Jul2016-19Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   25452590 2019-05-20 10:30 /user/hadoop/usage-stats/150JourneyDataExtract20Feb2019-26Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   21590398 2019-05-20 10:30 /user/hadoop/usage-stats/151JourneyDataExtract27Feb2019-05Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   18696462 2019-05-20 10:30 /user/hadoop/usage-stats/152JourneyDataExtract06Mar2019-12Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   19369083 2019-05-20 10:30 /user/hadoop/usage-stats/153JourneyDataExtract13Mar2019-19Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   35583327 2019-05-20 10:30 /user/hadoop/usage-stats/15JourneyDataExtract20Jul2016-26Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   32360902 2019-05-20 10:30 /user/hadoop/usage-stats/16JourneyDataExtract27Jul2016-02Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   33998852 2019-05-20 10:30 /user/hadoop/usage-stats/17JourneyDataExtract03Aug2016-09Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   35101289 2019-05-20 10:30 /user/hadoop/usage-stats/18JourneyDataExtract10Aug2016-16Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   29162758 2019-05-20 10:30 /user/hadoop/usage-stats/19JourneyDataExtract17Aug2016-23Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   32091911 2019-05-20 10:30 /user/hadoop/usage-stats/1a.JourneyDataExtract04Jan15-17Jan15.csv\n-rw-r--r--   3 zeppelin hdfs   34103997 2019-05-20 10:30 /user/hadoop/usage-stats/1b.JourneyDataExtract18Jan15-31Jan15.csv\n-rw-r--r--   3 zeppelin hdfs  208701113 2019-05-20 10:30 /user/hadoop/usage-stats/2015TripDatazip.zip\n-rw-r--r--   3 zeppelin hdfs  222067537 2019-05-20 10:30 /user/hadoop/usage-stats/2016TripDataZip.zip\n-rw-r--r--   3 zeppelin hdfs   30509039 2019-05-20 10:30 /user/hadoop/usage-stats/20JourneyDataExtract24Aug2016-30Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   29461893 2019-05-20 10:30 /user/hadoop/usage-stats/21JourneyDataExtract31Aug2016-06Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   31357377 2019-05-20 10:30 /user/hadoop/usage-stats/22JourneyDataExtract07Sep2016-13Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   28847793 2019-05-20 10:30 /user/hadoop/usage-stats/23JourneyDataExtract14Sep2016-20Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   30932775 2019-05-20 10:30 /user/hadoop/usage-stats/24JourneyDataExtract21Sep2016-27Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   27343703 2019-05-20 10:30 /user/hadoop/usage-stats/25JourneyDataExtract28Sep2016-04Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   27338225 2019-05-20 10:30 /user/hadoop/usage-stats/26JourneyDataExtract05Oct2016-11Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   24271970 2019-05-20 10:30 /user/hadoop/usage-stats/27JourneyDataExtract12Oct2016-18Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   25570755 2019-05-20 10:30 /user/hadoop/usage-stats/28JourneyDataExtract19Oct2016-25Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   26860667 2019-05-20 10:30 /user/hadoop/usage-stats/29JourneyDataExtract26Oct2016-01Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   32961852 2019-05-20 10:30 /user/hadoop/usage-stats/2a.JourneyDataExtract01Feb15-14Feb15.csv\n-rw-r--r--   3 zeppelin hdfs   32811094 2019-05-20 10:30 /user/hadoop/usage-stats/2b.JourneyDataExtract15Feb15-28Feb15.csv\n-rw-r--r--   3 zeppelin hdfs   19636191 2019-05-20 10:30 /user/hadoop/usage-stats/30JourneyDataExtract02Nov2016-08Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   18801936 2019-05-20 10:30 /user/hadoop/usage-stats/31JourneyDataExtract09Nov2016-15Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   19129013 2019-05-20 10:30 /user/hadoop/usage-stats/32JourneyDataExtract16Nov2016-22Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   21182586 2019-05-20 10:30 /user/hadoop/usage-stats/33JourneyDataExtract23Nov2016-29Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   20338179 2019-05-20 10:30 /user/hadoop/usage-stats/34JourneyDataExtract30Nov2016-06Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   20959011 2019-05-20 10:30 /user/hadoop/usage-stats/35JourneyDataExtract07Dec2016-13Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   20169339 2019-05-20 10:30 /user/hadoop/usage-stats/36JourneyDataExtract14Dec2016-20Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   15407215 2019-05-20 10:30 /user/hadoop/usage-stats/37JourneyDataExtract21Dec2016-27Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   10688750 2019-05-20 10:30 /user/hadoop/usage-stats/38JourneyDataExtract28Dec2016-03Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   19106889 2019-05-20 10:30 /user/hadoop/usage-stats/39JourneyDataExtract04Jan2017-10Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   41814096 2019-05-20 10:30 /user/hadoop/usage-stats/3a.JourneyDataExtract01Mar15-15Mar15.csv\n-rw-r--r--   3 zeppelin hdfs   42438904 2019-05-20 10:30 /user/hadoop/usage-stats/3b.JourneyDataExtract16Mar15-31Mar15.csv\n-rw-r--r--   3 zeppelin hdfs   16105427 2019-05-20 10:30 /user/hadoop/usage-stats/40JourneyDataExtract11Jan2017-17Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   19613581 2019-05-20 10:30 /user/hadoop/usage-stats/41JourneyDataExtract18Jan2017-24Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   17726211 2019-05-20 10:30 /user/hadoop/usage-stats/42JourneyDataExtract25Jan2017-31Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   18756158 2019-05-20 10:30 /user/hadoop/usage-stats/43JourneyDataExtract01Feb2017-07Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   16632899 2019-05-20 10:30 /user/hadoop/usage-stats/44JourneyDataExtract08Feb2017-14Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   21179040 2019-05-20 10:30 /user/hadoop/usage-stats/45JourneyDataExtract15Feb2017-21Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   18191104 2019-05-20 10:30 /user/hadoop/usage-stats/46JourneyDataExtract22Feb2017-28Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   19708533 2019-05-20 10:30 /user/hadoop/usage-stats/47JourneyDataExtract01Mar2017-07Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   23409196 2019-05-20 10:30 /user/hadoop/usage-stats/48JourneyDataExtract08Mar2017-14Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   10939999 2019-05-20 10:30 /user/hadoop/usage-stats/49JourneyDataExtract15Mar2017-21Mar2017.xlsx\n-rw-r--r--   3 zeppelin hdfs   51464075 2019-05-20 10:30 /user/hadoop/usage-stats/4a.JourneyDataExtract01Apr15-16Apr15.csv\n-rw-r--r--   3 zeppelin hdfs   55715516 2019-05-20 10:30 /user/hadoop/usage-stats/4b.JourneyDataExtract 17Apr15-02May15.csv\n-rw-r--r--   3 zeppelin hdfs   23600415 2019-05-20 10:30 /user/hadoop/usage-stats/50 Journey Data Extract 22Mar2017-28Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   26607449 2019-05-20 10:30 /user/hadoop/usage-stats/51 Journey Data Extract 29Mar2017-04Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   30823972 2019-05-20 10:30 /user/hadoop/usage-stats/52 Journey Data Extract 05Apr2017-11Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   22222026 2019-05-20 10:30 /user/hadoop/usage-stats/53JourneyDataExtract12Apr2017-18Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   26147246 2019-05-20 10:30 /user/hadoop/usage-stats/54JourneyDataExtract19Apr2017-25Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   23432931 2019-05-20 10:30 /user/hadoop/usage-stats/55JourneyData Extract26Apr2017-02May2017.csv\n-rw-r--r--   3 zeppelin hdfs   25584245 2019-05-20 10:30 /user/hadoop/usage-stats/56JourneyDataExtract 03May2017-09May2017.csv\n-rw-r--r--   3 zeppelin hdfs   27201854 2019-05-20 10:30 /user/hadoop/usage-stats/57JourneyDataExtract10May2017-16May2017.csv\n-rw-r--r--   3 zeppelin hdfs   25599256 2019-05-20 10:30 /user/hadoop/usage-stats/58JourneyDataExtract17May2017-23May2017.csv\n-rw-r--r--   3 zeppelin hdfs   31127688 2019-05-20 10:30 /user/hadoop/usage-stats/59JourneyDataExtract24May2017-30May2017.csv\n-rw-r--r--   3 zeppelin hdfs   48750814 2019-05-20 10:30 /user/hadoop/usage-stats/5a.JourneyDataExtract03May15-16May15.csv\n-rw-r--r--   3 zeppelin hdfs   51948160 2019-05-20 10:30 /user/hadoop/usage-stats/5b.JourneyDataExtract17May15-30May15.csv\n-rw-r--r--   3 zeppelin hdfs   28357343 2019-05-20 10:30 /user/hadoop/usage-stats/60JourneyDataExtract31May2017-06Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   30163497 2019-05-20 10:30 /user/hadoop/usage-stats/61JourneyDataExtract07Jun2017-13Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   35548875 2019-05-20 10:30 /user/hadoop/usage-stats/62JourneyDataExtract14Jun2017-20Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   30496272 2019-05-20 10:30 /user/hadoop/usage-stats/63JourneyDataExtract21Jun2017-27Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   33295260 2019-05-20 10:30 /user/hadoop/usage-stats/64JourneyDataExtract28Jun2017-04Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   34054170 2019-05-20 10:30 /user/hadoop/usage-stats/65JourneyDataExtract05Jul2017-11Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   32194296 2019-05-20 10:31 /user/hadoop/usage-stats/66JourneyDataExtract12Jul2017-18Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   27980289 2019-05-20 10:31 /user/hadoop/usage-stats/67JourneyDataExtract19Jul2017-25Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   23363051 2019-05-20 10:31 /user/hadoop/usage-stats/68JourneyDataExtract26Jul2017-31Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   27444824 2019-05-20 10:31 /user/hadoop/usage-stats/69JourneyDataExtract01Aug2017-07Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   51513415 2019-05-20 10:31 /user/hadoop/usage-stats/6aJourneyDataExtract31May15-12Jun15.csv\n-rw-r--r--   3 zeppelin hdfs   62770968 2019-05-20 10:31 /user/hadoop/usage-stats/6bJourneyDataExtract13Jun15-27Jun15.csv\n-rw-r--r--   3 zeppelin hdfs   26210762 2019-05-20 10:31 /user/hadoop/usage-stats/70JourneyDataExtract08Aug2017-14Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   30238704 2019-05-20 10:31 /user/hadoop/usage-stats/71JourneyDataExtract15Aug2017-22Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   28921544 2019-05-20 10:31 /user/hadoop/usage-stats/72JourneyDataExtract23Aug2017-29Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   25301473 2019-05-20 10:31 /user/hadoop/usage-stats/73JourneyDataExtract30Aug2017-05Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   24275323 2019-05-20 10:31 /user/hadoop/usage-stats/74JourneyDataExtract06Sep2017-12Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   25497631 2019-05-20 10:31 /user/hadoop/usage-stats/75JourneyDataExtract13Sep2017-19Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   27743176 2019-05-20 10:31 /user/hadoop/usage-stats/76JourneyDataExtract20Sep2017-26Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   26348497 2019-05-20 10:31 /user/hadoop/usage-stats/77JourneyDataExtract27Sep2017-03Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   27510237 2019-05-20 10:31 /user/hadoop/usage-stats/78JourneyDataExtract04Oct2017-10Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   28322922 2019-05-20 10:31 /user/hadoop/usage-stats/79JourneyDataExtract11Oct2017-17Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   69560935 2019-05-20 10:31 /user/hadoop/usage-stats/7a.JourneyDataExtract28Jun15-11Jul15.csv\n-rw-r--r--   3 zeppelin hdfs   57317508 2019-05-20 10:31 /user/hadoop/usage-stats/7b.JourneyDataExtract12Jul15-25Jul15.csv\n-rw-r--r--   3 zeppelin hdfs   23390665 2019-05-20 10:31 /user/hadoop/usage-stats/80JourneyDataExtract18Oct2017-24Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   25260806 2019-05-20 10:31 /user/hadoop/usage-stats/81JourneyDataExtract25Oct2017-31Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   24124382 2019-05-20 10:31 /user/hadoop/usage-stats/82JourneyDataExtract01Nov2017-07Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21684181 2019-05-20 10:31 /user/hadoop/usage-stats/83JourneyDataExtract08Nov2017-14Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21630715 2019-05-20 10:31 /user/hadoop/usage-stats/84JourneyDataExtract15Nov2017-21Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21045620 2019-05-20 10:31 /user/hadoop/usage-stats/85JourneyDataExtract22Nov2017-28Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   19704514 2019-05-20 10:31 /user/hadoop/usage-stats/86JourneyDataExtract29Nov2017-05Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   16113127 2019-05-20 10:31 /user/hadoop/usage-stats/87JourneyDataExtract06Dec2017-12Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   15695037 2019-05-20 10:31 /user/hadoop/usage-stats/88JourneyDataExtract13Dec2017-19Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   13306018 2019-05-20 10:31 /user/hadoop/usage-stats/89JourneyDataExtract20Dec2017-26Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   59614317 2019-05-20 10:31 /user/hadoop/usage-stats/8a-Journey-Data-Extract-26Jul15-07Aug15.csv\n-rw-r--r--   3 zeppelin hdfs   62706885 2019-05-20 10:31 /user/hadoop/usage-stats/8b-Journey-Data-Extract-08Aug15-22Aug15.csv\n-rw-r--r--   3 zeppelin hdfs    8384731 2019-05-20 10:31 /user/hadoop/usage-stats/90JourneyDataExtract27Dec2017-02Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   16006442 2019-05-20 10:31 /user/hadoop/usage-stats/91JourneyDataExtract03Jan2018-09Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   18618252 2019-05-20 10:31 /user/hadoop/usage-stats/92JourneyDataExtract10Jan2018-16Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   17591748 2019-05-20 10:31 /user/hadoop/usage-stats/93JourneyDataExtract17Jan2018-23Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   19317042 2019-05-20 10:31 /user/hadoop/usage-stats/94JourneyDataExtract24Jan2018-30Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   17621910 2019-05-20 10:31 /user/hadoop/usage-stats/95JourneyDataExtract31Jan2018-06Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   17072972 2019-05-20 10:31 /user/hadoop/usage-stats/96JourneyDataExtract07Feb2018-13Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   18701361 2019-05-20 10:31 /user/hadoop/usage-stats/97JourneyDataExtract14Feb2018-20Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   17237378 2019-05-20 10:31 /user/hadoop/usage-stats/98JourneyDataExtract21Feb2018-27Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   11650126 2019-05-20 10:31 /user/hadoop/usage-stats/99JourneyDataExtract28Feb2018-06Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   43178453 2019-05-20 10:31 /user/hadoop/usage-stats/9a-Journey-Data-Extract-23Aug15-05Sep15.csv\n-rw-r--r--   3 zeppelin hdfs   50958478 2019-05-20 10:31 /user/hadoop/usage-stats/9b-Journey-Data-Extract-06Sep15-19Sep15.csv\n-rw-r--r--   3 zeppelin hdfs  225391187 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2012.zip\n-rw-r--r--   3 zeppelin hdfs  183134189 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2013.zip\n-rw-r--r--   3 zeppelin hdfs  225215129 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2014.zip\n-rw-r--r--   3 zeppelin hdfs      10776 2019-05-20 10:31 /user/hadoop/usage-stats/cycling-load.json\n"}]},"apps":[],"jobName":"paragraph_1558311455928_647183211","id":"20190520-021735_735409098","dateCreated":"2019-05-20T02:17:35+0200","dateStarted":"2019-05-20T10:29:50+0200","dateFinished":"2019-05-20T10:31:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:170"},{"text":"%spark2.pyspark\n\n#read TfL raw data\nrdd_usage = spark.read.csv(\"/user/hadoop/usage-stats/*.csv\", header=True)\n#Took 12 seconds to read 38.122.372 lines\n#38.122.372 - 1 rows\n\nrdd_usage = rdd_usage.na.fill(0)\nrdd_usage.show()","user":"admin","dateUpdated":"2019-06-02T14:55:44+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n|Rental Id|Duration|Bike Id|        End Date|EndStation Id|     EndStation Name|      Start Date|StartStation Id|   StartStation Name|\n+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n| 52538879|     660|    395|01/04/2016 00:11|          137|Bourne Street, Be...|01/04/2016 00:00|            573|Limerston Street,...|\n| 52538876|     420|  12931|01/04/2016 00:07|          507|Clarkson Street, ...|01/04/2016 00:00|            399|Brick Lane Market...|\n| 52538877|     420|   7120|01/04/2016 00:07|          507|Clarkson Street, ...|01/04/2016 00:00|            399|Brick Lane Market...|\n| 52538878|     300|   1198|01/04/2016 00:05|          616|Aintree Street, F...|01/04/2016 00:00|            599|Manbre Road, Hamm...|\n| 52538874|    1260|  10739|01/04/2016 00:21|          486|Granby Street, Sh...|01/04/2016 00:00|            135|Clerkenwell Green...|\n| 52538875|    1260|  10949|01/04/2016 00:21|          486|Granby Street, Sh...|01/04/2016 00:00|            135|Clerkenwell Green...|\n| 52538880|     540|   8831|01/04/2016 00:09|          411|Walworth Road, El...|01/04/2016 00:00|            193|Bankside Mix, Ban...|\n| 52538881|     600|   8778|01/04/2016 00:11|          770|Gwendwr Road, Wes...|01/04/2016 00:01|            266|Queen's Gate (Nor...|\n| 52538882|     540|    700|01/04/2016 00:10|          294|St. George's Squa...|01/04/2016 00:01|            137|Bourne Street, Be...|\n| 52538883|    3000|   5017|01/04/2016 00:51|           39|Shoreditch High S...|01/04/2016 00:01|            456|Parkway, Camden Town|\n| 52538885|     240|   4359|01/04/2016 00:06|          445|Cheshire Street, ...|01/04/2016 00:02|            444|Bethnal Green Gar...|\n| 52538884|    5340|   2801|01/04/2016 01:31|          288|Elizabeth Bridge,...|01/04/2016 00:02|            288|Elizabeth Bridge,...|\n| 52538887|     300|   9525|01/04/2016 00:08|          592|Bishop's Bridge R...|01/04/2016 00:03|            265|Southwick Street,...|\n| 52538888|     840|   8410|01/04/2016 00:17|          360|Howick Place, Wes...|01/04/2016 00:03|            174|      Strand, Strand|\n| 52538886|     600|  12206|01/04/2016 00:13|          607|Putney Bridge Sta...|01/04/2016 00:03|            738|Imperial Road, Sa...|\n| 52538891|     120|   2998|01/04/2016 00:06|          384|Marloes Road, Ken...|01/04/2016 00:04|            157|Wright's Lane, Ke...|\n| 52538893|     300|  10970|01/04/2016 00:09|          242|Beaumont Street, ...|01/04/2016 00:04|            239|Warren Street Sta...|\n| 52538890|    1200|   9350|01/04/2016 00:24|          517| Ford Road, Old Ford|01/04/2016 00:04|            789|Podium, Queen Eli...|\n| 52538889|    1800|   7005|01/04/2016 00:34|          583|Abingdon Green, W...|01/04/2016 00:04|            354|Northumberland Av...|\n| 52538892|    1800|   9368|01/04/2016 00:34|          583|Abingdon Green, W...|01/04/2016 00:04|            354|Northumberland Av...|\n+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=0","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=2","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=3"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558311715439_1870867428","id":"20190520-022155_1304622215","dateCreated":"2019-05-20T02:21:55+0200","dateStarted":"2019-05-28T09:21:19+0200","dateFinished":"2019-05-28T09:22:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"text":"%spark2.pyspark\nrdd_usage.count()","user":"admin","dateUpdated":"2019-05-20T10:32:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"42826257\n"}]},"apps":[],"jobName":"paragraph_1558311735111_-280477074","id":"20190520-022215_2120296642","dateCreated":"2019-05-20T02:22:15+0200","dateStarted":"2019-05-20T10:32:14+0200","dateFinished":"2019-05-20T10:33:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"%spark2.pyspark\n\n#read weather data\nrdd_weather = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\nrdd_weather.show()","user":"admin","dateUpdated":"2019-06-02T14:56:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-------------------+--------+---------+--------------------------+--------------------+\n|Start Date|      Daily Weather|Humidity|Windspeed|Apparent Temperature (Avg)|      Hourly Weather|\n+----------+-------------------+--------+---------+--------------------------+--------------------+\n|04.01.2015|                fog|    0.94|     0.55|                    36.295|[{'time': 1420329...|\n|05.01.2015|  partly-cloudy-day|    0.88|     1.59|                     46.74|[{'time': 1420416...|\n|06.01.2015|  partly-cloudy-day|    0.86|     2.07|         42.15000000000001|[{'time': 1420502...|\n|07.01.2015|partly-cloudy-night|    0.86|     4.13|                     45.45|[{'time': 1420588...|\n|08.01.2015|               rain|    0.87|      3.6|                      46.2|[{'time': 1420675...|\n|09.01.2015|  partly-cloudy-day|    0.81|     7.43|                    56.085|[{'time': 1420761...|\n|10.01.2015|  partly-cloudy-day|    0.74|     7.05|                    44.615|[{'time': 1420848...|\n|11.01.2015|partly-cloudy-night|    0.74|     5.44|                    43.385|[{'time': 1420934...|\n|12.01.2015|               rain|    0.83|     6.57|                    49.535|[{'time': 1421020...|\n|13.01.2015|  partly-cloudy-day|    0.84|     4.12|                     39.87|[{'time': 1421107...|\n|14.01.2015|partly-cloudy-night|    0.76|     5.04|                     42.13|[{'time': 1421193...|\n|15.01.2015|               rain|    0.77|     6.71|                     41.05|[{'time': 1421280...|\n|16.01.2015|  partly-cloudy-day|    0.76|     2.31|                      37.0|[{'time': 1421366...|\n|17.01.2015|  partly-cloudy-day|    0.83|     1.72|                     39.13|[{'time': 1421452...|\n|18.01.2015|  partly-cloudy-day|    0.89|     0.84|                     35.21|[{'time': 1421539...|\n|19.01.2015|  partly-cloudy-day|    0.85|     0.65|                     32.85|[{'time': 1421625...|\n|20.01.2015|partly-cloudy-night|    0.85|     1.38|                    36.285|[{'time': 1421712...|\n|21.01.2015|  partly-cloudy-day|    0.87|      2.6|                    35.595|[{'time': 1421798...|\n|22.01.2015|  partly-cloudy-day|    0.83|     0.83|                    32.875|[{'time': 1421884...|\n|23.01.2015|               rain|    0.85|     1.95|                    36.575|[{'time': 1421971...|\n+----------+-------------------+--------+---------+--------------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558311748207_1320908318","id":"20190520-022228_1404530849","dateCreated":"2019-05-20T02:22:28+0200","dateStarted":"2019-05-20T10:32:27+0200","dateFinished":"2019-05-20T10:33:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"%md\n#### Jumping Window ####\n> <sup>First approach, to use a jumping window going from \"today\" `x` hours into past and future</sup>\n","user":"admin","dateUpdated":"2019-06-02T14:56:10+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Jumping Window</h4>\n<blockquote><p><sup>First approach, to use a jumping window going from &ldquo;today&rdquo; <code>x</code> hours into past and future</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559479640325_-466070116","id":"20190602-144720_92575506","dateCreated":"2019-06-02T14:47:20+0200","dateStarted":"2019-06-02T14:56:10+0200","dateFinished":"2019-06-02T14:56:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import from_json\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import unix_timestamp, hour, col, split, round\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\n\ndef generateWeatherDF(x):\n    \"\"\" Generates dataframe on hourly granularity \"\"\"\n\n    schema_hum = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"humidity\", FloatType(), True)]))\n        \n    schema_temp = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"apparentTemperature\", FloatType(), True)]))\n        \n    schema_wind = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"windSpeed\", FloatType(), True)]))\n                \n    schema_sum = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"icon\", StringType(), True)]))\n   \n\n    #adding past data from previous day\n    rdd_weather = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\n    columns_to_drop = [\"Hourly Weather\",\n                       \"Humidity\",\n                       \"Yesterday\",\n                       \"Daily Weather\",\n                       \"Apparent Temperature (Avg)\",\n                       \"Windspeed\"]\n                       \n    if x > 24:\n        y = 0\n        i = 0\n        b = False\n        \n        rdd_weather2 = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\n        \n        for j in range(x):\n            print(\"Day \", j)\n            rdd_weather = rdd_weather.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy')) \n            #check for -y \n            print(\"Y\", y)\n            rdd_weather = rdd_weather.withColumn('Yesterday'+`j`, F.date_add(rdd_weather['Start Date'], y))\n            rdd_weather2 = rdd_weather2.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy'))\n            rdd_weather = rdd_weather.alias(\"t\").join(rdd_weather2, rdd_weather[\"Yesterday\"+`j`] == rdd_weather2[\"Start Date\"], how='left_outer').select(\"t.*\", rdd_weather2[\"Start Date\"].alias(\"Start Date\"+`j`), rdd_weather2[\"Hourly Weather\"].alias(\"Hourly Weather\"+`j`))\n            \n            columns_to_drop.append(\"Hourly Weather\"+`j`)\n            columns_to_drop.append(\"Start Date\"+`j`)\n            columns_to_drop.append(\"Yesterday\"+`j`)\n\n                \n            # alt -> neu oder von neu -> alt\n            if (x > 24):\n                x -= 24\n                if (b == False):\n                    for k in range(24):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n                    b = True\n                else:\n                    for k in range(24):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n            else:\n                for k in range(x):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n                x = 0\n                break\n                \n            y -= 1\n\n    elif x <= 24 and x >= 0:\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_sum)[i].getItem(\"icon\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_hum)[i].getItem(\"humidity\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_temp)[i].getItem(\"apparentTemperature\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_wind)[i].getItem(\"windSpeed\"))\n\n    else:\n        print(\"Invalid value\")\n     \n    rdd_weather = rdd_weather[sorted(rdd_weather.columns)]\n #   rdd_weather = rdd_weather.drop(*columns_to_drop)\n  #  rdd_weather.printSchema()\n    rdd_weather = rdd_weather.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy'))\n    \n    return rdd_weather\n    \ndef colChecker(i, val):\n    \"\"\"Checks the columns for correct ordering\"\"\"\n    if (i >= 10):\n        if (val == \"sum\"):\n            return \"sum\"\n        elif (val == \"hum\"):\n            return \"hum\"\n        elif (val == \"temp\"):\n            return \"temp\"\n        elif (val == \"wind\"):\n            return \"wind\"\n    else:\n        if (val == \"sum\"):\n            return \"sum0\"\n        elif (val == \"hum\"):\n            return \"hum0\"\n        elif (val == \"temp\"):\n            return \"temp0\"\n        elif (val == \"wind\"):\n            return \"wind0\"\n            \n\ndef generateRentalDF(station): \n    \"\"\"Generate cycling usage DF of either all stations or only one station\"\"\"\n    \n    df_usage = spark.read.csv(\"/user/hadoop/usage-stats/*.csv\", header=True)\n    df_usage = df_usage.na.fill(0)\n    \n    #One station, TODO:// all stations\n    df_usage = df_usage.filter(col(\"StartStation Id\").isin([''+`station`]))\n    \n    df_usage.dropna(subset=[\"StartStation Id\", \"EndStation Id\", \"Start Date\", \"End Date\"])\n\n    #Change column types of ID columns\n    df_usage = df_usage.withColumn(\"EndStation ID\", df_usage[\"EndStation ID\"].cast(\"int\"))\n    df_usage = df_usage.withColumn(\"StartStation ID\", df_usage[\"StartStation ID\"].cast(\"int\"))\n\n    #Prevent StartStation ID = EndStation ID\n   # df_usage = df_usage[df_usage[\"StartStation Id\"] != df_usage[\"EndStation Id\"]]\n\n    #Keep only relevant columns\n    df_usage = df_usage[\"Start Date\", \"StartStation Id\", \"End Date\", \"EndStation Id\", \"Duration\"]\n    \n    #root |-- Start Date: string (nullable = true) |-- StartStation Id: integer (nullable = true) |-- End Date: string (nullable = true) |-- EndStation Id: integer (nullable = true) |-- Duration: string (nullable = true)\n    #Add hour column, rounded\n    df_usage = df_usage.alias(\"t\").select('t.*', unix_timestamp('Start Date', \"dd/MM/yyyy HH:mm\").cast(TimestampType()).alias(\"Timestamp\"))\n    #df_usage = df_usage.withColumn(\"Rounded Hour\", hour((round(unix_timestamp(\"Timestamp\")/3600)*3600).cast(\"Timestamp\")))\n    #change: round off hour values 23:34 -> 23:00\n    df_usage = df_usage.withColumn(\"Rounded Hour\", hour(unix_timestamp(\"Timestamp\").cast(\"Timestamp\")))\n\n    \n    #Preparing for join\n    split_col = split(df_usage['Start Date'], ' ')\n    df_usage = df_usage.withColumn('Datum', split_col.getItem(0)).withColumn('Hour', split_col.getItem(1))\n    \n    #Drop unuseful columns\n    columns_to_drop = [\"End Date\",\n                       \"EndStation Id\",\n                       \"Duration\"\n                      # \"Start Date\",\n                      # \"StartStation Id\"\n                       ]\n    df_usage = df_usage.drop(*columns_to_drop)\n\n\n    return df_usage\n    \n\ndef generatePast(x, df_usage):\n    \"\"\"Generate past hourly data\"\"\"\n    rm_cols = []\n    b = False\n    df_usage = df_usage.groupby(df_usage.Datum).pivot(\"Rounded Hour\").count().orderBy(\"Datum\")\n    df_usage = df_usage.na.fill(0)\n    df_usage = df_usage.alias(\"t\").select('t.*', unix_timestamp('Datum', \"dd/MM/yyyy\").cast(TimestampType()).alias(\"Timestamp\")).orderBy(\"Timestamp\")\n    df_usage = df_usage.drop(\"Timestamp\", \"null\")\n    df_usage = df_usage.withColumn('Datum',F.to_date(\"Datum\", 'dd/MM/yyyy'))\n    df_usage = rename_cols(df_usage, \"usage_\")\n    liste = []\n    \n    ##Past \n    if x > 24:\n        #time delta\n        y = 0\n        #column counter\n        i = 24\n        for j in range(x):\n            liste = []\n            print(\"Past \", j)\n            df_usage = df_usage.withColumn('Yesterday'+`j`, F.date_add(df_usage['Datum'], y))\n            if (x > 24):\n                x -= 24\n                if (b == True):\n                    for k in range(24):\n                        liste.append(\"usage_\"+`i`)\n                        i += 1\n                    df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Yesterday'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n                b = True\n            else:\n                for k in range(x):\n                    liste.append(\"usage_\"+`i`)\n                    i += 1\n                df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Yesterday'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n                x = 0\n                break\n            y -= 1\n    \n    else:\n        for i in range(24, x, -1):\n            rm_cols.append(\"usage_\"+`i`)\n            rm_cols.append(\"Yesterday\"+`i`) #disable this for previous date\n        df_usage = df_usage.drop(*rm_cols)\n    \n    \n    df_usage = df_usage.orderBy(\"Datum\")\n    return df_usage\n\ndef generateFuture(f, df_usage):\n    \"\"\"Generate future hourly data\"\"\"\n    ##Future\n    y = 0\n    i = 0\n    rm_cols = []\n    for j in range(f):\n        print(\"Future \", j)\n        liste = []\n        y += 1\n        df_usage = df_usage.withColumn('Tomorrow'+`j`, F.date_add(df_usage['Datum'], y))\n        if f > 24:\n            f -= 24\n            for k in range(24):\n                liste.append(\"future_\"+`i`)\n                i += 1\n            df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Tomorrow'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+ \"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n        else:\n            for k in range(f):\n                liste.append(\"future_\"+`i`)\n                i += 1\n            df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Tomorrow'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n            break\n        \n    df_usage = df_usage.orderBy(\"Datum\")\n    return df_usage\n    \ndef rename_cols(rename_df, z):\n    \"\"\"Rename all frequency columns\"\"\"\n    for column in rename_df.columns[1:]:\n        new_column = z+column \n        rename_df = rename_df.withColumnRenamed(column, new_column)\n    return rename_df\n    \ndef cleansingDF(df_rental, df_weather):\n    \"\"\"Cleaning null values, merge weather and usage\"\"\"\n    df_final = df_rental.alias('l').join(df_weather.alias('r'), col('l.Datum') == col('r.Start Date'), how='left').select(col('l.*'), col('r.*'))\n    df_final = df_final.orderBy(\"Datum\")\n    return df_final\n\n\n\n######Jumping Window Solution######\n######For using, just comment out below######\n######Note: some functions of this script is reused for sliding window approach below as well#######\n\n####Init\n#Set value x = hours to the past, station = station number, f = zielgröße (future)\nx = 24\nstation = 14 #Kings cross (most used station)\nf = 24\n\n####Generate Weather DF \n#df_weather = generateWeatherDF(x)\n#df_weather.show()\n\n####Generate Rental DF\ndf_rental = generateRentalDF(station)\ndf_rental.show()\n\n####Add Past hours\n#df_rental = generatePast(x, df_rental)\n#df_rental.show()\n\n####Add Future hours\n#df_rental = generateFuture(f, df_rental)\n#df_rental.show()\n\n####Merge both\n#df_final = cleansingDF(df_rental, df_weather)\n#df_final.printSchema()\n#df_final.show()\n\n####Save back as csv\n\n#df_final.coalesce(1).write.csv(\"/user/hadoop/preparedDataframe09\", header=True)\n\n#df_weather.coalesce(1).write.csv(\"/user/hadoop/WeatherData2\", header=True)\n\n","user":"admin","dateUpdated":"2019-06-11T02:18:16+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+---------------+-------------------+------------+----------+-----+\n|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|\n+----------------+---------------+-------------------+------------+----------+-----+\n|01/04/2016 06:14|             14|2016-04-01 06:14:00|           6|01/04/2016|06:14|\n|01/04/2016 06:23|             14|2016-04-01 06:23:00|           6|01/04/2016|06:23|\n|01/04/2016 06:25|             14|2016-04-01 06:25:00|           6|01/04/2016|06:25|\n|01/04/2016 06:41|             14|2016-04-01 06:41:00|           6|01/04/2016|06:41|\n|01/04/2016 06:42|             14|2016-04-01 06:42:00|           6|01/04/2016|06:42|\n|01/04/2016 06:50|             14|2016-04-01 06:50:00|           6|01/04/2016|06:50|\n|01/04/2016 06:51|             14|2016-04-01 06:51:00|           6|01/04/2016|06:51|\n|01/04/2016 06:52|             14|2016-04-01 06:52:00|           6|01/04/2016|06:52|\n|01/04/2016 06:53|             14|2016-04-01 06:53:00|           6|01/04/2016|06:53|\n|01/04/2016 06:57|             14|2016-04-01 06:57:00|           6|01/04/2016|06:57|\n|01/04/2016 07:09|             14|2016-04-01 07:09:00|           7|01/04/2016|07:09|\n|01/04/2016 07:13|             14|2016-04-01 07:13:00|           7|01/04/2016|07:13|\n|01/04/2016 07:13|             14|2016-04-01 07:13:00|           7|01/04/2016|07:13|\n|01/04/2016 07:16|             14|2016-04-01 07:16:00|           7|01/04/2016|07:16|\n|01/04/2016 07:17|             14|2016-04-01 07:17:00|           7|01/04/2016|07:17|\n|01/04/2016 07:18|             14|2016-04-01 07:18:00|           7|01/04/2016|07:18|\n|01/04/2016 07:18|             14|2016-04-01 07:18:00|           7|01/04/2016|07:18|\n|01/04/2016 07:18|             14|2016-04-01 07:18:00|           7|01/04/2016|07:18|\n|01/04/2016 07:19|             14|2016-04-01 07:19:00|           7|01/04/2016|07:19|\n|01/04/2016 07:22|             14|2016-04-01 07:22:00|           7|01/04/2016|07:22|\n+----------------+---------------+-------------------+------------+----------+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1210","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1211","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1212","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1213"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558311757445_353444766","id":"20190520-022237_1689603982","dateCreated":"2019-05-20T02:22:37+0200","dateStarted":"2019-06-11T02:17:29+0200","dateFinished":"2019-06-11T02:17:33+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"text":"%md\n#### Testing area ####\n<sup>Just for some tests... (You can safely ignore this)</sup>","user":"admin","dateUpdated":"2019-06-02T14:50:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Testing area</h4>\n<p><sup>Just for some tests&hellip; (You can safely ignore this)</sup></p>\n"}]},"apps":[],"jobName":"paragraph_1559181026812_-531700604","id":"20190530-035026_630194382","dateCreated":"2019-05-30T03:50:26+0200","dateStarted":"2019-06-02T14:50:04+0200","dateFinished":"2019-06-02T14:50:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"text":"%spark2.pyspark\n#This code is to compute a moving/rolling average over a DataFrame using Spark.\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as func\n\n#function to calculate number of seconds from number of days: thanks Bob Swain\ndays = lambda i: i * 86400\n\n\ndf = spark.createDataFrame([(17.00, \"2018-03-10T15:27:18+00:00\"), # The first six days are sequential\n  (13.00, \"2018-03-11T12:27:18+00:00\"),  # included ...  \n  (25.00, \"2018-03-12T11:27:18+00:00\"),  # included ...\n  (20.00, \"2018-03-13T15:27:18+00:00\"),  # included ...\n  (56.00, \"2018-03-14T12:27:18+00:00\"),  # included...\n  (99.00, \"2018-03-15T11:27:18+00:00\"),  # This one will be included with the next window\n  (156.00, \"2018-03-22T11:27:18+00:00\"), # This one is inside the 7 day window of the previous\n  (122.00, \"2018-03-31T11:27:18+00:00\"), # This one is a new window, outside the 7 day window of any previous...\n  (7000.00, \"2018-04-15T11:27:18+00:00\"),# This starts a * brand new window * with the next entry included next\n  (9999.00, \"2018-04-16T11:27:18+00:00\") # This should be part of the previous entry\n  ],\n  [\"dollars\", \"timestampGMT\"])\n\n# we need this timestampGMT as seconds for our Window time frame\n\ndf = df.withColumn('timestampGMT', df.timestampGMT.cast('timestamp'))\ndf.show()","user":"admin","dateUpdated":"2019-05-30T20:39:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-------------------+\n|dollars|       timestampGMT|\n+-------+-------------------+\n|   17.0|2018-03-10 16:27:18|\n|   13.0|2018-03-11 13:27:18|\n|   25.0|2018-03-12 12:27:18|\n|   20.0|2018-03-13 16:27:18|\n|   56.0|2018-03-14 13:27:18|\n|   99.0|2018-03-15 12:27:18|\n|  156.0|2018-03-22 12:27:18|\n|  122.0|2018-03-31 13:27:18|\n| 7000.0|2018-04-15 13:27:18|\n| 9999.0|2018-04-16 13:27:18|\n+-------+-------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=366","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=367"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558479051149_-1336600193","id":"20190522-005051_1167726115","dateCreated":"2019-05-22T00:50:51+0200","dateStarted":"2019-05-30T20:39:15+0200","dateFinished":"2019-05-30T20:39:15+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"text":"%spark2.pyspark\n# Create a \"seven day\" Window from seven days previous to the current day (row), using previous casting of timestamp to long (number of seconds).\n# remember a start value of \"-1\" means one off before the current row, and we are taking the timestamp as a long and comparing it to the rangeBetween amount of time.\n# remember an end value of 0 is the current row.\n\nwindowSpec = Window.orderBy(func.col(\"timestampGMT\").cast('long')).rangeBetween(-days(7), 0)","user":"admin","dateUpdated":"2019-05-30T20:39:17+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558633295545_-1981875225","id":"20190523-194135_37507681","dateCreated":"2019-05-23T19:41:35+0200","dateStarted":"2019-05-30T20:39:18+0200","dateFinished":"2019-05-30T20:39:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"%spark2.pyspark\n# Note the OVER clause added to AVG(), to define a windowing column.\ndf2 = df.withColumn('rolling_seven_day_average', func.avg(\"dollars\").over(windowSpec)) ","user":"admin","dateUpdated":"2019-05-30T20:39:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558633331034_1684729744","id":"20190523-194211_549915740","dateCreated":"2019-05-23T19:42:11+0200","dateStarted":"2019-05-30T20:39:21+0200","dateFinished":"2019-05-30T20:39:21+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"%spark2.pyspark\ndf2.show()","user":"admin","dateUpdated":"2019-05-30T20:39:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-------------------+-------------------------+\n|dollars|       timestampGMT|rolling_seven_day_average|\n+-------+-------------------+-------------------------+\n|   17.0|2018-03-10 16:27:18|                     17.0|\n|   13.0|2018-03-11 13:27:18|                     15.0|\n|   25.0|2018-03-12 12:27:18|       18.333333333333332|\n|   20.0|2018-03-13 16:27:18|                    18.75|\n|   56.0|2018-03-14 13:27:18|                     26.2|\n|   99.0|2018-03-15 12:27:18|       38.333333333333336|\n|  156.0|2018-03-22 12:27:18|                    127.5|\n|  122.0|2018-03-31 13:27:18|                    122.0|\n| 7000.0|2018-04-15 13:27:18|                   7000.0|\n| 9999.0|2018-04-16 13:27:18|                   8499.5|\n+-------+-------------------+-------------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=368"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558633345894_1546749903","id":"20190523-194225_705750735","dateCreated":"2019-05-23T19:42:25+0200","dateStarted":"2019-05-30T20:39:24+0200","dateFinished":"2019-05-30T20:39:24+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:180"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\n\ndf = sc.parallelize([(4, 9.0), (3, 7.0), (2, 3.0), (1, 5.0)]).toDF([\"id\", \"num\"]).orderBy(\"id\")\ndf.show()\nw = Window().partitionBy().orderBy(col(\"id\"))\n#Sliding into the past\ndf.select(\"*\", lag(\"num\", 1).over(w).alias(\"usage00\")).show()\n#Sliding into the future\ndf.select(\"*\", lead(\"num\", 1).over(w).alias(\"future00\")).show()","user":"admin","dateUpdated":"2019-06-01T00:56:00+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---+\n| id|num|\n+---+---+\n|  1|5.0|\n|  2|3.0|\n|  3|7.0|\n|  4|9.0|\n+---+---+\n\n+---+---+-------+\n| id|num|usage00|\n+---+---+-------+\n|  1|5.0|   null|\n|  2|3.0|    5.0|\n|  3|7.0|    3.0|\n|  4|9.0|    7.0|\n+---+---+-------+\n\n+---+---+--------+\n| id|num|future00|\n+---+---+--------+\n|  1|5.0|     3.0|\n|  2|3.0|     7.0|\n|  3|7.0|     9.0|\n|  4|9.0|    null|\n+---+---+--------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=577","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=578","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=579","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=580","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=581","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=582"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559249585195_1713955707","id":"20190530-225305_1762597926","dateCreated":"2019-05-30T22:53:05+0200","dateStarted":"2019-06-01T00:56:00+0200","dateFinished":"2019-06-01T00:56:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:181"},{"text":"%md\n#### Interpolation of weather data ####\n> <sup>When a row has less than 4 empty values than use linear interpolation otherwise use last valid value.</sup>\n","user":"admin","dateUpdated":"2019-06-02T14:50:37+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Interpolation of weather data</h4>\n<blockquote><p><sup>When a row has less than 4 empty values than use linear interpolation otherwise use last valid value.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559180917959_-573575428","id":"20190530-034837_360110099","dateCreated":"2019-05-30T03:48:37+0200","dateStarted":"2019-06-02T14:50:37+0200","dateFinished":"2019-06-02T14:50:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:182"},{"text":"%spark2.pyspark\nimport pandas as pd \npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\n\n#Set x for past\nx = 24\n\n#generate weather data frame\ndf_weather = generateWeatherDF(x)\ndf_weather = df_weather.drop(\"Hourly Weather\")\n# Since only 1562 rows, we can convert into a pandas df\ndf = df_weather.toPandas()  \n\ndef replacerNaN(p):\n    \"\"\"Replaces NaN\"\"\"\n    # interpolation does not work good for larger gaps\n    if (p.isnull().sum(axis=0) >= 4):\n        p.fillna(method=\"ffill\", inplace=True) #propagate last valid observation forward to next valid \n    return p\n    \ndf = df.apply(lambda x: replacerNaN(x), axis=1)\n\n#linear interpolation of remaining NaNs and round up\ndf = df.round(2)\ndf.interpolate(inplace=True)\n#df[df.isnull().any(axis=1)]\n\n\n#substitute hourly summaries with number values\ndef replacerText(p):\n    \"\"\"Replaces summaries\"\"\" \n    cleanup_nums =  {p.name: { \"clear-day\": 1, \"partly-cloudy-day\": 2, \"cloudy\":3, \"wind\":4, \"rain\": 5, \"fog\": 6, \"snow\": 7, \"partly-cloudy-night\": 8, \"clear-night\": 9, \"sleet\": 10} }\n    df.replace(cleanup_nums, inplace=True)\n    return df[p.name]\n\ndf = df.apply(lambda column: replacerText(column) if \"sum\" in column.name else column, axis=0)\n\nprint(df.iloc[1535])\n\n#convert back to pyspark df \ndf_weather = spark.createDataFrame(df)\ndf_weather.count()","user":"admin","dateUpdated":"2019-06-11T02:19:58+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Apparent Temperature (Avg)               50.315\nDaily Weather                 partly-cloudy-day\nHumidity                                   0.83\nStart Date                           2019-03-19\nWindspeed                                  6.26\nhum00                                      0.84\nhum01                                      0.84\nhum02                                      0.87\nhum03                                       0.9\nhum04                                      0.91\nhum05                                      0.92\nhum06                                      0.93\nhum07                                      0.91\nhum08                                      0.88\nhum09                                      0.85\nhum10                                      0.82\nhum11                                      0.75\nhum12                                      0.71\nhum13                                      0.67\nhum14                                      0.69\nhum15                                       0.7\nhum16                                      0.79\nhum17                                      0.77\nhum18                                      0.83\nhum19                                      0.85\nhum20                                      0.87\nhum21                                      0.87\nhum22                                      0.87\nhum23                                      0.87\nsum00                                         8\nsum01                                         8\nsum02                                         8\nsum03                                         8\nsum04                                         8\nsum05                                         8\nsum06                                         2\nsum07                                         2\nsum08                                         2\nsum09                                         2\nsum10                                         2\nsum11                                         2\nsum12                                         1\nsum13                                         2\nsum14                                         2\nsum15                                         2\nsum16                                         2\nsum17                                         2\nsum18                                         8\nsum19                                         8\nsum20                                         8\nsum21                                         8\nsum22                                         8\nsum23                                         8\ntemp00                                    42.53\ntemp01                                    42.42\ntemp02                                    42.77\ntemp03                                    44.41\ntemp04                                    42.88\ntemp05                                    43.54\ntemp06                                    42.23\ntemp07                                    43.67\ntemp08                                    45.55\ntemp09                                    50.04\ntemp10                                    51.89\ntemp11                                     53.1\ntemp12                                    53.94\ntemp13                                    55.21\ntemp14                                    54.68\ntemp15                                    54.35\ntemp16                                    52.69\ntemp17                                    52.46\ntemp18                                    51.13\ntemp19                                    50.67\ntemp20                                    46.45\ntemp21                                    46.53\ntemp22                                    46.17\ntemp23                                    46.17\nwind00                                     3.71\nwind01                                     3.48\nwind02                                     3.59\nwind03                                     2.82\nwind04                                     3.03\nwind05                                     3.37\nwind06                                     4.72\nwind07                                     5.51\nwind08                                     5.59\nwind09                                     5.93\nwind10                                      6.1\nwind11                                     6.96\nwind12                                     7.66\nwind13                                     8.07\nwind14                                     8.81\nwind15                                     9.14\nwind16                                     9.86\nwind17                                     9.21\nwind18                                     8.27\nwind19                                     7.97\nwind20                                     7.42\nwind21                                     7.25\nwind22                                     7.86\nwind23                                     7.86\nName: 1535, dtype: object\n1562\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1214","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1215","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1216"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558633351709_758859663","id":"20190523-194231_491425071","dateCreated":"2019-05-23T19:42:31+0200","dateStarted":"2019-06-11T02:19:58+0200","dateFinished":"2019-06-11T02:20:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:183"},{"text":"%md\n#### Each hour one line ####\n> <sup>Transform the `df_usage` dataframe so that each row represents one rounded hour.</sup>","user":"admin","dateUpdated":"2019-06-11T02:20:03+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Each hour one line</h4>\n<blockquote><p><sup>Transform the <code>df_usage</code> dataframe so that each row represents one rounded hour.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559181048483_-79285113","id":"20190530-035048_1967219093","dateCreated":"2019-05-30T03:50:48+0200","dateStarted":"2019-06-11T02:20:03+0200","dateFinished":"2019-06-11T02:20:03+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"text":"%spark2.pyspark\n#Previous solution with pivot tables\ntest = generateRentalDF(14)\ntest = test.groupby(test.Datum).pivot(\"Rounded Hour\").count().orderBy(\"Datum\")\ntest = test.na.fill(0)\ntest = test.alias(\"t\").select('t.*', unix_timestamp('Datum', \"dd/MM/yyyy\").cast(TimestampType()).alias(\"Timestamp\")).orderBy(\"Timestamp\")\ntest.show()","user":"admin","dateUpdated":"2019-06-11T02:20:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\n|     Datum|null|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23|          Timestamp|\n+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\n|04/01/2015|   0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  3|  0|  1|  2|  3|  6|  2|  1|  1|  5|  3|  1|  1|  4|2015-01-04 00:00:00|\n|05/01/2015|   0|  0|  0|  0|  0|  0|  1| 13| 80|143|  4|  3|  7|  4|  0|  1|  3|  3| 10|  5|  7|  2|  0|  0|  0|2015-01-05 00:00:00|\n|06/01/2015|   0|  1|  0|  0|  0|  0|  1| 15| 73|136| 25|  2|  2|  0|  3|  1|  8|  2|  6|  4|  3|  0|  3|  1|  0|2015-01-06 00:00:00|\n|07/01/2015|   0|  1|  0|  0|  0|  0|  0| 14| 83|132|  8|  5|  7|  2|  0|  5|  2|  2|  5|  6|  2|  4|  1|  1|  1|2015-01-07 00:00:00|\n|08/01/2015|   0|  0|  0|  0|  0|  0|  0|  7| 49| 45| 16|  4|  6|  3|  5|  2|  4|  3|  4|  3|  2|  3|  2|  3|  3|2015-01-08 00:00:00|\n|09/01/2015|   0|  1|  0|  0|  0|  0|  0| 11| 70|113| 42|  2|  2|  4|  5|  2|  2|  3|  8|  3|  4|  4|  3|  2|  2|2015-01-09 00:00:00|\n|10/01/2015|   0|  1|  2|  2|  0|  0|  0|  2|  1|  1|  3|  4|  5|  5| 10|  1|  4|  4| 11|  2|  4|  6|  1|  0|  0|2015-01-10 00:00:00|\n|11/01/2015|   0|  1|  0|  0|  3|  0|  0|  0|  1|  1|  9|  0|  1|  3|  4|  5|  4|  7|  1|  2|  2|  1|  3|  2|  5|2015-01-11 00:00:00|\n|12/01/2015|   0|  1|  0|  0|  0|  0|  1| 11| 69|141|  3|  7|  5|  5|  2|  1|  2|  2|  0|  2|  1|  4|  1|  0|  0|2015-01-12 00:00:00|\n|13/01/2015|   0|  0|  0|  0|  0|  0|  0| 18| 64|137| 37| 11|  3| 12|  2|  4|  4|  1|  5|  7|  3|  3|  3|  4|  1|2015-01-13 00:00:00|\n|14/01/2015|   0|  1|  0|  0|  0|  0|  0| 15| 79|133| 31| 10|  3|  4|  7|  7|  3|  3|  3|  7|  4|  4|  1|  1|  0|2015-01-14 00:00:00|\n|15/01/2015|   0|  0|  0|  0|  0|  0|  1| 14| 71|121| 16|  2|  2|  5|  5|  5|  2|  2|  6|  7|  6|  2|  0|  2|  0|2015-01-15 00:00:00|\n|16/01/2015|   0|  1|  0|  0|  0|  1|  0| 14| 83|130| 23|  9|  9|  8|  8|  1|  4|  2|  5|  3|  5|  3|  2|  1|  1|2015-01-16 00:00:00|\n|17/01/2015|   0|  5|  1|  4|  0|  0|  1|  1|  3|  0|  5|  4|  2|  1|  4|  4|  1|  2|  2|  4| 12|  1|  2|  2|  1|2015-01-17 00:00:00|\n|18/01/2015|   0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  1|  2|  6|  5|  4|  2|  6|  2|  2|  5|  0|  3|  5|2015-01-18 00:00:00|\n|19/01/2015|   0|  2|  1|  0|  0|  0|  0| 15| 84|125| 15|  9|  5|  4|  5|  9|  3|  4|  4|  8|  2|  2|  3|  2|  0|2015-01-19 00:00:00|\n|20/01/2015|   0|  0|  0|  1|  0|  0|  0| 11| 78|122| 35|  3|  0|  7|  6|  3|  6|  5|  5|  7|  7|  1|  3|  0|  2|2015-01-20 00:00:00|\n|21/01/2015|   0|  1|  0|  0|  0|  4|  0|  9| 72|122| 40|  4|  2|  6|  0|  4|  5|  5|  8|  4|  5|  3|  1|  3|  0|2015-01-21 00:00:00|\n|22/01/2015|   0|  2|  0|  0|  0|  0|  0| 11| 85|128|  6| 11|  2|  2|  4|  5|  2|  0|  4|  3|  3|  3|  2|  3|  1|2015-01-22 00:00:00|\n|23/01/2015|   0|  3|  0|  0|  0|  0|  0| 11| 81| 97| 35| 13|  8|  0|  2|  1|  6|  3|  9|  6|  1|  1|  3|  1|  1|2015-01-23 00:00:00|\n+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1217","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1218","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1219","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1220","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1221","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1222"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559181312974_-1224841101","id":"20190530-035512_256580383","dateCreated":"2019-05-30T03:55:12+0200","dateStarted":"2019-06-11T02:20:04+0200","dateFinished":"2019-06-11T02:23:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import concat, col, lit\n\n#Give me all station records from x\ntest2 = generateRentalDF(14)\ntest2 = test2.dropna()\n#Concatenate daily date column with rounded hour column\ntest2 = test2.withColumn(\"New Datum\", concat(col(\"Datum\"), lit(\" \"), col(\"Rounded Hour\")))\ntest2 = test2.alias(\"t\").select('t.*', unix_timestamp('New Datum', \"dd/MM/yyyy HH\").cast(TimestampType()).alias(\"New Date\")).orderBy(\"New Date\") #TODO:// zu viele order bys, später noch entfernen\ntest2.show()","user":"admin","dateUpdated":"2019-06-11T02:21:08+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\n|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|           New Date|\n+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\n|04/01/2015 07:44|             14|2015-01-04 07:44:00|           7|04/01/2015|07:44| 04/01/2015 7|2015-01-04 07:00:00|\n|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 10:04|             14|2015-01-04 10:04:00|          10|04/01/2015|10:04|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 12:50|             14|2015-01-04 12:50:00|          12|04/01/2015|12:50|04/01/2015 12|2015-01-04 12:00:00|\n|04/01/2015 13:42|             14|2015-01-04 13:42:00|          13|04/01/2015|13:42|04/01/2015 13|2015-01-04 13:00:00|\n|04/01/2015 13:09|             14|2015-01-04 13:09:00|          13|04/01/2015|13:09|04/01/2015 13|2015-01-04 13:00:00|\n|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 14:17|             14|2015-01-04 14:17:00|          14|04/01/2015|14:17|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:48|             14|2015-01-04 15:48:00|          15|04/01/2015|15:48|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:37|             14|2015-01-04 15:37:00|          15|04/01/2015|15:37|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:45|             14|2015-01-04 15:45:00|          15|04/01/2015|15:45|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:41|             14|2015-01-04 15:41:00|          15|04/01/2015|15:41|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:06|             14|2015-01-04 15:06:00|          15|04/01/2015|15:06|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 16:37|             14|2015-01-04 16:37:00|          16|04/01/2015|16:37|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 16:32|             14|2015-01-04 16:32:00|          16|04/01/2015|16:32|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 17:59|             14|2015-01-04 17:59:00|          17|04/01/2015|17:59|04/01/2015 17|2015-01-04 17:00:00|\n|04/01/2015 18:11|             14|2015-01-04 18:11:00|          18|04/01/2015|18:11|04/01/2015 18|2015-01-04 18:00:00|\n+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1223","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1224","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1225","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1226"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229521905_1962831237","id":"20190530-171841_2118270594","dateCreated":"2019-05-30T17:18:41+0200","dateStarted":"2019-06-11T02:21:08+0200","dateFinished":"2019-06-11T02:24:45+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"text":"%spark2.pyspark\n#count usage per hour and join back\ntest3 = test2.groupBy(\"New Date\").count().withColumnRenamed('count', 'Current Usage')\ntest2 = test2.join(test3, [\"New Date\"], \"leftouter\")\ntest2 = test2.drop_duplicates([\"New Date\"]).orderBy(\"New Date\")\ntest2.printSchema()\ntest2.show()\n","user":"admin","dateUpdated":"2019-06-11T02:21:11+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- New Date: timestamp (nullable = true)\n |-- Start Date: string (nullable = true)\n |-- StartStation Id: integer (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Rounded Hour: integer (nullable = true)\n |-- Datum: string (nullable = true)\n |-- Hour: string (nullable = true)\n |-- New Datum: string (nullable = true)\n |-- Current Usage: long (nullable = true)\n\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|           New Date|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|Current Usage|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|2015-01-04 07:00:00|04/01/2015 07:44|             14|2015-01-04 07:44:00|           7|04/01/2015|07:44| 04/01/2015 7|            1|\n|2015-01-04 10:00:00|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|            3|\n|2015-01-04 12:00:00|04/01/2015 12:50|             14|2015-01-04 12:50:00|          12|04/01/2015|12:50|04/01/2015 12|            1|\n|2015-01-04 13:00:00|04/01/2015 13:09|             14|2015-01-04 13:09:00|          13|04/01/2015|13:09|04/01/2015 13|            2|\n|2015-01-04 14:00:00|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|            3|\n|2015-01-04 15:00:00|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|            6|\n|2015-01-04 16:00:00|04/01/2015 16:32|             14|2015-01-04 16:32:00|          16|04/01/2015|16:32|04/01/2015 16|            2|\n|2015-01-04 17:00:00|04/01/2015 17:59|             14|2015-01-04 17:59:00|          17|04/01/2015|17:59|04/01/2015 17|            1|\n|2015-01-04 18:00:00|04/01/2015 18:11|             14|2015-01-04 18:11:00|          18|04/01/2015|18:11|04/01/2015 18|            1|\n|2015-01-04 19:00:00|04/01/2015 19:22|             14|2015-01-04 19:22:00|          19|04/01/2015|19:22|04/01/2015 19|            5|\n|2015-01-04 20:00:00|04/01/2015 20:01|             14|2015-01-04 20:01:00|          20|04/01/2015|20:01|04/01/2015 20|            3|\n|2015-01-04 21:00:00|04/01/2015 21:52|             14|2015-01-04 21:52:00|          21|04/01/2015|21:52|04/01/2015 21|            1|\n|2015-01-04 22:00:00|04/01/2015 22:49|             14|2015-01-04 22:49:00|          22|04/01/2015|22:49|04/01/2015 22|            1|\n|2015-01-04 23:00:00|04/01/2015 23:11|             14|2015-01-04 23:11:00|          23|04/01/2015|23:11|04/01/2015 23|            4|\n|2015-01-05 05:00:00|05/01/2015 05:06|             14|2015-01-05 05:06:00|           5|05/01/2015|05:06| 05/01/2015 5|            1|\n|2015-01-05 06:00:00|05/01/2015 06:18|             14|2015-01-05 06:18:00|           6|05/01/2015|06:18| 05/01/2015 6|           13|\n|2015-01-05 07:00:00|05/01/2015 07:00|             14|2015-01-05 07:00:00|           7|05/01/2015|07:00| 05/01/2015 7|           80|\n|2015-01-05 08:00:00|05/01/2015 08:01|             14|2015-01-05 08:01:00|           8|05/01/2015|08:01| 05/01/2015 8|          143|\n|2015-01-05 09:00:00|05/01/2015 09:12|             14|2015-01-05 09:12:00|           9|05/01/2015|09:12| 05/01/2015 9|            4|\n|2015-01-05 10:00:00|05/01/2015 10:41|             14|2015-01-05 10:41:00|          10|05/01/2015|10:41|05/01/2015 10|            3|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1227","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1228","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1229"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229458509_228380051","id":"20190530-171738_373187699","dateCreated":"2019-05-30T17:17:38+0200","dateStarted":"2019-06-11T02:23:13+0200","dateFinished":"2019-06-11T02:30:48+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"%spark2.pyspark\n#fill empty or missing hours with 0, supplement missing hours (e.g. 1, 8 -> supplement 2,3,4...)\nfrom pyspark.sql.functions import col, min as min_, max as max_\n\nstep = 60 * 60\n\nminp, maxp = test2.select(\n    min_(\"New Date\").cast(\"long\"), max_(\"New Date\").cast(\"long\")\n).first()\n\nreference = spark.range(\n    (minp / step) * step, ((maxp / step) + 1) * step, step\n).select(col(\"id\").cast(\"timestamp\").alias(\"New Date\"))\n\nreference = reference.join(test2, [\"New Date\"], \"leftouter\").orderBy(\"New Date\")\nreference.show()","user":"admin","dateUpdated":"2019-06-11T02:21:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|           New Date|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|Current Usage|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|2015-01-04 07:00:00|04/01/2015 07:44|             14|2015-01-04 07:44:00|           7|04/01/2015|07:44| 04/01/2015 7|            1|\n|2015-01-04 08:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 09:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 10:00:00|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|            3|\n|2015-01-04 11:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 12:00:00|04/01/2015 12:50|             14|2015-01-04 12:50:00|          12|04/01/2015|12:50|04/01/2015 12|            1|\n|2015-01-04 13:00:00|04/01/2015 13:09|             14|2015-01-04 13:09:00|          13|04/01/2015|13:09|04/01/2015 13|            2|\n|2015-01-04 14:00:00|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|            3|\n|2015-01-04 15:00:00|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|            6|\n|2015-01-04 16:00:00|04/01/2015 16:32|             14|2015-01-04 16:32:00|          16|04/01/2015|16:32|04/01/2015 16|            2|\n|2015-01-04 17:00:00|04/01/2015 17:59|             14|2015-01-04 17:59:00|          17|04/01/2015|17:59|04/01/2015 17|            1|\n|2015-01-04 18:00:00|04/01/2015 18:11|             14|2015-01-04 18:11:00|          18|04/01/2015|18:11|04/01/2015 18|            1|\n|2015-01-04 19:00:00|04/01/2015 19:22|             14|2015-01-04 19:22:00|          19|04/01/2015|19:22|04/01/2015 19|            5|\n|2015-01-04 20:00:00|04/01/2015 20:01|             14|2015-01-04 20:01:00|          20|04/01/2015|20:01|04/01/2015 20|            3|\n|2015-01-04 21:00:00|04/01/2015 21:52|             14|2015-01-04 21:52:00|          21|04/01/2015|21:52|04/01/2015 21|            1|\n|2015-01-04 22:00:00|04/01/2015 22:49|             14|2015-01-04 22:49:00|          22|04/01/2015|22:49|04/01/2015 22|            1|\n|2015-01-04 23:00:00|04/01/2015 23:11|             14|2015-01-04 23:11:00|          23|04/01/2015|23:11|04/01/2015 23|            4|\n|2015-01-05 00:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-05 01:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-05 02:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1230","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1231","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1232","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1233","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1234","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1235","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1236"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229827411_1189361632","id":"20190530-172347_798841573","dateCreated":"2019-05-30T17:23:47+0200","dateStarted":"2019-06-11T02:24:46+0200","dateFinished":"2019-06-11T02:40:28+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"%spark2.pyspark\n# Remove unused columns\ncolumns_to_drop = [\"Start Date\", \n                   \"Timestamp\",\n                   \"Rounded Hour\",\n                   \"Datum\",\n                   \"Hour\",\n                   \"New Datum\"]\n                   \nreference = reference.drop(*columns_to_drop)\n# fill null values\nreference = reference.na.fill(14, subset=['StartStation Id'])\nreference = reference.na.fill(0, subset=['Current Usage'])\nreference.show()","user":"admin","dateUpdated":"2019-06-11T02:22:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+---------------+-------------+\n|           New Date|StartStation Id|Current Usage|\n+-------------------+---------------+-------------+\n|2015-01-04 07:00:00|             14|            1|\n|2015-01-04 08:00:00|             14|            0|\n|2015-01-04 09:00:00|             14|            0|\n|2015-01-04 10:00:00|             14|            3|\n|2015-01-04 11:00:00|             14|            0|\n|2015-01-04 12:00:00|             14|            1|\n|2015-01-04 13:00:00|             14|            2|\n|2015-01-04 14:00:00|             14|            3|\n|2015-01-04 15:00:00|             14|            6|\n|2015-01-04 16:00:00|             14|            2|\n|2015-01-04 17:00:00|             14|            1|\n|2015-01-04 18:00:00|             14|            1|\n|2015-01-04 19:00:00|             14|            5|\n|2015-01-04 20:00:00|             14|            3|\n|2015-01-04 21:00:00|             14|            1|\n|2015-01-04 22:00:00|             14|            1|\n|2015-01-04 23:00:00|             14|            4|\n|2015-01-05 00:00:00|             14|            0|\n|2015-01-05 01:00:00|             14|            0|\n|2015-01-05 02:00:00|             14|            0|\n+-------------------+---------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1237","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1238","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1239","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1240"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559237800485_1829515935","id":"20190530-193640_1766444645","dateCreated":"2019-05-30T19:36:40+0200","dateStarted":"2019-06-11T02:30:49+0200","dateFinished":"2019-06-11T02:46:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"text":"%md\n#### Sliding Window ####\n> <sup>Use a sliding window on a hourly base with dynamic size to go from current hour to `past` and to `predict` the next 24 hours from the current hour.</sup>","user":"admin","dateUpdated":"2019-06-11T02:22:46+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Sliding Window</h4>\n<blockquote><p><sup>Use a sliding window on a hourly base with dynamic size to go from current hour to <code>past</code> and to <code>predict</code> the next 24 hours from the current hour.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559236207640_-633690624","id":"20190530-191007_2070186282","dateCreated":"2019-05-30T19:10:07+0200","dateStarted":"2019-06-11T02:22:46+0200","dateFinished":"2019-06-11T02:22:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\n\n#set past\nx = 48 \n#set future\nf = 24 \n\n# go x hours back\nfor i in range(x):\n    w = Window().partitionBy().orderBy(col(\"New Date\"))\n    reference = reference.select(\"*\", lag(\"Current Usage\", i+1).over(w).alias(\"Usage\"+`i`))\n\n# Aggregate values\nif x >= 48:\n    marksColumns = []\n    z = 24\n    r = x - z\n    j = x\n    t = 1\n    if r % 6 == 0:\n        for i in range(r):\n            marksColumns.append(col('Usage'+`z`))\n            z = z + 1\n            if t % 6 == 0:\n                averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n                reference = reference.withColumn('Usage ' + `j` + ' Avg. over 6h', averageFunc)\n                j = j + 1\n                marksColumns = []\n            t = t + 1\n\n# go f hours future\nfor i in range(f):\n    w = Window().partitionBy().orderBy(col(\"New Date\"))\n    reference = reference.select(\"*\", lead(\"Current Usage\", i+1).over(w).alias(\"Future\"+`i`))\n\nreference.show()","user":"admin","dateUpdated":"2019-06-11T02:22:58+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------------------+---------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|           New Date|StartStation Id|Current Usage|Usage0|Usage1|Usage2|Usage3|Usage4|Usage5|Usage6|Usage7|Usage8|Usage9|Usage10|Usage11|Usage12|Usage13|Usage14|Usage15|Usage16|Usage17|Usage18|Usage19|Usage20|Usage21|Usage22|Usage23|Usage24|Usage25|Usage26|Usage27|Usage28|Usage29|Usage30|Usage31|Usage32|Usage33|Usage34|Usage35|Usage36|Usage37|Usage38|Usage39|Usage40|Usage41|Usage42|Usage43|Usage44|Usage45|Usage46|Usage47|Usage 48 Avg. over 6h|Usage 49 Avg. over 6h|Usage 50 Avg. over 6h|Usage 51 Avg. over 6h|Future0|Future1|Future2|Future3|Future4|Future5|Future6|Future7|Future8|Future9|Future10|Future11|Future12|Future13|Future14|Future15|Future16|Future17|Future18|Future19|Future20|Future21|Future22|Future23|\n+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------------------+---------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|2015-01-04 07:00:00|             14|            1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      0|      3|      0|      1|      2|      3|      6|      2|      1|       1|       5|       3|       1|       1|       4|       0|       0|       0|       0|       0|       1|      13|      80|\n|2015-01-04 08:00:00|             14|            0|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      3|      0|      1|      2|      3|      6|      2|      1|      1|       5|       3|       1|       1|       4|       0|       0|       0|       0|       0|       1|      13|      80|     143|\n|2015-01-04 09:00:00|             14|            0|     0|     1|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      3|      0|      1|      2|      3|      6|      2|      1|      1|      5|       3|       1|       1|       4|       0|       0|       0|       0|       0|       1|      13|      80|     143|       4|\n|2015-01-04 10:00:00|             14|            3|     0|     0|     1|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      1|      2|      3|      6|      2|      1|      1|      5|      3|       1|       1|       4|       0|       0|       0|       0|       0|       1|      13|      80|     143|       4|       3|\n|2015-01-04 11:00:00|             14|            0|     3|     0|     0|     1|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      1|      2|      3|      6|      2|      1|      1|      5|      3|      1|       1|       4|       0|       0|       0|       0|       0|       1|      13|      80|     143|       4|       3|       7|\n|2015-01-04 12:00:00|             14|            1|     0|     3|     0|     0|     1|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      2|      3|      6|      2|      1|      1|      5|      3|      1|      1|       4|       0|       0|       0|       0|       0|       1|      13|      80|     143|       4|       3|       7|       4|\n|2015-01-04 13:00:00|             14|            2|     1|     0|     3|     0|     0|     1|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      3|      6|      2|      1|      1|      5|      3|      1|      1|      4|       0|       0|       0|       0|       0|       1|      13|      80|     143|       4|       3|       7|       4|       0|\n|2015-01-04 14:00:00|             14|            3|     2|     1|     0|     3|     0|     0|     1|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      6|      2|      1|      1|      5|      3|      1|      1|      4|      0|       0|       0|       0|       0|       1|      13|      80|     143|       4|       3|       7|       4|       0|       1|\n|2015-01-04 15:00:00|             14|            6|     3|     2|     1|     0|     3|     0|     0|     1|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      2|      1|      1|      5|      3|      1|      1|      4|      0|      0|       0|       0|       0|       1|      13|      80|     143|       4|       3|       7|       4|       0|       1|       3|\n|2015-01-04 16:00:00|             14|            2|     6|     3|     2|     1|     0|     3|     0|     0|     1|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      1|      1|      5|      3|      1|      1|      4|      0|      0|      0|       0|       0|       1|      13|      80|     143|       4|       3|       7|       4|       0|       1|       3|       3|\n|2015-01-04 17:00:00|             14|            1|     2|     6|     3|     2|     1|     0|     3|     0|     0|     1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      1|      5|      3|      1|      1|      4|      0|      0|      0|      0|       0|       1|      13|      80|     143|       4|       3|       7|       4|       0|       1|       3|       3|      10|\n|2015-01-04 18:00:00|             14|            1|     1|     2|     6|     3|     2|     1|     0|     3|     0|     0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      5|      3|      1|      1|      4|      0|      0|      0|      0|      0|       1|      13|      80|     143|       4|       3|       7|       4|       0|       1|       3|       3|      10|       5|\n|2015-01-04 19:00:00|             14|            5|     1|     1|     2|     6|     3|     2|     1|     0|     3|     0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      3|      1|      1|      4|      0|      0|      0|      0|      0|      1|      13|      80|     143|       4|       3|       7|       4|       0|       1|       3|       3|      10|       5|       7|\n|2015-01-04 20:00:00|             14|            3|     5|     1|     1|     2|     6|     3|     2|     1|     0|     3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      1|      1|      4|      0|      0|      0|      0|      0|      1|     13|      80|     143|       4|       3|       7|       4|       0|       1|       3|       3|      10|       5|       7|       2|\n|2015-01-04 21:00:00|             14|            1|     3|     5|     1|     1|     2|     6|     3|     2|     1|     0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      1|      4|      0|      0|      0|      0|      0|      1|     13|     80|     143|       4|       3|       7|       4|       0|       1|       3|       3|      10|       5|       7|       2|       0|\n|2015-01-04 22:00:00|             14|            1|     1|     3|     5|     1|     1|     2|     6|     3|     2|     1|      0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      4|      0|      0|      0|      0|      0|      1|     13|     80|    143|       4|       3|       7|       4|       0|       1|       3|       3|      10|       5|       7|       2|       0|       0|\n|2015-01-04 23:00:00|             14|            4|     1|     1|     3|     5|     1|     1|     2|     6|     3|     2|      1|      0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      0|      0|      0|      0|      1|     13|     80|    143|      4|       3|       7|       4|       0|       1|       3|       3|      10|       5|       7|       2|       0|       0|       0|\n|2015-01-05 00:00:00|             14|            0|     4|     1|     1|     3|     5|     1|     1|     2|     6|     3|      2|      1|      0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      0|      0|      0|      1|     13|     80|    143|      4|      3|       7|       4|       0|       1|       3|       3|      10|       5|       7|       2|       0|       0|       0|       1|\n|2015-01-05 01:00:00|             14|            0|     0|     4|     1|     1|     3|     5|     1|     1|     2|     6|      3|      2|      1|      0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      0|      0|      1|     13|     80|    143|      4|      3|      7|       4|       0|       1|       3|       3|      10|       5|       7|       2|       0|       0|       0|       1|       0|\n|2015-01-05 02:00:00|             14|            0|     0|     0|     4|     1|     1|     3|     5|     1|     1|     2|      6|      3|      2|      1|      0|      3|      0|      0|      1|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|                 null|                 null|                 null|                 null|      0|      0|      1|     13|     80|    143|      4|      3|      7|      4|       0|       1|       3|       3|      10|       5|       7|       2|       0|       0|       0|       1|       0|       0|\n+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+---------------------+---------------------+---------------------+---------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1241","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1242","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1243","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1244","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1245"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559245155625_-582948985","id":"20190530-213915_977149296","dateCreated":"2019-05-30T21:39:15+0200","dateStarted":"2019-06-11T02:40:29+0200","dateFinished":"2019-06-11T02:52:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"%md\n<b>Problem</b>: Weather dataframe has totally different structure.\nIdea: Use the power of `explode` function in PySpark.","user":"admin","dateUpdated":"2019-06-11T02:23:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><b>Problem</b>: Weather dataframe has totally different structure.\n<br  />Idea: Use the power of <code>explode</code> function in PySpark.</p>\n"}]},"apps":[],"jobName":"paragraph_1559400109899_704791655","id":"20190601-164149_110811586","dateCreated":"2019-06-01T16:41:49+0200","dateStarted":"2019-06-11T02:23:04+0200","dateFinished":"2019-06-11T02:23:04+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%spark2.pyspark\nweather = generateWeatherDF(2)\nweather = weather.drop(*[\"Apparent Temperature (Avg)\", \"Daily Weather\", \"Hourly Weather\", \"Humidity\", \"Windspeed\"])\nweather.show()","user":"admin","dateUpdated":"2019-06-11T02:23:13+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+-----+-------------------+-------------------+------+------+------+------+\n|Start Date|hum00|hum01|              sum00|              sum01|temp00|temp01|wind00|wind01|\n+----------+-----+-----+-------------------+-------------------+------+------+------+------+\n|2015-01-04| 0.93| 0.94|partly-cloudy-night|partly-cloudy-night| 37.24| 36.35|  0.94|  0.56|\n|2015-01-05| 0.94| 0.95|partly-cloudy-night|             cloudy| 37.62| 37.97|  0.37|  0.18|\n|2015-01-06| 0.82| 0.83|             cloudy|             cloudy| 45.99| 45.87|  1.44|  1.58|\n|2015-01-07|  0.9| 0.91|        clear-night|        clear-night| 35.64| 35.42|  0.64|  0.64|\n|2015-01-08|  0.9|  0.9|partly-cloudy-night|partly-cloudy-night| 47.21|  50.5|  6.27|  5.51|\n|2015-01-09| 0.81| 0.79|partly-cloudy-night|partly-cloudy-night| 44.75| 46.14|  6.13|  5.73|\n|2015-01-10| 0.82| 0.85|partly-cloudy-night|             cloudy| 56.73| 55.95|  7.88|  8.55|\n|2015-01-11| 0.76| 0.79|        clear-night|        clear-night| 35.06| 35.14|  4.56|  4.27|\n|2015-01-12| 0.78| 0.74|partly-cloudy-night|partly-cloudy-night| 43.09| 43.76|  6.61|  6.71|\n|2015-01-13| 0.92| 0.92|               rain|               rain| 52.35| 52.47|  5.45|  6.51|\n|2015-01-14| 0.79| 0.74|partly-cloudy-night|partly-cloudy-night| 35.57| 35.51|  4.32|  6.12|\n|2015-01-15| 0.89| 0.88|partly-cloudy-night|partly-cloudy-night| 44.16| 45.37|  9.22|  9.56|\n|2015-01-16| 0.67| 0.65|        clear-night|        clear-night| 38.98| 39.24|  5.17|  5.45|\n|2015-01-17| 0.89| 0.89|        clear-night|        clear-night| 31.72| 31.44|  0.63|  1.15|\n|2015-01-18| 0.86| 0.86|        clear-night|partly-cloudy-night| 38.46| 38.16|  0.35|  0.32|\n|2015-01-19| 0.93| 0.93|partly-cloudy-night|partly-cloudy-night| 30.99| 31.57|  0.59|  0.66|\n|2015-01-20|  0.9|  0.9|        clear-night|        clear-night| 29.39| 28.92|  0.03|  0.03|\n|2015-01-21|  0.8| 0.83|partly-cloudy-night|partly-cloudy-night| 33.45| 32.72|  5.25|  5.17|\n|2015-01-22| 0.89|  0.9|partly-cloudy-night|partly-cloudy-night| 34.62| 34.05|  0.23|  0.34|\n|2015-01-23| 0.89| 0.89|        clear-night|        clear-night| 27.81| 27.44|  0.04|  0.05|\n+----------+-----+-----+-------------------+-------------------+------+------+------+------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1246","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1247"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559340647166_-2019231479","id":"20190601-001047_2064169923","dateCreated":"2019-06-01T00:10:47+0200","dateStarted":"2019-06-11T02:46:36+0200","dateFinished":"2019-06-11T02:53:00+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"%spark2.pyspark\ndf_weather.show()","user":"admin","dateUpdated":"2019-06-11T02:23:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------+-------------------+--------+----------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n|Apparent Temperature (Avg)|      Daily Weather|Humidity|Start Date|Windspeed|hum00|hum01|hum02|hum03|hum04|hum05|hum06|hum07|hum08|hum09|hum10|hum11|hum12|hum13|hum14|hum15|hum16|hum17|hum18|hum19|hum20|hum21|hum22|hum23|sum00|sum01|sum02|sum03|sum04|sum05|sum06|sum07|sum08|sum09|sum10|sum11|sum12|sum13|sum14|sum15|sum16|sum17|sum18|sum19|sum20|sum21|sum22|sum23|temp00|temp01|temp02|temp03|temp04|temp05|temp06|temp07|temp08|temp09|temp10|temp11|temp12|temp13|temp14|temp15|temp16|temp17|temp18|temp19|temp20|temp21|temp22|temp23|wind00|wind01|wind02|wind03|wind04|wind05|wind06|wind07|wind08|wind09|wind10|wind11|wind12|wind13|wind14|wind15|wind16|wind17|wind18|wind19|wind20|wind21|wind22|wind23|\n+--------------------------+-------------------+--------+----------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n|                    36.295|                fog|    0.94|2015-01-04|     0.55| 0.93| 0.94| 0.94| 0.95| 0.94| 0.94| 0.94| 0.96| 0.95| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.94| 0.95| 0.94|    8|    8|    6|    9|    6|    6|    6|    6|    6|    6|    6|    6|    6|    6|    2|    2|    8|    6|    6|    6|    6|    6|    8|    3| 37.24| 36.35| 35.61| 34.36| 33.43| 33.14| 32.86| 32.98| 33.09| 33.65| 34.03| 34.91| 35.65| 36.23| 36.49| 36.57| 36.18| 36.31| 36.42| 36.32| 36.02| 36.29| 36.56|  37.1|  0.94|  0.56|  0.34|  0.05|  0.08|  0.18|  0.28|  0.03|  0.14|  0.34|  1.04|   1.3|  1.02|  1.53|  1.36|   0.8|   1.6|  0.84|  0.94|  1.04|   0.8|  0.41|  0.19|  0.33|\n|                     46.74|  partly-cloudy-day|    0.88|2015-01-05|     1.59| 0.94| 0.95| 0.94| 0.94| 0.94| 0.93| 0.93| 0.92| 0.92| 0.92| 0.91| 0.92| 0.92|  0.9| 0.88| 0.85| 0.84| 0.82| 0.82| 0.81|  0.8|  0.8| 0.81| 0.81|    8|    3|    3|    3|    3|    3|    3|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    8|    8|    8|    8|    8|    8|    3| 37.62| 37.97| 38.53| 38.94| 39.62| 40.46| 40.91| 41.63| 42.35| 43.31| 45.06|  46.4| 47.68| 48.61| 49.24| 48.56| 48.56| 48.03| 47.73| 46.84| 46.31| 46.75| 46.45|  46.2|  0.37|  0.18|  0.24|   0.9|  0.88|   0.9|  0.95|  0.75|  1.16|  1.13|   1.5|  1.75|  2.15|   2.4|  2.32|  3.17|  2.94|  2.11|  2.72|  3.02|  3.08|   2.5|  1.44|  0.94|\n|         42.15000000000001|  partly-cloudy-day|    0.86|2015-01-06|     2.07| 0.82| 0.83| 0.82| 0.82| 0.82| 0.84| 0.86| 0.87| 0.89| 0.91| 0.92| 0.91| 0.91| 0.92| 0.88| 0.81| 0.79|  0.8| 0.83| 0.84| 0.85| 0.86| 0.88| 0.89|    3|    3|    8|    3|    3|    3|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    9|    9|    9|    9|    9|    9|    9| 45.99| 45.87| 45.91| 45.74|  45.9| 45.02| 44.24| 44.79| 44.87| 44.55|  45.8| 47.42| 48.88| 48.41| 44.49| 43.39| 42.65| 43.27| 41.52| 40.24| 39.25| 38.78| 37.24| 36.36|  1.44|  1.58|  2.83|  1.49|  2.93|  3.11|  4.13|  3.81|  3.96|   4.7|  4.08|  4.05|  3.89|  3.51|   6.8|  5.92|  4.48|  2.96|   1.2|   2.1|  1.44|  1.07|  0.78|  0.82|\n|                     45.45|partly-cloudy-night|    0.86|2015-01-07|     4.13|  0.9| 0.91|  0.9|  0.9|  0.9| 0.91|  0.9| 0.91| 0.91| 0.91| 0.89| 0.88| 0.85| 0.82| 0.77| 0.75| 0.78|  0.8|  0.8| 0.81| 0.83| 0.86| 0.86| 0.88|    9|    9|    9|    8|    9|    8|    8|    9|    8|    2|    2|    2|    1|    2|    2|    2|    2|    8|    8|    8|    8|    8|    8|    8| 35.64| 35.42| 35.68| 35.62| 36.01| 36.09| 36.62| 36.86| 35.72| 37.78| 40.39| 42.98| 43.98| 45.01| 44.83| 44.71|  44.8| 44.61| 44.84| 45.14| 45.76| 45.96| 46.44| 46.91|  0.64|  0.64|  0.71|  1.01|  1.05|  0.78|  0.92|  1.76|  3.18|  3.37|  4.03|  3.77|  5.25|  5.86|  6.98|  7.11|  6.14|  6.69|  6.93|  7.37|  6.65|  6.57|   6.4|  6.38|\n|                      46.2|               rain|    0.87|2015-01-08|      3.6|  0.9|  0.9|  0.9| 0.91| 0.92| 0.91| 0.87| 0.84| 0.88| 0.91| 0.92| 0.92| 0.92|  0.9| 0.85| 0.81| 0.82| 0.84| 0.84| 0.83| 0.83| 0.84| 0.83| 0.82|    8|    8|    8|    8|    8|    8|    8|    8|    8|    5|    5|    5|    5|    2|    2|    1|    2|    9|    8|    8|    8|    9|    8|    9| 47.21|  50.5| 51.44| 51.95| 52.21|  52.1| 50.83| 50.15| 48.41| 47.67| 47.24| 45.31| 46.36| 45.61| 45.97| 46.72| 46.14| 45.69| 43.24|  42.5| 42.27| 42.37| 42.25| 43.33|  6.27|  5.51|  4.79|  5.46|  4.68|  3.94|  4.21|  1.84|  3.07|  1.29|  2.45|  3.47|  2.47|  3.73|   5.0|   4.0|  3.16|  2.68|  4.36|  3.87|  3.36|  3.96|  5.14|   5.5|\n|                    56.085|  partly-cloudy-day|    0.81|2015-01-09|     7.43| 0.81| 0.79| 0.84| 0.86| 0.88| 0.88| 0.88| 0.87| 0.86| 0.82|  0.8| 0.77| 0.75| 0.72|  0.7| 0.72|  0.8| 0.87| 0.87| 0.87| 0.81| 0.78| 0.79| 0.79|    8|    8|    8|    8|    8|    8|    8|    8|    8|    2|    1|    2|    1|    1|    2|    2|    2|    8|    8|    8|    8|    8|    8|    8| 44.75| 46.14| 45.16|  46.1| 50.17| 51.14| 51.85| 52.52| 52.87| 52.48| 52.44| 53.87| 54.43| 54.53| 54.98| 54.89| 54.28| 55.07|  56.9| 58.08| 58.72| 58.13| 57.45| 57.07|  6.13|  5.73|  7.31|   7.9|  7.47|  7.07|  6.52|  8.01|  6.99|  7.07|  5.72|  6.92|  6.55|  6.29|  7.23|  7.54|  7.68|  9.08|  7.95|  8.69| 10.44|  9.45|  9.26|  9.22|\n|                    44.615|  partly-cloudy-day|    0.74|2015-01-10|     7.05| 0.82| 0.85| 0.84| 0.81| 0.83| 0.85| 0.85| 0.87| 0.84| 0.85| 0.85| 0.79| 0.73| 0.64| 0.58| 0.57| 0.56| 0.59| 0.61| 0.65| 0.66| 0.69| 0.72| 0.74|    8|    3|    3|    8|    8|    8|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    1|    9|    9|    9|    8|    8|    9|    9| 56.73| 55.95| 55.45|  55.1|  54.6| 54.09| 54.25|  54.3| 55.06| 55.13| 55.54| 55.22| 51.64| 51.04| 45.76| 44.13| 42.55| 41.58| 40.46| 38.72| 38.13| 36.96| 36.83| 35.67|  7.88|  8.55|  8.57|  8.92|  8.83|  8.79|  8.48|  9.49| 10.33| 10.35| 10.44|  8.63|  8.39|  8.67|  9.39|  7.54|  6.42|  5.34|  4.71|  4.84|  4.57|  4.79|  3.91|  4.59|\n|                    43.385|partly-cloudy-night|    0.74|2015-01-11|     5.44| 0.76| 0.79|  0.8| 0.81|  0.8| 0.77| 0.76| 0.76| 0.75| 0.74| 0.71| 0.68| 0.65| 0.64| 0.65|  0.7| 0.72| 0.73| 0.74| 0.78| 0.77| 0.77| 0.76| 0.77|    9|    9|    9|    9|    9|    9|    9|    9|    9|    1|    1|    1|    1|    1|    1|    2|    2|    8|    8|    8|    8|    8|    8|    8| 35.06| 35.14| 34.11| 34.13| 33.69| 34.01| 34.91| 34.07| 34.69| 35.49| 37.46| 39.35| 42.57| 43.45| 43.86| 43.45| 43.61| 43.45| 43.76|  43.0| 42.91| 43.05| 43.39| 43.34|  4.56|  4.27|  5.22|  5.23|  5.96|   5.3|  4.58|  5.69|  5.14|  5.36|  5.97|  6.98|  5.77|  6.72|  6.18|  5.48|  4.87|  5.14|  5.28|  5.47|  5.82|   5.5|  5.78|  6.46|\n|                    49.535|               rain|    0.83|2015-01-12|     6.57| 0.78| 0.74| 0.73| 0.74| 0.74| 0.75| 0.77| 0.82|  0.8| 0.78| 0.76| 0.78| 0.83| 0.87| 0.84| 0.86| 0.89| 0.89| 0.92| 0.93| 0.92| 0.91| 0.92| 0.93|    8|    8|    8|    8|    8|    8|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    8|    5|    5|    8|    8|    8|    5| 43.09| 43.76| 44.48| 44.76| 45.09| 45.55| 45.21| 45.58| 45.54| 46.82|  51.5| 52.36| 52.49| 52.47| 52.89| 52.62| 52.14| 52.26| 51.48| 51.49| 51.34| 51.64| 51.89| 51.96|  6.61|  6.71|  7.33|  7.63|  7.53|  7.77|  8.89|  7.31|  8.19|  7.16|  8.96|  8.03|  8.34|  6.72|  6.47|  6.06|   4.9|  5.07|  5.55|  4.46|  4.33|  5.39|   4.5|  5.68|\n|                     39.87|  partly-cloudy-day|    0.84|2015-01-13|     4.12| 0.92| 0.92| 0.92|  0.9| 0.88|  0.9|  0.9| 0.88| 0.85| 0.84| 0.84| 0.79| 0.76| 0.78| 0.83| 0.78| 0.77| 0.77| 0.79| 0.82| 0.84| 0.82| 0.83| 0.83|    5|    5|    5|    5|    5|    8|    8|    8|    8|    2|    2|    1|    1|    2|    2|    2|    1|    8|    9|    8|    8|    8|    9|    8| 52.35| 52.47| 52.02| 51.35| 50.52| 48.26| 47.56| 46.25| 46.18| 45.24| 45.44| 45.81| 46.66| 46.04| 42.88| 42.64| 42.07| 40.11| 42.33| 41.66| 38.43| 37.18| 37.04| 36.05|  5.45|  6.51|  5.89|  4.28|  4.24|  3.84|  3.31|  4.07|  2.17|  1.55|  2.92|  3.95|  4.28|  4.23|  5.63|  6.26|  5.18|  5.31|  2.78|  2.78|  3.93|   4.5|   3.7|  3.78|\n|                     42.13|partly-cloudy-night|    0.76|2015-01-14|     5.04| 0.79| 0.74| 0.75| 0.78| 0.79|  0.8|  0.8| 0.78| 0.79|  0.8| 0.78| 0.76| 0.71| 0.68| 0.65| 0.65| 0.67|  0.7| 0.76| 0.82| 0.81| 0.81| 0.85| 0.87|    8|    8|    8|    9|    9|    8|    8|    8|    9|    1|    1|    1|    1|    1|    1|    1|    2|    8|    8|    8|    8|    8|    8|    8| 35.57| 35.51| 35.49| 34.87| 37.93| 35.67| 35.63| 35.07| 33.08| 33.58|  33.9| 35.69| 38.34| 39.95| 40.99| 40.75| 40.53| 40.94| 40.67|  41.5| 42.76| 43.52| 43.46| 43.37|  4.32|  6.12|  4.77|  4.47|  2.79|  3.66|  4.45|  4.59|  5.97|  5.08|  6.29|  6.48|  5.42|   5.4|  5.23|  4.28|  4.23|  3.75|  5.33|  6.69|  8.26|  9.29|  8.42|  9.71|\n|                     41.05|               rain|    0.77|2015-01-15|     6.71| 0.89| 0.88| 0.89| 0.89| 0.88| 0.86| 0.87| 0.84| 0.83| 0.83| 0.84| 0.83| 0.78| 0.76| 0.69| 0.66| 0.64| 0.64| 0.67| 0.68| 0.63| 0.65| 0.66| 0.66|    8|    8|    8|    5|    5|    5|    5|    8|    8|    2|    1|    2|    2|    2|    2|    2|    2|    9|    9|    9|    8|    9|    8|    8| 44.16| 45.37| 50.02| 50.88| 50.66| 46.12| 44.98| 44.76| 43.94| 44.14| 43.56| 45.06| 44.48|  43.9|  43.7| 43.45| 40.92| 40.08| 39.13| 39.28| 38.22| 37.49| 37.04| 38.21|  9.22|  9.56| 11.62|  9.26|  7.67|  7.03|  6.02|  4.56|  4.52|  3.57|  4.18|  4.22|  5.45|  6.64|  8.13|  7.05|   8.3|  6.46|  6.46|  6.74|  8.31|  6.79|  7.67|  6.31|\n|                      37.0|  partly-cloudy-day|    0.76|2015-01-16|     2.31| 0.67| 0.65| 0.67|  0.7| 0.71| 0.69| 0.68| 0.71| 0.73| 0.78| 0.79|  0.8| 0.81| 0.79| 0.75| 0.73| 0.75|  0.8| 0.81| 0.81| 0.83| 0.83| 0.85| 0.85|    9|    9|    9|    9|    8|    8|    8|    8|    8|    2|    2|    2|    2|    2|    1|    1|    2|    8|    8|    9|    9|    9|    9|    9| 38.98| 39.24|  39.8| 39.34| 39.16| 38.99| 39.37| 39.89| 40.21| 41.62|  41.7| 41.82| 41.86| 42.53| 43.54| 44.05| 42.64| 40.06| 39.21| 38.07| 36.55| 35.31| 34.09| 32.96|  5.17|  5.45|  4.01|  3.47|  3.37|  3.27|  3.41|  3.29|  3.09|  2.31|  2.27|  1.84|  2.41|  2.43|  2.74|  2.22|  1.76|  1.09|  2.18|  1.75|  1.47|  1.24|  0.84|  0.61|\n|                     39.13|  partly-cloudy-day|    0.83|2015-01-17|     1.72| 0.89| 0.89| 0.87| 0.87| 0.88| 0.87| 0.87| 0.84| 0.83|  0.8| 0.77| 0.79| 0.81| 0.78| 0.76| 0.75| 0.79| 0.82| 0.84| 0.84| 0.84| 0.85| 0.85| 0.85|    9|    9|    9|    9|    9|    9|    9|    8|    8|    2|    2|    2|    2|    2|    2|    1|    1|    9|    9|    9|    8|    9|    9|    8| 31.72| 31.44| 31.22| 30.53| 30.15| 29.98| 29.95| 30.79|  31.5| 33.92| 36.62| 38.16|  37.6| 40.67| 41.07| 40.82| 40.74| 39.03| 38.38| 38.52|  38.9| 38.97| 39.13| 38.87|  0.63|  1.15|  1.01|  0.58|  0.82|  1.04|   1.2|  2.11|  2.02|  2.25|  2.68|  2.64|   3.7|  3.42|  3.61|  3.62|  1.71|  1.47|  1.32|  1.39|  1.29|  1.34|  0.77|  0.51|\n|                     35.21|  partly-cloudy-day|    0.89|2015-01-18|     0.84| 0.86| 0.86| 0.87| 0.89|  0.9| 0.92| 0.92| 0.92| 0.92| 0.92| 0.91|  0.9| 0.88| 0.84| 0.83| 0.82| 0.83| 0.86| 0.88|  0.9|  0.9|  0.9| 0.91| 0.92|    9|    8|    8|    8|    8|    8|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    9|    9|    9|    9|    9|    9|    9| 38.46| 38.16| 38.04| 37.85| 37.66| 37.35| 37.23| 37.23| 37.19| 37.38| 37.92|  38.3| 39.01| 39.44| 37.81|  39.0| 35.49| 35.32| 33.88| 32.46| 32.26| 31.67| 31.13| 30.98|  0.35|  0.32|  0.31|   0.2|  0.15|   0.0|  0.03|   0.0|  0.16|  0.24|  1.09|  1.25|  2.83|  2.21|  3.01|  2.67|  3.03|  1.02|  0.68|  0.73|  0.65|  0.65|   0.4|  0.48|\n|                     32.85|  partly-cloudy-day|    0.85|2015-01-19|     0.65| 0.93| 0.93| 0.92| 0.92| 0.92| 0.92| 0.91| 0.91|  0.9|  0.9| 0.88| 0.82| 0.78| 0.75| 0.73| 0.72| 0.73| 0.77| 0.82| 0.83| 0.85| 0.86| 0.88| 0.89|    8|    8|    8|    8|    8|    8|    8|    8|    8|    2|    2|    2|    1|    2|    2|    2|    2|    8|    8|    8|    8|    8|    8|    8| 30.99| 31.57|  32.6| 32.85| 32.88| 32.89| 33.69| 33.79| 33.63| 33.81| 34.88| 36.99| 38.56| 39.42| 39.27| 39.14| 38.12| 36.06| 33.97| 34.24| 34.05| 33.19| 31.94| 30.81|  0.59|  0.66|   1.1|  1.09|  0.81|  1.35|  1.15|  0.88|  0.55|  0.48|  0.99|  1.33|   0.6|  1.85|  1.49|  1.08|  0.22|   0.0|   0.0|  0.03|  0.02|   0.0|   0.0|  0.09|\n|                    36.285|partly-cloudy-night|    0.85|2015-01-20|     1.38|  0.9|  0.9|  0.9| 0.91| 0.91| 0.91| 0.92| 0.91| 0.91| 0.91| 0.91| 0.87| 0.81| 0.77| 0.76| 0.77|  0.8| 0.84| 0.85| 0.84|  0.8| 0.77| 0.76| 0.77|    9|    9|    9|    9|    9|    9|    9|    9|    9|    1|    1|    1|    2|    1|    1|    2|    2|    8|    8|    8|    8|    8|    8|    8| 29.39| 28.92| 28.01| 27.27| 26.69| 26.28| 26.43| 26.85| 26.77| 28.31| 30.89| 34.24| 37.54| 38.95| 39.85| 39.42| 38.17| 35.09| 37.31| 37.64| 36.07| 36.06| 35.41| 35.28|  0.03|  0.03|  0.03|  0.02|  0.05|   0.0|   0.0|  0.03|  0.32|  0.23|  0.33|  0.43|  0.81|  2.07|  2.38|  2.45|  2.12|   3.3|  1.93|  2.61|  3.07|  3.43|   4.3|  4.15|\n|                    35.595|  partly-cloudy-day|    0.87|2015-01-21|      2.6|  0.8| 0.83| 0.85| 0.86| 0.87| 0.87| 0.89| 0.89| 0.89| 0.89| 0.89| 0.88| 0.87| 0.88| 0.87| 0.87| 0.87| 0.87| 0.87| 0.87| 0.87| 0.87| 0.87| 0.88|    8|    8|    3|    8|    3|    3|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    8|    8|    8|    8|    8|    8|    8| 33.45| 32.72| 33.53| 33.08| 33.57| 34.49| 34.64| 34.24| 34.69| 34.46| 37.25| 35.28| 35.74| 37.88| 36.39| 38.36| 38.06| 37.74| 37.43| 37.21| 36.85| 36.42| 35.99| 35.52|  5.25|  5.17|  4.17|  4.61|   4.1|   3.4|  3.36|  3.57|   3.0|   3.2|  2.99|   3.2|  3.05|  2.69|  3.12|  2.77|  2.76|   2.3|  1.19|   1.3|  0.79|  0.71|  0.29|  0.23|\n|                    32.875|  partly-cloudy-day|    0.83|2015-01-22|     0.83| 0.89|  0.9| 0.91| 0.92| 0.92| 0.92| 0.91|  0.9|  0.9| 0.89| 0.87| 0.83| 0.77| 0.68| 0.66| 0.66| 0.69| 0.73| 0.74| 0.77| 0.81| 0.82| 0.84| 0.87|    8|    8|    8|    9|    8|    8|    8|    8|    8|    2|    2|    2|    2|    2|    2|    2|    2|    8|    8|    8|    8|    9|    9|    9| 34.62| 34.05| 33.37| 32.83| 33.06| 33.19| 33.64| 33.57| 32.92| 33.23| 34.99| 36.82| 38.99| 40.45| 40.17| 39.41|  36.0| 36.63| 35.48| 33.78| 32.17| 31.08| 30.14| 28.95|  0.23|  0.34|  0.08|  0.11|  0.19|  0.16|   0.2|  0.18|  0.22|  0.25|  0.32|  0.65|  1.23|  1.89|  2.94|  2.51|  3.07|  1.96|  0.99|  0.59|  0.47|  0.46|  1.27|   0.0|\n|                    36.575|               rain|    0.85|2015-01-23|     1.95| 0.89| 0.89| 0.89|  0.9|  0.9| 0.89| 0.89|  0.9| 0.91|  0.9| 0.89| 0.83| 0.78| 0.74| 0.73| 0.75| 0.78| 0.81| 0.83| 0.84| 0.85| 0.87| 0.89| 0.92|    9|    9|    9|    9|    8|    8|    9|    9|    1|    1|    1|    1|    1|    2|    1|    1|    1|    9|    8|    8|    8|    8|    8|    8| 27.81| 27.44| 27.21| 26.62| 26.31| 26.42| 26.16| 25.31|  25.3| 28.04|  30.6| 34.68| 35.72| 36.68| 37.52| 37.86|  39.0| 37.51| 35.69|  36.3| 38.09| 39.41| 41.33| 42.83|  0.04|  0.05|  0.05|  0.01|  0.03|   0.0|   0.0|  0.17|  0.26|  0.46|  0.37|  1.21|  3.34|  3.95|  4.16|  3.64|  2.67|  2.83|  3.16|  4.06|  4.33|  4.83|  4.43|   3.6|\n+--------------------------+-------------------+--------+----------+---------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1248"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559489843166_1479078472","id":"20190602-173723_1300760227","dateCreated":"2019-06-02T17:37:23+0200","dateStarted":"2019-06-11T02:53:00+0200","dateFinished":"2019-06-11T02:53:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%spark2.pyspark\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\nfrom collections import Counter\n\ndef _combine(x,y):\n    \"\"\"creates hourly interval for weather df\"\"\"\n    print(y)\n    d = str(x) + ' {}:00:00'.format(y)\n    return d\n\ndef transformWeatherDF(df, past):\n    \"\"\"add hours on key column and transform columoriented hours to roworiented hours\"\"\"\n    \n    #Remove daily columns\n    df = df.drop(*[\"Apparent Temperature (Avg)\", \"Daily Weather\", \"Hourly Weather\", \"Humidity\", \"Windspeed\"])\n    \n    #use udf to generate hours on the dates\n    combine = F.udf(lambda x,y: _combine(x,y))\n\n    #fetch relevant columns and keep column datatype\n    cols_temp, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"temp\")))\n    cols_hum, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"hum\")))\n    cols_sum, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"sum\")))\n    cols_wind, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"wind\")))\n    \n    ##temperature\n    #explode function for changing structure of dataframe (temperature)\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_temp],\n          )).alias(\"kvs\")\n\n    df_temp = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Temperature')])\\\n       .withColumn('key', F.regexp_replace('key', 'temp', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n\n    # go x hours back (temperature)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df_temp = df_temp.select(\"*\", lag(\"Current Temperature\", i+1).over(w).alias(\"temp\"+`i`))\n        \n    # Aggregate temperature values\n    if past >= 48:\n        marksColumns = []\n        z = 24\n        r = past - z\n        j = past\n        t = 1\n        if r % 6 == 0:\n            for i in range(r):\n                marksColumns.append(col('temp'+`z`))\n                z = z + 1\n                if t % 6 == 0:\n                    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n                    df_temp = df_temp.withColumn('temp ' + `j` + ' Avg. over 6h', averageFunc)\n                    j = j + 1\n                    marksColumns = []\n                t = t + 1\n\n    ##humidity\n    #explode humidity columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_hum],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Humidity')])\\\n       .withColumn('key', F.regexp_replace('key', 'hum', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (humidity)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Humidity\", i+1).over(w).alias(\"hum\"+`i`))\n        \n    # Aggregate humidity values\n    if past >= 48:\n        marksColumns = []\n        z = 24\n        r = past - z\n        j = past\n        t = 1\n        if r % 6 == 0:\n            for i in range(r):\n                marksColumns.append(col('hum'+`z`))\n                z = z + 1\n                if t % 6 == 0:\n                    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n                    df2 = df2.withColumn('hum ' + `j` + ' Avg. over 6h', averageFunc)\n                    j = j + 1\n                    marksColumns = []\n                t = t + 1\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n    \n    ##summary\n    #explode summary columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_sum],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Summary')])\\\n       .withColumn('key', F.regexp_replace('key', 'sum', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (summary)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Summary\", i+1).over(w).alias(\"sum\"+`i`))\n    \n    # Aggregate summary values\n    if past >= 48:\n        marksColumns = []\n        z = 24\n        r = past - z\n        j = past\n        t = 1\n        if r % 6 == 0:\n            for i in range(r):\n                marksColumns.append(col('sum'+`z`))\n                z = z + 1\n                if t % 6 == 0:\n                    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n                    df2 = df2.withColumn('sum ' + `j` + ' Avg. over 6h', averageFunc)\n                    j = j + 1\n                    marksColumns = []\n                t = t + 1\n\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n    \n    ##windspeed\n    #explode windspeed columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_wind],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Windspeed')])\\\n       .withColumn('key', F.regexp_replace('key', 'wind', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (windspeed)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Windspeed\", i+1).over(w).alias(\"wind\"+`i`))\n        \n    # Aggregate windspeed values\n    if past >= 48:\n        marksColumns = []\n        z = 24\n        r = past - z\n        j = past\n        t = 1\n        if r % 6 == 0:\n            for i in range(r):\n                marksColumns.append(col('wind'+`z`))\n                z = z + 1\n                if t % 6 == 0:\n                    averageFunc = sum(x for x in marksColumns)/len(marksColumns)\n                    df2 = df2.withColumn('wind ' + `j` + ' Avg. over 6h', averageFunc)\n                    j = j + 1\n                    marksColumns = []\n                t = t + 1\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n\n    return df_temp\n\n#Set x for past hours\nx = 48\n\ntransformWeatherDF(df_weather, 3).show()\n\ndf_weather = transformWeatherDF(df_weather, x) #Note: For reusing, first execute Interpolation cell above","user":"admin","dateUpdated":"2019-06-11T02:23:26+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+---------------+----+----+----+-----------------+-----+-----+-----+\n|         Start Date|Current Temperature|temp0|temp1|temp2|Current Humidity|hum0|hum1|hum2|Current Summary|sum0|sum1|sum2|Current Windspeed|wind0|wind1|wind2|\n+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+---------------+----+----+----+-----------------+-----+-----+-----+\n|2015-01-04 00:00:00|              37.24| null| null| null|            0.93|null|null|null|              8|null|null|null|             0.94| null| null| null|\n|2015-01-04 01:00:00|              36.35|37.24| null| null|            0.94|0.93|null|null|              8|   8|null|null|             0.56| 0.94| null| null|\n|2015-01-04 02:00:00|              35.61|36.35|37.24| null|            0.94|0.94|0.93|null|              6|   8|   8|null|             0.34| 0.56| 0.94| null|\n|2015-01-04 03:00:00|              34.36|35.61|36.35|37.24|            0.95|0.94|0.94|0.93|              9|   6|   8|   8|             0.05| 0.34| 0.56| 0.94|\n|2015-01-04 04:00:00|              33.43|34.36|35.61|36.35|            0.94|0.95|0.94|0.94|              6|   9|   6|   8|             0.08| 0.05| 0.34| 0.56|\n|2015-01-04 05:00:00|              33.14|33.43|34.36|35.61|            0.94|0.94|0.95|0.94|              6|   6|   9|   6|             0.18| 0.08| 0.05| 0.34|\n|2015-01-04 06:00:00|              32.86|33.14|33.43|34.36|            0.94|0.94|0.94|0.95|              6|   6|   6|   9|             0.28| 0.18| 0.08| 0.05|\n|2015-01-04 07:00:00|              32.98|32.86|33.14|33.43|            0.96|0.94|0.94|0.94|              6|   6|   6|   6|             0.03| 0.28| 0.18| 0.08|\n|2015-01-04 08:00:00|              33.09|32.98|32.86|33.14|            0.95|0.96|0.94|0.94|              6|   6|   6|   6|             0.14| 0.03| 0.28| 0.18|\n|2015-01-04 09:00:00|              33.65|33.09|32.98|32.86|            0.94|0.95|0.96|0.94|              6|   6|   6|   6|             0.34| 0.14| 0.03| 0.28|\n|2015-01-04 10:00:00|              34.03|33.65|33.09|32.98|            0.94|0.94|0.95|0.96|              6|   6|   6|   6|             1.04| 0.34| 0.14| 0.03|\n|2015-01-04 11:00:00|              34.91|34.03|33.65|33.09|            0.94|0.94|0.94|0.95|              6|   6|   6|   6|              1.3| 1.04| 0.34| 0.14|\n|2015-01-04 12:00:00|              35.65|34.91|34.03|33.65|            0.94|0.94|0.94|0.94|              6|   6|   6|   6|             1.02|  1.3| 1.04| 0.34|\n|2015-01-04 13:00:00|              36.23|35.65|34.91|34.03|            0.94|0.94|0.94|0.94|              6|   6|   6|   6|             1.53| 1.02|  1.3| 1.04|\n|2015-01-04 14:00:00|              36.49|36.23|35.65|34.91|            0.94|0.94|0.94|0.94|              2|   6|   6|   6|             1.36| 1.53| 1.02|  1.3|\n|2015-01-04 15:00:00|              36.57|36.49|36.23|35.65|            0.94|0.94|0.94|0.94|              2|   2|   6|   6|              0.8| 1.36| 1.53| 1.02|\n|2015-01-04 16:00:00|              36.18|36.57|36.49|36.23|            0.94|0.94|0.94|0.94|              8|   2|   2|   6|              1.6|  0.8| 1.36| 1.53|\n|2015-01-04 17:00:00|              36.31|36.18|36.57|36.49|            0.94|0.94|0.94|0.94|              6|   8|   2|   2|             0.84|  1.6|  0.8| 1.36|\n|2015-01-04 18:00:00|              36.42|36.31|36.18|36.57|            0.94|0.94|0.94|0.94|              6|   6|   8|   2|             0.94| 0.84|  1.6|  0.8|\n|2015-01-04 19:00:00|              36.32|36.42|36.31|36.18|            0.94|0.94|0.94|0.94|              6|   6|   6|   8|             1.04| 0.94| 0.84|  1.6|\n+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+---------------+----+----+----+-----------------+-----+-----+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1249"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559339728262_-1967668579","id":"20190531-235528_1485729994","dateCreated":"2019-05-31T23:55:28+0200","dateStarted":"2019-06-11T02:53:00+0200","dateFinished":"2019-06-11T02:53:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%spark2.pyspark\ndf_weather.printSchema()\nreference.printSchema()","user":"admin","dateUpdated":"2019-06-11T02:23:39+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Start Date: timestamp (nullable = true)\n |-- Current Temperature: double (nullable = true)\n |-- temp0: double (nullable = true)\n |-- temp1: double (nullable = true)\n |-- temp2: double (nullable = true)\n |-- temp3: double (nullable = true)\n |-- temp4: double (nullable = true)\n |-- temp5: double (nullable = true)\n |-- temp6: double (nullable = true)\n |-- temp7: double (nullable = true)\n |-- temp8: double (nullable = true)\n |-- temp9: double (nullable = true)\n |-- temp10: double (nullable = true)\n |-- temp11: double (nullable = true)\n |-- temp12: double (nullable = true)\n |-- temp13: double (nullable = true)\n |-- temp14: double (nullable = true)\n |-- temp15: double (nullable = true)\n |-- temp16: double (nullable = true)\n |-- temp17: double (nullable = true)\n |-- temp18: double (nullable = true)\n |-- temp19: double (nullable = true)\n |-- temp20: double (nullable = true)\n |-- temp21: double (nullable = true)\n |-- temp22: double (nullable = true)\n |-- temp23: double (nullable = true)\n |-- temp24: double (nullable = true)\n |-- temp25: double (nullable = true)\n |-- temp26: double (nullable = true)\n |-- temp27: double (nullable = true)\n |-- temp28: double (nullable = true)\n |-- temp29: double (nullable = true)\n |-- temp30: double (nullable = true)\n |-- temp31: double (nullable = true)\n |-- temp32: double (nullable = true)\n |-- temp33: double (nullable = true)\n |-- temp34: double (nullable = true)\n |-- temp35: double (nullable = true)\n |-- temp36: double (nullable = true)\n |-- temp37: double (nullable = true)\n |-- temp38: double (nullable = true)\n |-- temp39: double (nullable = true)\n |-- temp40: double (nullable = true)\n |-- temp41: double (nullable = true)\n |-- temp42: double (nullable = true)\n |-- temp43: double (nullable = true)\n |-- temp44: double (nullable = true)\n |-- temp45: double (nullable = true)\n |-- temp46: double (nullable = true)\n |-- temp47: double (nullable = true)\n |-- temp 48 Avg. over 6h: double (nullable = true)\n |-- temp 49 Avg. over 6h: double (nullable = true)\n |-- temp 50 Avg. over 6h: double (nullable = true)\n |-- temp 51 Avg. over 6h: double (nullable = true)\n |-- Current Humidity: double (nullable = true)\n |-- hum0: double (nullable = true)\n |-- hum1: double (nullable = true)\n |-- hum2: double (nullable = true)\n |-- hum3: double (nullable = true)\n |-- hum4: double (nullable = true)\n |-- hum5: double (nullable = true)\n |-- hum6: double (nullable = true)\n |-- hum7: double (nullable = true)\n |-- hum8: double (nullable = true)\n |-- hum9: double (nullable = true)\n |-- hum10: double (nullable = true)\n |-- hum11: double (nullable = true)\n |-- hum12: double (nullable = true)\n |-- hum13: double (nullable = true)\n |-- hum14: double (nullable = true)\n |-- hum15: double (nullable = true)\n |-- hum16: double (nullable = true)\n |-- hum17: double (nullable = true)\n |-- hum18: double (nullable = true)\n |-- hum19: double (nullable = true)\n |-- hum20: double (nullable = true)\n |-- hum21: double (nullable = true)\n |-- hum22: double (nullable = true)\n |-- hum23: double (nullable = true)\n |-- hum24: double (nullable = true)\n |-- hum25: double (nullable = true)\n |-- hum26: double (nullable = true)\n |-- hum27: double (nullable = true)\n |-- hum28: double (nullable = true)\n |-- hum29: double (nullable = true)\n |-- hum30: double (nullable = true)\n |-- hum31: double (nullable = true)\n |-- hum32: double (nullable = true)\n |-- hum33: double (nullable = true)\n |-- hum34: double (nullable = true)\n |-- hum35: double (nullable = true)\n |-- hum36: double (nullable = true)\n |-- hum37: double (nullable = true)\n |-- hum38: double (nullable = true)\n |-- hum39: double (nullable = true)\n |-- hum40: double (nullable = true)\n |-- hum41: double (nullable = true)\n |-- hum42: double (nullable = true)\n |-- hum43: double (nullable = true)\n |-- hum44: double (nullable = true)\n |-- hum45: double (nullable = true)\n |-- hum46: double (nullable = true)\n |-- hum47: double (nullable = true)\n |-- hum 48 Avg. over 6h: double (nullable = true)\n |-- hum 49 Avg. over 6h: double (nullable = true)\n |-- hum 50 Avg. over 6h: double (nullable = true)\n |-- hum 51 Avg. over 6h: double (nullable = true)\n |-- Current Summary: long (nullable = true)\n |-- sum0: long (nullable = true)\n |-- sum1: long (nullable = true)\n |-- sum2: long (nullable = true)\n |-- sum3: long (nullable = true)\n |-- sum4: long (nullable = true)\n |-- sum5: long (nullable = true)\n |-- sum6: long (nullable = true)\n |-- sum7: long (nullable = true)\n |-- sum8: long (nullable = true)\n |-- sum9: long (nullable = true)\n |-- sum10: long (nullable = true)\n |-- sum11: long (nullable = true)\n |-- sum12: long (nullable = true)\n |-- sum13: long (nullable = true)\n |-- sum14: long (nullable = true)\n |-- sum15: long (nullable = true)\n |-- sum16: long (nullable = true)\n |-- sum17: long (nullable = true)\n |-- sum18: long (nullable = true)\n |-- sum19: long (nullable = true)\n |-- sum20: long (nullable = true)\n |-- sum21: long (nullable = true)\n |-- sum22: long (nullable = true)\n |-- sum23: long (nullable = true)\n |-- sum24: long (nullable = true)\n |-- sum25: long (nullable = true)\n |-- sum26: long (nullable = true)\n |-- sum27: long (nullable = true)\n |-- sum28: long (nullable = true)\n |-- sum29: long (nullable = true)\n |-- sum30: long (nullable = true)\n |-- sum31: long (nullable = true)\n |-- sum32: long (nullable = true)\n |-- sum33: long (nullable = true)\n |-- sum34: long (nullable = true)\n |-- sum35: long (nullable = true)\n |-- sum36: long (nullable = true)\n |-- sum37: long (nullable = true)\n |-- sum38: long (nullable = true)\n |-- sum39: long (nullable = true)\n |-- sum40: long (nullable = true)\n |-- sum41: long (nullable = true)\n |-- sum42: long (nullable = true)\n |-- sum43: long (nullable = true)\n |-- sum44: long (nullable = true)\n |-- sum45: long (nullable = true)\n |-- sum46: long (nullable = true)\n |-- sum47: long (nullable = true)\n |-- sum 48 Avg. over 6h: double (nullable = true)\n |-- sum 49 Avg. over 6h: double (nullable = true)\n |-- sum 50 Avg. over 6h: double (nullable = true)\n |-- sum 51 Avg. over 6h: double (nullable = true)\n |-- Current Windspeed: double (nullable = true)\n |-- wind0: double (nullable = true)\n |-- wind1: double (nullable = true)\n |-- wind2: double (nullable = true)\n |-- wind3: double (nullable = true)\n |-- wind4: double (nullable = true)\n |-- wind5: double (nullable = true)\n |-- wind6: double (nullable = true)\n |-- wind7: double (nullable = true)\n |-- wind8: double (nullable = true)\n |-- wind9: double (nullable = true)\n |-- wind10: double (nullable = true)\n |-- wind11: double (nullable = true)\n |-- wind12: double (nullable = true)\n |-- wind13: double (nullable = true)\n |-- wind14: double (nullable = true)\n |-- wind15: double (nullable = true)\n |-- wind16: double (nullable = true)\n |-- wind17: double (nullable = true)\n |-- wind18: double (nullable = true)\n |-- wind19: double (nullable = true)\n |-- wind20: double (nullable = true)\n |-- wind21: double (nullable = true)\n |-- wind22: double (nullable = true)\n |-- wind23: double (nullable = true)\n |-- wind24: double (nullable = true)\n |-- wind25: double (nullable = true)\n |-- wind26: double (nullable = true)\n |-- wind27: double (nullable = true)\n |-- wind28: double (nullable = true)\n |-- wind29: double (nullable = true)\n |-- wind30: double (nullable = true)\n |-- wind31: double (nullable = true)\n |-- wind32: double (nullable = true)\n |-- wind33: double (nullable = true)\n |-- wind34: double (nullable = true)\n |-- wind35: double (nullable = true)\n |-- wind36: double (nullable = true)\n |-- wind37: double (nullable = true)\n |-- wind38: double (nullable = true)\n |-- wind39: double (nullable = true)\n |-- wind40: double (nullable = true)\n |-- wind41: double (nullable = true)\n |-- wind42: double (nullable = true)\n |-- wind43: double (nullable = true)\n |-- wind44: double (nullable = true)\n |-- wind45: double (nullable = true)\n |-- wind46: double (nullable = true)\n |-- wind47: double (nullable = true)\n |-- wind 48 Avg. over 6h: double (nullable = true)\n |-- wind 49 Avg. over 6h: double (nullable = true)\n |-- wind 50 Avg. over 6h: double (nullable = true)\n |-- wind 51 Avg. over 6h: double (nullable = true)\n\nroot\n |-- New Date: timestamp (nullable = false)\n |-- StartStation Id: integer (nullable = true)\n |-- Current Usage: long (nullable = true)\n |-- Usage0: long (nullable = true)\n |-- Usage1: long (nullable = true)\n |-- Usage2: long (nullable = true)\n |-- Usage3: long (nullable = true)\n |-- Usage4: long (nullable = true)\n |-- Usage5: long (nullable = true)\n |-- Usage6: long (nullable = true)\n |-- Usage7: long (nullable = true)\n |-- Usage8: long (nullable = true)\n |-- Usage9: long (nullable = true)\n |-- Usage10: long (nullable = true)\n |-- Usage11: long (nullable = true)\n |-- Usage12: long (nullable = true)\n |-- Usage13: long (nullable = true)\n |-- Usage14: long (nullable = true)\n |-- Usage15: long (nullable = true)\n |-- Usage16: long (nullable = true)\n |-- Usage17: long (nullable = true)\n |-- Usage18: long (nullable = true)\n |-- Usage19: long (nullable = true)\n |-- Usage20: long (nullable = true)\n |-- Usage21: long (nullable = true)\n |-- Usage22: long (nullable = true)\n |-- Usage23: long (nullable = true)\n |-- Usage24: long (nullable = true)\n |-- Usage25: long (nullable = true)\n |-- Usage26: long (nullable = true)\n |-- Usage27: long (nullable = true)\n |-- Usage28: long (nullable = true)\n |-- Usage29: long (nullable = true)\n |-- Usage30: long (nullable = true)\n |-- Usage31: long (nullable = true)\n |-- Usage32: long (nullable = true)\n |-- Usage33: long (nullable = true)\n |-- Usage34: long (nullable = true)\n |-- Usage35: long (nullable = true)\n |-- Usage36: long (nullable = true)\n |-- Usage37: long (nullable = true)\n |-- Usage38: long (nullable = true)\n |-- Usage39: long (nullable = true)\n |-- Usage40: long (nullable = true)\n |-- Usage41: long (nullable = true)\n |-- Usage42: long (nullable = true)\n |-- Usage43: long (nullable = true)\n |-- Usage44: long (nullable = true)\n |-- Usage45: long (nullable = true)\n |-- Usage46: long (nullable = true)\n |-- Usage47: long (nullable = true)\n |-- Usage 48 Avg. over 6h: double (nullable = true)\n |-- Usage 49 Avg. over 6h: double (nullable = true)\n |-- Usage 50 Avg. over 6h: double (nullable = true)\n |-- Usage 51 Avg. over 6h: double (nullable = true)\n |-- Future0: long (nullable = true)\n |-- Future1: long (nullable = true)\n |-- Future2: long (nullable = true)\n |-- Future3: long (nullable = true)\n |-- Future4: long (nullable = true)\n |-- Future5: long (nullable = true)\n |-- Future6: long (nullable = true)\n |-- Future7: long (nullable = true)\n |-- Future8: long (nullable = true)\n |-- Future9: long (nullable = true)\n |-- Future10: long (nullable = true)\n |-- Future11: long (nullable = true)\n |-- Future12: long (nullable = true)\n |-- Future13: long (nullable = true)\n |-- Future14: long (nullable = true)\n |-- Future15: long (nullable = true)\n |-- Future16: long (nullable = true)\n |-- Future17: long (nullable = true)\n |-- Future18: long (nullable = true)\n |-- Future19: long (nullable = true)\n |-- Future20: long (nullable = true)\n |-- Future21: long (nullable = true)\n |-- Future22: long (nullable = true)\n |-- Future23: long (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1559400421007_-787111632","id":"20190601-164701_235173671","dateCreated":"2019-06-01T16:47:01+0200","dateStarted":"2019-06-11T02:53:01+0200","dateFinished":"2019-06-11T02:53:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import to_date, col\n\n##Merge prepared usage frame with prepared weather dataframe\ndf_final = reference.alias('l').join(df_weather.alias('r'), col('l.New Date') == col('r.Start Date'), how='left').select(col('l.*'), col('r.*'))\ndf_final = df_final.drop(*[\"Start Date\"])\ndf_final = df_final.orderBy(\"New Date\")\ndf_final = df_final.na.fill(0)\ndf_final.printSchema()\n\n##remove first and last rows according to x\ndf = df_final.toPandas()\ndf = df.iloc[x:]\ndf.drop(df.tail(x).index,inplace=True)\ndf_final = spark.createDataFrame(df)","user":"admin","dateUpdated":"2019-06-11T02:23:47+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- New Date: timestamp (nullable = false)\n |-- StartStation Id: integer (nullable = true)\n |-- Current Usage: long (nullable = true)\n |-- Usage0: long (nullable = true)\n |-- Usage1: long (nullable = true)\n |-- Usage2: long (nullable = true)\n |-- Usage3: long (nullable = true)\n |-- Usage4: long (nullable = true)\n |-- Usage5: long (nullable = true)\n |-- Usage6: long (nullable = true)\n |-- Usage7: long (nullable = true)\n |-- Usage8: long (nullable = true)\n |-- Usage9: long (nullable = true)\n |-- Usage10: long (nullable = true)\n |-- Usage11: long (nullable = true)\n |-- Usage12: long (nullable = true)\n |-- Usage13: long (nullable = true)\n |-- Usage14: long (nullable = true)\n |-- Usage15: long (nullable = true)\n |-- Usage16: long (nullable = true)\n |-- Usage17: long (nullable = true)\n |-- Usage18: long (nullable = true)\n |-- Usage19: long (nullable = true)\n |-- Usage20: long (nullable = true)\n |-- Usage21: long (nullable = true)\n |-- Usage22: long (nullable = true)\n |-- Usage23: long (nullable = true)\n |-- Usage24: long (nullable = true)\n |-- Usage25: long (nullable = true)\n |-- Usage26: long (nullable = true)\n |-- Usage27: long (nullable = true)\n |-- Usage28: long (nullable = true)\n |-- Usage29: long (nullable = true)\n |-- Usage30: long (nullable = true)\n |-- Usage31: long (nullable = true)\n |-- Usage32: long (nullable = true)\n |-- Usage33: long (nullable = true)\n |-- Usage34: long (nullable = true)\n |-- Usage35: long (nullable = true)\n |-- Usage36: long (nullable = true)\n |-- Usage37: long (nullable = true)\n |-- Usage38: long (nullable = true)\n |-- Usage39: long (nullable = true)\n |-- Usage40: long (nullable = true)\n |-- Usage41: long (nullable = true)\n |-- Usage42: long (nullable = true)\n |-- Usage43: long (nullable = true)\n |-- Usage44: long (nullable = true)\n |-- Usage45: long (nullable = true)\n |-- Usage46: long (nullable = true)\n |-- Usage47: long (nullable = true)\n |-- Usage 48 Avg. over 6h: double (nullable = false)\n |-- Usage 49 Avg. over 6h: double (nullable = false)\n |-- Usage 50 Avg. over 6h: double (nullable = false)\n |-- Usage 51 Avg. over 6h: double (nullable = false)\n |-- Future0: long (nullable = true)\n |-- Future1: long (nullable = true)\n |-- Future2: long (nullable = true)\n |-- Future3: long (nullable = true)\n |-- Future4: long (nullable = true)\n |-- Future5: long (nullable = true)\n |-- Future6: long (nullable = true)\n |-- Future7: long (nullable = true)\n |-- Future8: long (nullable = true)\n |-- Future9: long (nullable = true)\n |-- Future10: long (nullable = true)\n |-- Future11: long (nullable = true)\n |-- Future12: long (nullable = true)\n |-- Future13: long (nullable = true)\n |-- Future14: long (nullable = true)\n |-- Future15: long (nullable = true)\n |-- Future16: long (nullable = true)\n |-- Future17: long (nullable = true)\n |-- Future18: long (nullable = true)\n |-- Future19: long (nullable = true)\n |-- Future20: long (nullable = true)\n |-- Future21: long (nullable = true)\n |-- Future22: long (nullable = true)\n |-- Future23: long (nullable = true)\n |-- Current Temperature: double (nullable = false)\n |-- temp0: double (nullable = false)\n |-- temp1: double (nullable = false)\n |-- temp2: double (nullable = false)\n |-- temp3: double (nullable = false)\n |-- temp4: double (nullable = false)\n |-- temp5: double (nullable = false)\n |-- temp6: double (nullable = false)\n |-- temp7: double (nullable = false)\n |-- temp8: double (nullable = false)\n |-- temp9: double (nullable = false)\n |-- temp10: double (nullable = false)\n |-- temp11: double (nullable = false)\n |-- temp12: double (nullable = false)\n |-- temp13: double (nullable = false)\n |-- temp14: double (nullable = false)\n |-- temp15: double (nullable = false)\n |-- temp16: double (nullable = false)\n |-- temp17: double (nullable = false)\n |-- temp18: double (nullable = false)\n |-- temp19: double (nullable = false)\n |-- temp20: double (nullable = false)\n |-- temp21: double (nullable = false)\n |-- temp22: double (nullable = false)\n |-- temp23: double (nullable = false)\n |-- temp24: double (nullable = false)\n |-- temp25: double (nullable = false)\n |-- temp26: double (nullable = false)\n |-- temp27: double (nullable = false)\n |-- temp28: double (nullable = false)\n |-- temp29: double (nullable = false)\n |-- temp30: double (nullable = false)\n |-- temp31: double (nullable = false)\n |-- temp32: double (nullable = false)\n |-- temp33: double (nullable = false)\n |-- temp34: double (nullable = false)\n |-- temp35: double (nullable = false)\n |-- temp36: double (nullable = false)\n |-- temp37: double (nullable = false)\n |-- temp38: double (nullable = false)\n |-- temp39: double (nullable = false)\n |-- temp40: double (nullable = false)\n |-- temp41: double (nullable = false)\n |-- temp42: double (nullable = false)\n |-- temp43: double (nullable = false)\n |-- temp44: double (nullable = false)\n |-- temp45: double (nullable = false)\n |-- temp46: double (nullable = false)\n |-- temp47: double (nullable = false)\n |-- temp 48 Avg. over 6h: double (nullable = false)\n |-- temp 49 Avg. over 6h: double (nullable = false)\n |-- temp 50 Avg. over 6h: double (nullable = false)\n |-- temp 51 Avg. over 6h: double (nullable = false)\n |-- Current Humidity: double (nullable = false)\n |-- hum0: double (nullable = false)\n |-- hum1: double (nullable = false)\n |-- hum2: double (nullable = false)\n |-- hum3: double (nullable = false)\n |-- hum4: double (nullable = false)\n |-- hum5: double (nullable = false)\n |-- hum6: double (nullable = false)\n |-- hum7: double (nullable = false)\n |-- hum8: double (nullable = false)\n |-- hum9: double (nullable = false)\n |-- hum10: double (nullable = false)\n |-- hum11: double (nullable = false)\n |-- hum12: double (nullable = false)\n |-- hum13: double (nullable = false)\n |-- hum14: double (nullable = false)\n |-- hum15: double (nullable = false)\n |-- hum16: double (nullable = false)\n |-- hum17: double (nullable = false)\n |-- hum18: double (nullable = false)\n |-- hum19: double (nullable = false)\n |-- hum20: double (nullable = false)\n |-- hum21: double (nullable = false)\n |-- hum22: double (nullable = false)\n |-- hum23: double (nullable = false)\n |-- hum24: double (nullable = false)\n |-- hum25: double (nullable = false)\n |-- hum26: double (nullable = false)\n |-- hum27: double (nullable = false)\n |-- hum28: double (nullable = false)\n |-- hum29: double (nullable = false)\n |-- hum30: double (nullable = false)\n |-- hum31: double (nullable = false)\n |-- hum32: double (nullable = false)\n |-- hum33: double (nullable = false)\n |-- hum34: double (nullable = false)\n |-- hum35: double (nullable = false)\n |-- hum36: double (nullable = false)\n |-- hum37: double (nullable = false)\n |-- hum38: double (nullable = false)\n |-- hum39: double (nullable = false)\n |-- hum40: double (nullable = false)\n |-- hum41: double (nullable = false)\n |-- hum42: double (nullable = false)\n |-- hum43: double (nullable = false)\n |-- hum44: double (nullable = false)\n |-- hum45: double (nullable = false)\n |-- hum46: double (nullable = false)\n |-- hum47: double (nullable = false)\n |-- hum 48 Avg. over 6h: double (nullable = false)\n |-- hum 49 Avg. over 6h: double (nullable = false)\n |-- hum 50 Avg. over 6h: double (nullable = false)\n |-- hum 51 Avg. over 6h: double (nullable = false)\n |-- Current Summary: long (nullable = true)\n |-- sum0: long (nullable = true)\n |-- sum1: long (nullable = true)\n |-- sum2: long (nullable = true)\n |-- sum3: long (nullable = true)\n |-- sum4: long (nullable = true)\n |-- sum5: long (nullable = true)\n |-- sum6: long (nullable = true)\n |-- sum7: long (nullable = true)\n |-- sum8: long (nullable = true)\n |-- sum9: long (nullable = true)\n |-- sum10: long (nullable = true)\n |-- sum11: long (nullable = true)\n |-- sum12: long (nullable = true)\n |-- sum13: long (nullable = true)\n |-- sum14: long (nullable = true)\n |-- sum15: long (nullable = true)\n |-- sum16: long (nullable = true)\n |-- sum17: long (nullable = true)\n |-- sum18: long (nullable = true)\n |-- sum19: long (nullable = true)\n |-- sum20: long (nullable = true)\n |-- sum21: long (nullable = true)\n |-- sum22: long (nullable = true)\n |-- sum23: long (nullable = true)\n |-- sum24: long (nullable = true)\n |-- sum25: long (nullable = true)\n |-- sum26: long (nullable = true)\n |-- sum27: long (nullable = true)\n |-- sum28: long (nullable = true)\n |-- sum29: long (nullable = true)\n |-- sum30: long (nullable = true)\n |-- sum31: long (nullable = true)\n |-- sum32: long (nullable = true)\n |-- sum33: long (nullable = true)\n |-- sum34: long (nullable = true)\n |-- sum35: long (nullable = true)\n |-- sum36: long (nullable = true)\n |-- sum37: long (nullable = true)\n |-- sum38: long (nullable = true)\n |-- sum39: long (nullable = true)\n |-- sum40: long (nullable = true)\n |-- sum41: long (nullable = true)\n |-- sum42: long (nullable = true)\n |-- sum43: long (nullable = true)\n |-- sum44: long (nullable = true)\n |-- sum45: long (nullable = true)\n |-- sum46: long (nullable = true)\n |-- sum47: long (nullable = true)\n |-- sum 48 Avg. over 6h: double (nullable = false)\n |-- sum 49 Avg. over 6h: double (nullable = false)\n |-- sum 50 Avg. over 6h: double (nullable = false)\n |-- sum 51 Avg. over 6h: double (nullable = false)\n |-- Current Windspeed: double (nullable = false)\n |-- wind0: double (nullable = false)\n |-- wind1: double (nullable = false)\n |-- wind2: double (nullable = false)\n |-- wind3: double (nullable = false)\n |-- wind4: double (nullable = false)\n |-- wind5: double (nullable = false)\n |-- wind6: double (nullable = false)\n |-- wind7: double (nullable = false)\n |-- wind8: double (nullable = false)\n |-- wind9: double (nullable = false)\n |-- wind10: double (nullable = false)\n |-- wind11: double (nullable = false)\n |-- wind12: double (nullable = false)\n |-- wind13: double (nullable = false)\n |-- wind14: double (nullable = false)\n |-- wind15: double (nullable = false)\n |-- wind16: double (nullable = false)\n |-- wind17: double (nullable = false)\n |-- wind18: double (nullable = false)\n |-- wind19: double (nullable = false)\n |-- wind20: double (nullable = false)\n |-- wind21: double (nullable = false)\n |-- wind22: double (nullable = false)\n |-- wind23: double (nullable = false)\n |-- wind24: double (nullable = false)\n |-- wind25: double (nullable = false)\n |-- wind26: double (nullable = false)\n |-- wind27: double (nullable = false)\n |-- wind28: double (nullable = false)\n |-- wind29: double (nullable = false)\n |-- wind30: double (nullable = false)\n |-- wind31: double (nullable = false)\n |-- wind32: double (nullable = false)\n |-- wind33: double (nullable = false)\n |-- wind34: double (nullable = false)\n |-- wind35: double (nullable = false)\n |-- wind36: double (nullable = false)\n |-- wind37: double (nullable = false)\n |-- wind38: double (nullable = false)\n |-- wind39: double (nullable = false)\n |-- wind40: double (nullable = false)\n |-- wind41: double (nullable = false)\n |-- wind42: double (nullable = false)\n |-- wind43: double (nullable = false)\n |-- wind44: double (nullable = false)\n |-- wind45: double (nullable = false)\n |-- wind46: double (nullable = false)\n |-- wind47: double (nullable = false)\n |-- wind 48 Avg. over 6h: double (nullable = false)\n |-- wind 49 Avg. over 6h: double (nullable = false)\n |-- wind 50 Avg. over 6h: double (nullable = false)\n |-- wind 51 Avg. over 6h: double (nullable = false)\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1250","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1251","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1252","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1253","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1254"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559249631058_1551676240","id":"20190530-225351_364238333","dateCreated":"2019-05-30T22:53:51+0200","dateStarted":"2019-06-11T02:53:25+0200","dateFinished":"2019-06-11T03:02:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%spark2.pyspark\n##save back\ndf_final.coalesce(1).write.csv(\"/user/hadoop/preparedDataframe015\", header=True)\n","user":"admin","dateUpdated":"2019-06-11T02:24:08+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1255"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559400371895_1123902302","id":"20190601-164611_12466624","dateCreated":"2019-06-01T16:46:11+0200","dateStarted":"2019-06-11T02:53:26+0200","dateFinished":"2019-06-11T03:03:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"%spark2.pyspark\n","user":"admin","dateUpdated":"2019-06-09T03:29:20+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560043760725_1361635468","id":"20190609-032920_1005773725","dateCreated":"2019-06-09T03:29:20+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:199"}],"name":"DFGeneration","id":"2EE43ZRBW","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}