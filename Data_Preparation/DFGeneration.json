{"paragraphs":[{"text":"%md\n###DataFrame Generation###\nProblem: resolve the data on hourly level\nIdea: use a sliding window and move the window according to the wished length of hours\n- Get rid of the json format in the weather hourly data\n- Prepare and add hourly usage of a given station\n- Merge weather and usage to one DF\n- Resolve `null` values\n","user":"admin","dateUpdated":"2019-05-20T10:29:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>DataFrame Generation</h3>\n<p>Problem: resolve the data on hourly level\n<br  />Idea: use a sliding window and move the window according to the wished length of hours</p>\n<ul>\n<li>Get rid of the json format in the weather hourly data</li>\n<li>Prepare and add hourly usage of a given station</li>\n<li>Merge weather and usage to one DF</li>\n<li>Resolve <code>null</code> values</li>\n</ul>\n"}]},"apps":[],"jobName":"paragraph_1558311442059_-537161341","id":"20190520-021722_1715877619","dateCreated":"2019-05-20T02:17:22+0200","dateStarted":"2019-05-20T10:29:14+0200","dateFinished":"2019-05-20T10:29:17+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7985"},{"text":"%sh\nd_name=/user/hadoop/\nf_name=\"test\"\nhdfs dfs -count $d_name\na=0\nfor i in $(hdfs dfs -ls $d_name)\ndo\n let \"a += 1\"\n #echo $a\n echo \"Räume auf...\"\n hdfs dfs -rm -r $d_name$f_name$a\ndone\necho $d_name\n","user":"admin","dateUpdated":"2019-05-28T09:10:58+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"          42          274        14780201860 /user/hadoop\nMove test file to trash\nrm: `/user/hadoop/test1': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test2': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test3': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test4': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test5': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test6': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test7': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test8': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test9': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test10': No such file or directory\nMove test file to trash\n19/05/20 10:58:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test11' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test11\nMove test file to trash\nrm: `/user/hadoop/test12': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test13': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test14': No such file or directory\nMove test file to trash\n19/05/20 10:58:26 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test15' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test15\nMove test file to trash\n19/05/20 10:58:29 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test16' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test16\nMove test file to trash\n19/05/20 10:58:32 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test17' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test17\nMove test file to trash\nrm: `/user/hadoop/test18': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test19': No such file or directory\nMove test file to trash\n19/05/20 10:58:40 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test20' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test20\nMove test file to trash\n19/05/20 10:58:43 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test21' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test21\nMove test file to trash\n19/05/20 10:58:46 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test22' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test22\nMove test file to trash\n19/05/20 10:58:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test23' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test23\nMove test file to trash\n19/05/20 10:58:53 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test24' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test24\nMove test file to trash\n19/05/20 10:58:57 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test25' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test25\nMove test file to trash\nrm: `/user/hadoop/test26': No such file or directory\nMove test file to trash\n19/05/20 10:59:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test27' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test27\nMove test file to trash\n19/05/20 10:59:05 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test28' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test28\nMove test file to trash\n19/05/20 10:59:08 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test29' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test29\nMove test file to trash\n19/05/20 10:59:11 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test30' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test30\nMove test file to trash\n19/05/20 10:59:14 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test31' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test31\nMove test file to trash\n19/05/20 10:59:17 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test32' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test32\nMove test file to trash\n19/05/20 10:59:21 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test33' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test33\nMove test file to trash\nrm: `/user/hadoop/test34': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test35': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test36': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test37': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test38': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test39': No such file or directory\nMove test file to trash\n19/05/20 10:59:42 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test40' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test40\nMove test file to trash\n19/05/20 10:59:45 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test41' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test41\nMove test file to trash\n19/05/20 10:59:48 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test42' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test42\nMove test file to trash\n19/05/20 10:59:52 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test43' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test43\nMove test file to trash\nrm: `/user/hadoop/test44': No such file or directory\nMove test file to trash\n19/05/20 10:59:59 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test45' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test45\nMove test file to trash\n19/05/20 11:00:02 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test46' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test46\nMove test file to trash\n19/05/20 11:00:05 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test47' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test47\nMove test file to trash\n19/05/20 11:00:08 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test48' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test48\nMove test file to trash\n19/05/20 11:00:11 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test49' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test49\nMove test file to trash\n19/05/20 11:00:13 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test50' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test50\nMove test file to trash\n19/05/20 11:00:16 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test51' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test51\nMove test file to trash\n19/05/20 11:00:20 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test52' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test52\nMove test file to trash\n19/05/20 11:00:25 INFO fs.TrashPolicyDefault: Moved: 'hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/hadoop/test53' to trash at: hdfs://i-hadoop-01.informatik.hs-ulm.de:8020/user/zeppelin/.Trash/Current/user/hadoop/test53\nMove test file to trash\nrm: `/user/hadoop/test54': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test55': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test56': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test57': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test58': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test59': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test60': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test61': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test62': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test63': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test64': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test65': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test66': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test67': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test68': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test69': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test70': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test71': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test72': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test73': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test74': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test75': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test76': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test77': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test78': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test79': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test80': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test81': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test82': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test83': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test84': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test85': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test86': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test87': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test88': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test89': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test90': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test91': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test92': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test93': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test94': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test95': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test96': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test97': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test98': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test99': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test100': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test101': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test102': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test103': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test104': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test105': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test106': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test107': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test108': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test109': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test110': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test111': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test112': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test113': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test114': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test115': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test116': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test117': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test118': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test119': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test120': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test121': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test122': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test123': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test124': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test125': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test126': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test127': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test128': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test129': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test130': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test131': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test132': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test133': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test134': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test135': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test136': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test137': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test138': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test139': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test140': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test141': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test142': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test143': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test144': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test145': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test146': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test147': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test148': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test149': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test150': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test151': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test152': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test153': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test154': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test155': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test156': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test157': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test158': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test159': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test160': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test161': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test162': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test163': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test164': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test165': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test166': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test167': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test168': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test169': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test170': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test171': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test172': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test173': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test174': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test175': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test176': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test177': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test178': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test179': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test180': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test181': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test182': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test183': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test184': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test185': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test186': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test187': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test188': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test189': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test190': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test191': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test192': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test193': No such file or directory\nMove test file to trash\nrm: `/user/hadoop/test194': No such file or directory\nMove test file to trash\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1558341692370_-1255032650","id":"20190520-104132_920406303","dateCreated":"2019-05-20T10:41:32+0200","dateStarted":"2019-05-20T10:57:34+0200","dateFinished":"2019-05-20T11:07:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7986"},{"text":"%sh\n\n# Remove old dataset if already exists in hadoop /user/hadoop directory\nd_name=/user/hadoop/usage-stats/\n\nhdfs dfs -test -d $d_name\n    if [ $? != 0 ]\n            then\n                echo \"Move folder to HDFS...\"\n                hdfs dfs -put /home/hadoop/TFL_Cycling_Data_raw/usage-stats /user/hadoop/usage-stats\n                echo \"Folder successfully uploaded to HDFS\"\n                hdfs dfs -ls $d_name\n            else\n                echo \"Directory already present in HDFS\"\n                echo \"Move folder to trash\"\n                hdfs dfs -rm -r -skipTrash $d_name\n                echo \"$d_name\" \"has been deleted\"\n    fi","user":"admin","dateUpdated":"2019-05-20T10:29:50+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Move folder to HDFS...\nFolder successfully uploaded to HDFS\nFound 188 items\n-rw-r--r--   3 zeppelin hdfs   34451153 2019-05-20 10:29 /user/hadoop/usage-stats/01aJourneyDataExtract10Jan16-23Jan16.csv\n-rw-r--r--   3 zeppelin hdfs   35486081 2019-05-20 10:29 /user/hadoop/usage-stats/01b Journey Data Extract 24Jan16-06Feb16.csv\n-rw-r--r--   3 zeppelin hdfs   35486081 2019-05-20 10:29 /user/hadoop/usage-stats/01bJourneyDataExtract24Jan16-06Feb16.csv\n-rw-r--r--   3 zeppelin hdfs   33287674 2019-05-20 10:29 /user/hadoop/usage-stats/02aJourneyDataExtract07Fe16-20Feb2016.csv\n-rw-r--r--   3 zeppelin hdfs   34882871 2019-05-20 10:29 /user/hadoop/usage-stats/02bJourneyDataExtract21Feb16-05Mar2016.csv\n-rw-r--r--   3 zeppelin hdfs   67717329 2019-05-20 10:29 /user/hadoop/usage-stats/03JourneyDataExtract06Mar2016-31Mar2016.csv\n-rw-r--r--   3 zeppelin hdfs   94572397 2019-05-20 10:29 /user/hadoop/usage-stats/04JourneyDataExtract01Apr2016-30Apr2016.csv\n-rw-r--r--   3 zeppelin hdfs   71655072 2019-05-20 10:30 /user/hadoop/usage-stats/05JourneyDataExtract01May2016-17May2016.csv\n-rw-r--r--   3 zeppelin hdfs   27903078 2019-05-20 10:30 /user/hadoop/usage-stats/06JourneyDataExtract18May2016-24May2016.csv\n-rw-r--r--   3 zeppelin hdfs   25920903 2019-05-20 10:30 /user/hadoop/usage-stats/07JourneyDataExtract25May2016-31May2016.csv\n-rw-r--r--   3 zeppelin hdfs   28771414 2019-05-20 10:30 /user/hadoop/usage-stats/08JourneyDataExtract01Jun2016-07Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   27477331 2019-05-20 10:30 /user/hadoop/usage-stats/09JourneyDataExtract08Jun2016-14Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   17945922 2019-05-20 10:30 /user/hadoop/usage-stats/100JourneyDataExtract07Mar2018-13Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   17008201 2019-05-20 10:30 /user/hadoop/usage-stats/101JourneyDataExtract14Mar2018-20Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   20611024 2019-05-20 10:30 /user/hadoop/usage-stats/102JourneyDataExtract21Mar2018-27Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   11698458 2019-05-20 10:30 /user/hadoop/usage-stats/103JourneyDataExtract28Mar2018-03Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   19569924 2019-05-20 10:30 /user/hadoop/usage-stats/104JourneyDataExtract04Apr2018-10Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   23764410 2019-05-20 10:30 /user/hadoop/usage-stats/105JourneyDataExtract11Apr2018-17Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   33599248 2019-05-20 10:30 /user/hadoop/usage-stats/106JourneyDataExtract18Apr2018-24Apr2018.csv\n-rw-r--r--   3 zeppelin hdfs   20983000 2019-05-20 10:30 /user/hadoop/usage-stats/107JourneyDataExtract25Apr2018-01May2018.csv\n-rw-r--r--   3 zeppelin hdfs   31624193 2019-05-20 10:30 /user/hadoop/usage-stats/108JourneyDataExtract02May2018-08May2018.csv\n-rw-r--r--   3 zeppelin hdfs   29317958 2019-05-20 10:30 /user/hadoop/usage-stats/109JourneyDataExtract09May2018-15May2018.csv\n-rw-r--r--   3 zeppelin hdfs   27103628 2019-05-20 10:30 /user/hadoop/usage-stats/10JourneyDataExtract15Jun2016-21Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   53052751 2019-05-20 10:30 /user/hadoop/usage-stats/10a-Journey-Data-Extract-20Sep15-03Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   49626229 2019-05-20 10:30 /user/hadoop/usage-stats/10b-Journey-Data-Extract-04Oct15-17Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   32103278 2019-05-20 10:30 /user/hadoop/usage-stats/110JourneyDataExtract16May2018-22May2018.csv\n-rw-r--r--   3 zeppelin hdfs   28111400 2019-05-20 10:30 /user/hadoop/usage-stats/111JourneyDataExtract23May2018-29May2018.csv\n-rw-r--r--   3 zeppelin hdfs   31383816 2019-05-20 10:30 /user/hadoop/usage-stats/112JourneyDataExtract30May2018-05June2018.csv\n-rw-r--r--   3 zeppelin hdfs   32989219 2019-05-20 10:30 /user/hadoop/usage-stats/113JourneyDataExtract06June2018-12June2018.csv\n-rw-r--r--   3 zeppelin hdfs   31084740 2019-05-20 10:30 /user/hadoop/usage-stats/114JourneyDataExtract13June2018-19June2018.csv\n-rw-r--r--   3 zeppelin hdfs   35340649 2019-05-20 10:30 /user/hadoop/usage-stats/115JourneyDataExtract20June2018-26June2018.csv\n-rw-r--r--   3 zeppelin hdfs   35471699 2019-05-20 10:30 /user/hadoop/usage-stats/116JourneyDataExtract27June2018-03July2018.csv\n-rw-r--r--   3 zeppelin hdfs   35034559 2019-05-20 10:30 /user/hadoop/usage-stats/117JourneyDataExtract04July2018-10July2018.csv\n-rw-r--r--   3 zeppelin hdfs   34344069 2019-05-20 10:30 /user/hadoop/usage-stats/118JourneyDataExtract11July2018-17July2018.csv\n-rw-r--r--   3 zeppelin hdfs   34922018 2019-05-20 10:30 /user/hadoop/usage-stats/119JourneyDataExtract18July2018-24July2018.csv\n-rw-r--r--   3 zeppelin hdfs   27282325 2019-05-20 10:30 /user/hadoop/usage-stats/11JourneyDataExtract22Jun2016-28Jun2016.csv\n-rw-r--r--   3 zeppelin hdfs   44970052 2019-05-20 10:30 /user/hadoop/usage-stats/11a-Journey-Data-Extract-18Oct15-31Oct15.csv\n-rw-r--r--   3 zeppelin hdfs   41144797 2019-05-20 10:30 /user/hadoop/usage-stats/11b-Journey-Data-Extract-01Nov15-14Nov15.csv\n-rw-r--r--   3 zeppelin hdfs   31772001 2019-05-20 10:30 /user/hadoop/usage-stats/120JourneyDataExtract25July2018-31July2018.csv\n-rw-r--r--   3 zeppelin hdfs   35021999 2019-05-20 10:30 /user/hadoop/usage-stats/121JourneyDataExtract01Aug2018-07Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   27192201 2019-05-20 10:30 /user/hadoop/usage-stats/122JourneyDataExtract08Aug2018-14Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   29937814 2019-05-20 10:30 /user/hadoop/usage-stats/123JourneyDataExtract15Aug2018-21Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   23412636 2019-05-20 10:30 /user/hadoop/usage-stats/124JourneyDataExtract22Aug2018-28Aug2018.csv\n-rw-r--r--   3 zeppelin hdfs   29254055 2019-05-20 10:30 /user/hadoop/usage-stats/125JourneyDataExtract29Aug2018-04Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   28857237 2019-05-20 10:30 /user/hadoop/usage-stats/126JourneyDataExtract05Sep2018-11Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   30181508 2019-05-20 10:30 /user/hadoop/usage-stats/127JourneyDataExtract12Sep2018-18Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   24268772 2019-05-20 10:30 /user/hadoop/usage-stats/128JourneyDataExtract19Sep2018-25Sep2018.csv\n-rw-r--r--   3 zeppelin hdfs   29482432 2019-05-20 10:30 /user/hadoop/usage-stats/129JourneyDataExtract26Sep2018-02Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   29674143 2019-05-20 10:30 /user/hadoop/usage-stats/12JourneyDataExtract29Jun2016-05Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   35179761 2019-05-20 10:30 /user/hadoop/usage-stats/12aJourneyDataExtract15Nov15-27Nov15.csv\n-rw-r--r--   3 zeppelin hdfs   39630539 2019-05-20 10:30 /user/hadoop/usage-stats/12bJourneyDataExtract28Nov15-12Dec15.csv\n-rw-r--r--   3 zeppelin hdfs   27050318 2019-05-20 10:30 /user/hadoop/usage-stats/130JourneyDataExtract03Oct2018-09Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   26875110 2019-05-20 10:30 /user/hadoop/usage-stats/131JourneyDataExtract10Oct2018-16Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   28228103 2019-05-20 10:30 /user/hadoop/usage-stats/132JourneyDataExtract17Oct2018-23Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   22841542 2019-05-20 10:30 /user/hadoop/usage-stats/133JourneyDataExtract24Oct2018-30Oct2018.csv\n-rw-r--r--   3 zeppelin hdfs   24277034 2019-05-20 10:30 /user/hadoop/usage-stats/134JourneyDataExtract31Oct2018-06Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   21479754 2019-05-20 10:30 /user/hadoop/usage-stats/135JourneyDataExtract07Nov2018-13Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   18968325 2019-05-20 10:30 /user/hadoop/usage-stats/136JourneyDataExtract14Nov2018-20Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   18804556 2019-05-20 10:30 /user/hadoop/usage-stats/137JourneyDataExtract21Nov2018-27Nov2018.csv\n-rw-r--r--   3 zeppelin hdfs   19409452 2019-05-20 10:30 /user/hadoop/usage-stats/138JourneyDataExtract28Nov2018-04Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   17720079 2019-05-20 10:30 /user/hadoop/usage-stats/139JourneyDataExtract05Dec2018-11Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   29899103 2019-05-20 10:30 /user/hadoop/usage-stats/13JourneyDataExtract06Jul2016-12Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   28981434 2019-05-20 10:30 /user/hadoop/usage-stats/13aJourneyDataExtract13Dec15-24Dec15.csv\n-rw-r--r--   3 zeppelin hdfs   30064333 2019-05-20 10:30 /user/hadoop/usage-stats/13bJourneyDataExtract25Dec15-09Jan16.csv\n-rw-r--r--   3 zeppelin hdfs   17156314 2019-05-20 10:30 /user/hadoop/usage-stats/140JourneyDataExtract12Dec2018-18Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   15629698 2019-05-20 10:30 /user/hadoop/usage-stats/141JourneyDataExtract19Dec2018-25Dec2018.csv\n-rw-r--r--   3 zeppelin hdfs   11287217 2019-05-20 10:30 /user/hadoop/usage-stats/142JourneyDataExtract26Dec2018-01Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   17278475 2019-05-20 10:30 /user/hadoop/usage-stats/143JourneyDataExtract02Jan2019-08Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   20381535 2019-05-20 10:30 /user/hadoop/usage-stats/144JourneyDataExtract09Jan2019-15Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   19152163 2019-05-20 10:30 /user/hadoop/usage-stats/145JourneyDataExtract16Jan2019-22Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   18065288 2019-05-20 10:30 /user/hadoop/usage-stats/146JourneyDataExtract23Jan2019-29Jan2019.csv\n-rw-r--r--   3 zeppelin hdfs   16157228 2019-05-20 10:30 /user/hadoop/usage-stats/147JourneyDataExtract30Jan2019-05Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   18602268 2019-05-20 10:30 /user/hadoop/usage-stats/148JourneyDataExtract06Feb2019-12Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   22287567 2019-05-20 10:30 /user/hadoop/usage-stats/149JourneyDataExtract13Feb2019-19Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   34977660 2019-05-20 10:30 /user/hadoop/usage-stats/14JourneyDataExtract13Jul2016-19Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   25452590 2019-05-20 10:30 /user/hadoop/usage-stats/150JourneyDataExtract20Feb2019-26Feb2019.csv\n-rw-r--r--   3 zeppelin hdfs   21590398 2019-05-20 10:30 /user/hadoop/usage-stats/151JourneyDataExtract27Feb2019-05Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   18696462 2019-05-20 10:30 /user/hadoop/usage-stats/152JourneyDataExtract06Mar2019-12Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   19369083 2019-05-20 10:30 /user/hadoop/usage-stats/153JourneyDataExtract13Mar2019-19Mar2019.csv\n-rw-r--r--   3 zeppelin hdfs   35583327 2019-05-20 10:30 /user/hadoop/usage-stats/15JourneyDataExtract20Jul2016-26Jul2016.csv\n-rw-r--r--   3 zeppelin hdfs   32360902 2019-05-20 10:30 /user/hadoop/usage-stats/16JourneyDataExtract27Jul2016-02Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   33998852 2019-05-20 10:30 /user/hadoop/usage-stats/17JourneyDataExtract03Aug2016-09Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   35101289 2019-05-20 10:30 /user/hadoop/usage-stats/18JourneyDataExtract10Aug2016-16Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   29162758 2019-05-20 10:30 /user/hadoop/usage-stats/19JourneyDataExtract17Aug2016-23Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   32091911 2019-05-20 10:30 /user/hadoop/usage-stats/1a.JourneyDataExtract04Jan15-17Jan15.csv\n-rw-r--r--   3 zeppelin hdfs   34103997 2019-05-20 10:30 /user/hadoop/usage-stats/1b.JourneyDataExtract18Jan15-31Jan15.csv\n-rw-r--r--   3 zeppelin hdfs  208701113 2019-05-20 10:30 /user/hadoop/usage-stats/2015TripDatazip.zip\n-rw-r--r--   3 zeppelin hdfs  222067537 2019-05-20 10:30 /user/hadoop/usage-stats/2016TripDataZip.zip\n-rw-r--r--   3 zeppelin hdfs   30509039 2019-05-20 10:30 /user/hadoop/usage-stats/20JourneyDataExtract24Aug2016-30Aug2016.csv\n-rw-r--r--   3 zeppelin hdfs   29461893 2019-05-20 10:30 /user/hadoop/usage-stats/21JourneyDataExtract31Aug2016-06Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   31357377 2019-05-20 10:30 /user/hadoop/usage-stats/22JourneyDataExtract07Sep2016-13Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   28847793 2019-05-20 10:30 /user/hadoop/usage-stats/23JourneyDataExtract14Sep2016-20Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   30932775 2019-05-20 10:30 /user/hadoop/usage-stats/24JourneyDataExtract21Sep2016-27Sep2016.csv\n-rw-r--r--   3 zeppelin hdfs   27343703 2019-05-20 10:30 /user/hadoop/usage-stats/25JourneyDataExtract28Sep2016-04Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   27338225 2019-05-20 10:30 /user/hadoop/usage-stats/26JourneyDataExtract05Oct2016-11Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   24271970 2019-05-20 10:30 /user/hadoop/usage-stats/27JourneyDataExtract12Oct2016-18Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   25570755 2019-05-20 10:30 /user/hadoop/usage-stats/28JourneyDataExtract19Oct2016-25Oct2016.csv\n-rw-r--r--   3 zeppelin hdfs   26860667 2019-05-20 10:30 /user/hadoop/usage-stats/29JourneyDataExtract26Oct2016-01Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   32961852 2019-05-20 10:30 /user/hadoop/usage-stats/2a.JourneyDataExtract01Feb15-14Feb15.csv\n-rw-r--r--   3 zeppelin hdfs   32811094 2019-05-20 10:30 /user/hadoop/usage-stats/2b.JourneyDataExtract15Feb15-28Feb15.csv\n-rw-r--r--   3 zeppelin hdfs   19636191 2019-05-20 10:30 /user/hadoop/usage-stats/30JourneyDataExtract02Nov2016-08Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   18801936 2019-05-20 10:30 /user/hadoop/usage-stats/31JourneyDataExtract09Nov2016-15Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   19129013 2019-05-20 10:30 /user/hadoop/usage-stats/32JourneyDataExtract16Nov2016-22Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   21182586 2019-05-20 10:30 /user/hadoop/usage-stats/33JourneyDataExtract23Nov2016-29Nov2016.csv\n-rw-r--r--   3 zeppelin hdfs   20338179 2019-05-20 10:30 /user/hadoop/usage-stats/34JourneyDataExtract30Nov2016-06Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   20959011 2019-05-20 10:30 /user/hadoop/usage-stats/35JourneyDataExtract07Dec2016-13Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   20169339 2019-05-20 10:30 /user/hadoop/usage-stats/36JourneyDataExtract14Dec2016-20Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   15407215 2019-05-20 10:30 /user/hadoop/usage-stats/37JourneyDataExtract21Dec2016-27Dec2016.csv\n-rw-r--r--   3 zeppelin hdfs   10688750 2019-05-20 10:30 /user/hadoop/usage-stats/38JourneyDataExtract28Dec2016-03Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   19106889 2019-05-20 10:30 /user/hadoop/usage-stats/39JourneyDataExtract04Jan2017-10Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   41814096 2019-05-20 10:30 /user/hadoop/usage-stats/3a.JourneyDataExtract01Mar15-15Mar15.csv\n-rw-r--r--   3 zeppelin hdfs   42438904 2019-05-20 10:30 /user/hadoop/usage-stats/3b.JourneyDataExtract16Mar15-31Mar15.csv\n-rw-r--r--   3 zeppelin hdfs   16105427 2019-05-20 10:30 /user/hadoop/usage-stats/40JourneyDataExtract11Jan2017-17Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   19613581 2019-05-20 10:30 /user/hadoop/usage-stats/41JourneyDataExtract18Jan2017-24Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   17726211 2019-05-20 10:30 /user/hadoop/usage-stats/42JourneyDataExtract25Jan2017-31Jan2017.csv\n-rw-r--r--   3 zeppelin hdfs   18756158 2019-05-20 10:30 /user/hadoop/usage-stats/43JourneyDataExtract01Feb2017-07Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   16632899 2019-05-20 10:30 /user/hadoop/usage-stats/44JourneyDataExtract08Feb2017-14Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   21179040 2019-05-20 10:30 /user/hadoop/usage-stats/45JourneyDataExtract15Feb2017-21Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   18191104 2019-05-20 10:30 /user/hadoop/usage-stats/46JourneyDataExtract22Feb2017-28Feb2017.csv\n-rw-r--r--   3 zeppelin hdfs   19708533 2019-05-20 10:30 /user/hadoop/usage-stats/47JourneyDataExtract01Mar2017-07Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   23409196 2019-05-20 10:30 /user/hadoop/usage-stats/48JourneyDataExtract08Mar2017-14Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   10939999 2019-05-20 10:30 /user/hadoop/usage-stats/49JourneyDataExtract15Mar2017-21Mar2017.xlsx\n-rw-r--r--   3 zeppelin hdfs   51464075 2019-05-20 10:30 /user/hadoop/usage-stats/4a.JourneyDataExtract01Apr15-16Apr15.csv\n-rw-r--r--   3 zeppelin hdfs   55715516 2019-05-20 10:30 /user/hadoop/usage-stats/4b.JourneyDataExtract 17Apr15-02May15.csv\n-rw-r--r--   3 zeppelin hdfs   23600415 2019-05-20 10:30 /user/hadoop/usage-stats/50 Journey Data Extract 22Mar2017-28Mar2017.csv\n-rw-r--r--   3 zeppelin hdfs   26607449 2019-05-20 10:30 /user/hadoop/usage-stats/51 Journey Data Extract 29Mar2017-04Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   30823972 2019-05-20 10:30 /user/hadoop/usage-stats/52 Journey Data Extract 05Apr2017-11Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   22222026 2019-05-20 10:30 /user/hadoop/usage-stats/53JourneyDataExtract12Apr2017-18Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   26147246 2019-05-20 10:30 /user/hadoop/usage-stats/54JourneyDataExtract19Apr2017-25Apr2017.csv\n-rw-r--r--   3 zeppelin hdfs   23432931 2019-05-20 10:30 /user/hadoop/usage-stats/55JourneyData Extract26Apr2017-02May2017.csv\n-rw-r--r--   3 zeppelin hdfs   25584245 2019-05-20 10:30 /user/hadoop/usage-stats/56JourneyDataExtract 03May2017-09May2017.csv\n-rw-r--r--   3 zeppelin hdfs   27201854 2019-05-20 10:30 /user/hadoop/usage-stats/57JourneyDataExtract10May2017-16May2017.csv\n-rw-r--r--   3 zeppelin hdfs   25599256 2019-05-20 10:30 /user/hadoop/usage-stats/58JourneyDataExtract17May2017-23May2017.csv\n-rw-r--r--   3 zeppelin hdfs   31127688 2019-05-20 10:30 /user/hadoop/usage-stats/59JourneyDataExtract24May2017-30May2017.csv\n-rw-r--r--   3 zeppelin hdfs   48750814 2019-05-20 10:30 /user/hadoop/usage-stats/5a.JourneyDataExtract03May15-16May15.csv\n-rw-r--r--   3 zeppelin hdfs   51948160 2019-05-20 10:30 /user/hadoop/usage-stats/5b.JourneyDataExtract17May15-30May15.csv\n-rw-r--r--   3 zeppelin hdfs   28357343 2019-05-20 10:30 /user/hadoop/usage-stats/60JourneyDataExtract31May2017-06Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   30163497 2019-05-20 10:30 /user/hadoop/usage-stats/61JourneyDataExtract07Jun2017-13Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   35548875 2019-05-20 10:30 /user/hadoop/usage-stats/62JourneyDataExtract14Jun2017-20Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   30496272 2019-05-20 10:30 /user/hadoop/usage-stats/63JourneyDataExtract21Jun2017-27Jun2017.csv\n-rw-r--r--   3 zeppelin hdfs   33295260 2019-05-20 10:30 /user/hadoop/usage-stats/64JourneyDataExtract28Jun2017-04Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   34054170 2019-05-20 10:30 /user/hadoop/usage-stats/65JourneyDataExtract05Jul2017-11Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   32194296 2019-05-20 10:31 /user/hadoop/usage-stats/66JourneyDataExtract12Jul2017-18Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   27980289 2019-05-20 10:31 /user/hadoop/usage-stats/67JourneyDataExtract19Jul2017-25Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   23363051 2019-05-20 10:31 /user/hadoop/usage-stats/68JourneyDataExtract26Jul2017-31Jul2017.csv\n-rw-r--r--   3 zeppelin hdfs   27444824 2019-05-20 10:31 /user/hadoop/usage-stats/69JourneyDataExtract01Aug2017-07Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   51513415 2019-05-20 10:31 /user/hadoop/usage-stats/6aJourneyDataExtract31May15-12Jun15.csv\n-rw-r--r--   3 zeppelin hdfs   62770968 2019-05-20 10:31 /user/hadoop/usage-stats/6bJourneyDataExtract13Jun15-27Jun15.csv\n-rw-r--r--   3 zeppelin hdfs   26210762 2019-05-20 10:31 /user/hadoop/usage-stats/70JourneyDataExtract08Aug2017-14Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   30238704 2019-05-20 10:31 /user/hadoop/usage-stats/71JourneyDataExtract15Aug2017-22Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   28921544 2019-05-20 10:31 /user/hadoop/usage-stats/72JourneyDataExtract23Aug2017-29Aug2017.csv\n-rw-r--r--   3 zeppelin hdfs   25301473 2019-05-20 10:31 /user/hadoop/usage-stats/73JourneyDataExtract30Aug2017-05Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   24275323 2019-05-20 10:31 /user/hadoop/usage-stats/74JourneyDataExtract06Sep2017-12Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   25497631 2019-05-20 10:31 /user/hadoop/usage-stats/75JourneyDataExtract13Sep2017-19Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   27743176 2019-05-20 10:31 /user/hadoop/usage-stats/76JourneyDataExtract20Sep2017-26Sep2017.csv\n-rw-r--r--   3 zeppelin hdfs   26348497 2019-05-20 10:31 /user/hadoop/usage-stats/77JourneyDataExtract27Sep2017-03Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   27510237 2019-05-20 10:31 /user/hadoop/usage-stats/78JourneyDataExtract04Oct2017-10Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   28322922 2019-05-20 10:31 /user/hadoop/usage-stats/79JourneyDataExtract11Oct2017-17Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   69560935 2019-05-20 10:31 /user/hadoop/usage-stats/7a.JourneyDataExtract28Jun15-11Jul15.csv\n-rw-r--r--   3 zeppelin hdfs   57317508 2019-05-20 10:31 /user/hadoop/usage-stats/7b.JourneyDataExtract12Jul15-25Jul15.csv\n-rw-r--r--   3 zeppelin hdfs   23390665 2019-05-20 10:31 /user/hadoop/usage-stats/80JourneyDataExtract18Oct2017-24Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   25260806 2019-05-20 10:31 /user/hadoop/usage-stats/81JourneyDataExtract25Oct2017-31Oct2017.csv\n-rw-r--r--   3 zeppelin hdfs   24124382 2019-05-20 10:31 /user/hadoop/usage-stats/82JourneyDataExtract01Nov2017-07Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21684181 2019-05-20 10:31 /user/hadoop/usage-stats/83JourneyDataExtract08Nov2017-14Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21630715 2019-05-20 10:31 /user/hadoop/usage-stats/84JourneyDataExtract15Nov2017-21Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   21045620 2019-05-20 10:31 /user/hadoop/usage-stats/85JourneyDataExtract22Nov2017-28Nov2017.csv\n-rw-r--r--   3 zeppelin hdfs   19704514 2019-05-20 10:31 /user/hadoop/usage-stats/86JourneyDataExtract29Nov2017-05Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   16113127 2019-05-20 10:31 /user/hadoop/usage-stats/87JourneyDataExtract06Dec2017-12Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   15695037 2019-05-20 10:31 /user/hadoop/usage-stats/88JourneyDataExtract13Dec2017-19Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   13306018 2019-05-20 10:31 /user/hadoop/usage-stats/89JourneyDataExtract20Dec2017-26Dec2017.csv\n-rw-r--r--   3 zeppelin hdfs   59614317 2019-05-20 10:31 /user/hadoop/usage-stats/8a-Journey-Data-Extract-26Jul15-07Aug15.csv\n-rw-r--r--   3 zeppelin hdfs   62706885 2019-05-20 10:31 /user/hadoop/usage-stats/8b-Journey-Data-Extract-08Aug15-22Aug15.csv\n-rw-r--r--   3 zeppelin hdfs    8384731 2019-05-20 10:31 /user/hadoop/usage-stats/90JourneyDataExtract27Dec2017-02Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   16006442 2019-05-20 10:31 /user/hadoop/usage-stats/91JourneyDataExtract03Jan2018-09Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   18618252 2019-05-20 10:31 /user/hadoop/usage-stats/92JourneyDataExtract10Jan2018-16Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   17591748 2019-05-20 10:31 /user/hadoop/usage-stats/93JourneyDataExtract17Jan2018-23Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   19317042 2019-05-20 10:31 /user/hadoop/usage-stats/94JourneyDataExtract24Jan2018-30Jan2018.csv\n-rw-r--r--   3 zeppelin hdfs   17621910 2019-05-20 10:31 /user/hadoop/usage-stats/95JourneyDataExtract31Jan2018-06Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   17072972 2019-05-20 10:31 /user/hadoop/usage-stats/96JourneyDataExtract07Feb2018-13Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   18701361 2019-05-20 10:31 /user/hadoop/usage-stats/97JourneyDataExtract14Feb2018-20Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   17237378 2019-05-20 10:31 /user/hadoop/usage-stats/98JourneyDataExtract21Feb2018-27Feb2018.csv\n-rw-r--r--   3 zeppelin hdfs   11650126 2019-05-20 10:31 /user/hadoop/usage-stats/99JourneyDataExtract28Feb2018-06Mar2018.csv\n-rw-r--r--   3 zeppelin hdfs   43178453 2019-05-20 10:31 /user/hadoop/usage-stats/9a-Journey-Data-Extract-23Aug15-05Sep15.csv\n-rw-r--r--   3 zeppelin hdfs   50958478 2019-05-20 10:31 /user/hadoop/usage-stats/9b-Journey-Data-Extract-06Sep15-19Sep15.csv\n-rw-r--r--   3 zeppelin hdfs  225391187 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2012.zip\n-rw-r--r--   3 zeppelin hdfs  183134189 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2013.zip\n-rw-r--r--   3 zeppelin hdfs  225215129 2019-05-20 10:31 /user/hadoop/usage-stats/cyclehireusagestats-2014.zip\n-rw-r--r--   3 zeppelin hdfs      10776 2019-05-20 10:31 /user/hadoop/usage-stats/cycling-load.json\n"}]},"apps":[],"jobName":"paragraph_1558311455928_647183211","id":"20190520-021735_735409098","dateCreated":"2019-05-20T02:17:35+0200","dateStarted":"2019-05-20T10:29:50+0200","dateFinished":"2019-05-20T10:31:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7987"},{"text":"%spark2.pyspark\n\nrdd_usage = spark.read.csv(\"/user/hadoop/usage-stats/*.csv\", header=True)\n#Took 12 seconds to read 38.122.372 lines\n#38.122.372 - 1 rows\n#rdd_usage.schema.names\n#print(rdd_usage.count())\nrdd_usage.show()\n\nrdd_usage = rdd_usage.na.fill(0)\n#sqlContext.clearCache()\n#rdd_usage.unpersist()","user":"admin","dateUpdated":"2019-05-28T09:21:18+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n|Rental Id|Duration|Bike Id|        End Date|EndStation Id|     EndStation Name|      Start Date|StartStation Id|   StartStation Name|\n+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n| 52538879|     660|    395|01/04/2016 00:11|          137|Bourne Street, Be...|01/04/2016 00:00|            573|Limerston Street,...|\n| 52538876|     420|  12931|01/04/2016 00:07|          507|Clarkson Street, ...|01/04/2016 00:00|            399|Brick Lane Market...|\n| 52538877|     420|   7120|01/04/2016 00:07|          507|Clarkson Street, ...|01/04/2016 00:00|            399|Brick Lane Market...|\n| 52538878|     300|   1198|01/04/2016 00:05|          616|Aintree Street, F...|01/04/2016 00:00|            599|Manbre Road, Hamm...|\n| 52538874|    1260|  10739|01/04/2016 00:21|          486|Granby Street, Sh...|01/04/2016 00:00|            135|Clerkenwell Green...|\n| 52538875|    1260|  10949|01/04/2016 00:21|          486|Granby Street, Sh...|01/04/2016 00:00|            135|Clerkenwell Green...|\n| 52538880|     540|   8831|01/04/2016 00:09|          411|Walworth Road, El...|01/04/2016 00:00|            193|Bankside Mix, Ban...|\n| 52538881|     600|   8778|01/04/2016 00:11|          770|Gwendwr Road, Wes...|01/04/2016 00:01|            266|Queen's Gate (Nor...|\n| 52538882|     540|    700|01/04/2016 00:10|          294|St. George's Squa...|01/04/2016 00:01|            137|Bourne Street, Be...|\n| 52538883|    3000|   5017|01/04/2016 00:51|           39|Shoreditch High S...|01/04/2016 00:01|            456|Parkway, Camden Town|\n| 52538885|     240|   4359|01/04/2016 00:06|          445|Cheshire Street, ...|01/04/2016 00:02|            444|Bethnal Green Gar...|\n| 52538884|    5340|   2801|01/04/2016 01:31|          288|Elizabeth Bridge,...|01/04/2016 00:02|            288|Elizabeth Bridge,...|\n| 52538887|     300|   9525|01/04/2016 00:08|          592|Bishop's Bridge R...|01/04/2016 00:03|            265|Southwick Street,...|\n| 52538888|     840|   8410|01/04/2016 00:17|          360|Howick Place, Wes...|01/04/2016 00:03|            174|      Strand, Strand|\n| 52538886|     600|  12206|01/04/2016 00:13|          607|Putney Bridge Sta...|01/04/2016 00:03|            738|Imperial Road, Sa...|\n| 52538891|     120|   2998|01/04/2016 00:06|          384|Marloes Road, Ken...|01/04/2016 00:04|            157|Wright's Lane, Ke...|\n| 52538893|     300|  10970|01/04/2016 00:09|          242|Beaumont Street, ...|01/04/2016 00:04|            239|Warren Street Sta...|\n| 52538890|    1200|   9350|01/04/2016 00:24|          517| Ford Road, Old Ford|01/04/2016 00:04|            789|Podium, Queen Eli...|\n| 52538889|    1800|   7005|01/04/2016 00:34|          583|Abingdon Green, W...|01/04/2016 00:04|            354|Northumberland Av...|\n| 52538892|    1800|   9368|01/04/2016 00:34|          583|Abingdon Green, W...|01/04/2016 00:04|            354|Northumberland Av...|\n+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=0","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=1","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=2","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=3"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558311715439_1870867428","id":"20190520-022155_1304622215","dateCreated":"2019-05-20T02:21:55+0200","dateStarted":"2019-05-28T09:21:19+0200","dateFinished":"2019-05-28T09:22:34+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7988"},{"text":"%spark2.pyspark\nrdd_usage.count()","user":"admin","dateUpdated":"2019-05-20T10:32:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"42826257\n"}]},"apps":[],"jobName":"paragraph_1558311735111_-280477074","id":"20190520-022215_2120296642","dateCreated":"2019-05-20T02:22:15+0200","dateStarted":"2019-05-20T10:32:14+0200","dateFinished":"2019-05-20T10:33:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7989"},{"text":"%spark2.pyspark\nrdd_weather = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\nrdd_weather.show()","user":"admin","dateUpdated":"2019-05-20T10:32:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-------------------+--------+---------+--------------------------+--------------------+\n|Start Date|      Daily Weather|Humidity|Windspeed|Apparent Temperature (Avg)|      Hourly Weather|\n+----------+-------------------+--------+---------+--------------------------+--------------------+\n|04.01.2015|                fog|    0.94|     0.55|                    36.295|[{'time': 1420329...|\n|05.01.2015|  partly-cloudy-day|    0.88|     1.59|                     46.74|[{'time': 1420416...|\n|06.01.2015|  partly-cloudy-day|    0.86|     2.07|         42.15000000000001|[{'time': 1420502...|\n|07.01.2015|partly-cloudy-night|    0.86|     4.13|                     45.45|[{'time': 1420588...|\n|08.01.2015|               rain|    0.87|      3.6|                      46.2|[{'time': 1420675...|\n|09.01.2015|  partly-cloudy-day|    0.81|     7.43|                    56.085|[{'time': 1420761...|\n|10.01.2015|  partly-cloudy-day|    0.74|     7.05|                    44.615|[{'time': 1420848...|\n|11.01.2015|partly-cloudy-night|    0.74|     5.44|                    43.385|[{'time': 1420934...|\n|12.01.2015|               rain|    0.83|     6.57|                    49.535|[{'time': 1421020...|\n|13.01.2015|  partly-cloudy-day|    0.84|     4.12|                     39.87|[{'time': 1421107...|\n|14.01.2015|partly-cloudy-night|    0.76|     5.04|                     42.13|[{'time': 1421193...|\n|15.01.2015|               rain|    0.77|     6.71|                     41.05|[{'time': 1421280...|\n|16.01.2015|  partly-cloudy-day|    0.76|     2.31|                      37.0|[{'time': 1421366...|\n|17.01.2015|  partly-cloudy-day|    0.83|     1.72|                     39.13|[{'time': 1421452...|\n|18.01.2015|  partly-cloudy-day|    0.89|     0.84|                     35.21|[{'time': 1421539...|\n|19.01.2015|  partly-cloudy-day|    0.85|     0.65|                     32.85|[{'time': 1421625...|\n|20.01.2015|partly-cloudy-night|    0.85|     1.38|                    36.285|[{'time': 1421712...|\n|21.01.2015|  partly-cloudy-day|    0.87|      2.6|                    35.595|[{'time': 1421798...|\n|22.01.2015|  partly-cloudy-day|    0.83|     0.83|                    32.875|[{'time': 1421884...|\n|23.01.2015|               rain|    0.85|     1.95|                    36.575|[{'time': 1421971...|\n+----------+-------------------+--------+---------+--------------------------+--------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1558311748207_1320908318","id":"20190520-022228_1404530849","dateCreated":"2019-05-20T02:22:28+0200","dateStarted":"2019-05-20T10:32:27+0200","dateFinished":"2019-05-20T10:33:25+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7990"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import from_json\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import unix_timestamp, hour, col, split, round\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\n\ndef generateWeatherDF(x):\n    \"\"\" Generates dataframe on hourly granularity \"\"\"\n\n    schema_hum = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"humidity\", FloatType(), True)]))\n        \n    schema_temp = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"apparentTemperature\", FloatType(), True)]))\n        \n    schema_wind = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"windSpeed\", FloatType(), True)]))\n                \n    schema_sum = ArrayType(\n        StructType([StructField(\"time\", StringType(), True), \n                StructField(\"icon\", StringType(), True)]))\n   \n\n    #adding past data from previous day\n    rdd_weather = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\n    columns_to_drop = [\"Hourly Weather\",\n                       \"Humidity\",\n                       \"Yesterday\",\n                       \"Daily Weather\",\n                       \"Apparent Temperature (Avg)\",\n                       \"Windspeed\"]\n                       \n    if x > 24:\n        y = 0\n        i = 0\n        b = False\n        \n        rdd_weather2 = spark.read.csv(\"/user/hadoop/weather_new.csv\", header=True, sep=\",\")\n        \n        for j in range(x):\n            print(\"Day \", j)\n            rdd_weather = rdd_weather.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy')) \n            #check for -y \n            print(\"Y\", y)\n            rdd_weather = rdd_weather.withColumn('Yesterday'+`j`, F.date_add(rdd_weather['Start Date'], y))\n            rdd_weather2 = rdd_weather2.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy'))\n            rdd_weather = rdd_weather.alias(\"t\").join(rdd_weather2, rdd_weather[\"Yesterday\"+`j`] == rdd_weather2[\"Start Date\"], how='left_outer').select(\"t.*\", rdd_weather2[\"Start Date\"].alias(\"Start Date\"+`j`), rdd_weather2[\"Hourly Weather\"].alias(\"Hourly Weather\"+`j`))\n            \n            columns_to_drop.append(\"Hourly Weather\"+`j`)\n            columns_to_drop.append(\"Start Date\"+`j`)\n            columns_to_drop.append(\"Yesterday\"+`j`)\n\n                \n            # alt -> neu oder von neu -> alt\n            if (x > 24):\n                x -= 24\n                if (b == False):\n                    for k in range(24):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n                    b = True\n                else:\n                    for k in range(24):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n            else:\n                for k in range(x):\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_sum)[k].getItem(\"icon\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_hum)[k].getItem(\"humidity\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_temp)[k].getItem(\"apparentTemperature\"))\n                        rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\")+`i`, from_json(rdd_weather[\"Hourly Weather\"+`j`], schema_wind)[k].getItem(\"windSpeed\"))\n                        i += 1\n                x = 0\n                break\n                \n            y -= 1\n\n    elif x <= 24 and x >= 0:\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"sum\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_sum)[i].getItem(\"icon\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"hum\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_hum)[i].getItem(\"humidity\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"temp\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_temp)[i].getItem(\"apparentTemperature\"))\n        for i in range(x):\n            rdd_weather = rdd_weather.withColumn(colChecker(i, \"wind\") +`i`, from_json(rdd_weather[\"Hourly Weather\"], schema_wind)[i].getItem(\"windSpeed\"))\n\n    else:\n        print(\"Invalid value\")\n     \n    rdd_weather = rdd_weather[sorted(rdd_weather.columns)]\n #   rdd_weather = rdd_weather.drop(*columns_to_drop)\n  #  rdd_weather.printSchema()\n    rdd_weather = rdd_weather.withColumn('Start Date',F.to_date(\"Start Date\", 'dd.MM.yyyy'))\n    \n    return rdd_weather\n    \ndef colChecker(i, val):\n    \"\"\"Checks the columns for correct ordering\"\"\"\n    if (i >= 10):\n        if (val == \"sum\"):\n            return \"sum\"\n        elif (val == \"hum\"):\n            return \"hum\"\n        elif (val == \"temp\"):\n            return \"temp\"\n        elif (val == \"wind\"):\n            return \"wind\"\n    else:\n        if (val == \"sum\"):\n            return \"sum0\"\n        elif (val == \"hum\"):\n            return \"hum0\"\n        elif (val == \"temp\"):\n            return \"temp0\"\n        elif (val == \"wind\"):\n            return \"wind0\"\n            \n\ndef generateRentalDF(station): \n    \"\"\"Generate cycling usage DF of either all stations or only one station\"\"\"\n    \n    df_usage = spark.read.csv(\"/user/hadoop/usage-stats/*.csv\", header=True)\n    df_usage = df_usage.na.fill(0)\n    \n    #One station, TODO:// all stations\n    df_usage = df_usage.filter(col(\"StartStation Id\").isin([''+`station`]))\n    \n    df_usage.dropna(subset=[\"StartStation Id\", \"EndStation Id\", \"Start Date\", \"End Date\"])\n\n    #Change column types of ID columns\n    df_usage = df_usage.withColumn(\"EndStation ID\", df_usage[\"EndStation ID\"].cast(\"int\"))\n    df_usage = df_usage.withColumn(\"StartStation ID\", df_usage[\"StartStation ID\"].cast(\"int\"))\n\n    #Prevent StartStation ID = EndStation ID\n   # df_usage = df_usage[df_usage[\"StartStation Id\"] != df_usage[\"EndStation Id\"]]\n\n    #Keep only relevant columns\n    df_usage = df_usage[\"Start Date\", \"StartStation Id\", \"End Date\", \"EndStation Id\", \"Duration\"]\n\n\n    \n    #root |-- Start Date: string (nullable = true) |-- StartStation Id: integer (nullable = true) |-- End Date: string (nullable = true) |-- EndStation Id: integer (nullable = true) |-- Duration: string (nullable = true)\n    #Add hour column, rounded\n    df_usage = df_usage.alias(\"t\").select('t.*', unix_timestamp('Start Date', \"dd/MM/yyyy HH:mm\").cast(TimestampType()).alias(\"Timestamp\"))\n    df_usage = df_usage.withColumn(\"Rounded Hour\", hour((round(unix_timestamp(\"Timestamp\")/3600)*3600).cast(\"Timestamp\")))\n    \n    #Preparing for join\n    split_col = split(df_usage['Start Date'], ' ')\n    df_usage = df_usage.withColumn('Datum', split_col.getItem(0)).withColumn('Hour', split_col.getItem(1))\n    \n    #Drop unuseful columns\n    columns_to_drop = [\"End Date\",\n                       \"EndStation Id\",\n                       \"Duration\"\n                      # \"Start Date\",\n                      # \"StartStation Id\"\n                       ]\n    df_usage = df_usage.drop(*columns_to_drop)\n\n\n    return df_usage\n    \n\ndef generatePast(x, df_usage):\n    \"\"\"Generate past hourly data\"\"\"\n    rm_cols = []\n    b = False\n    df_usage = df_usage.groupby(df_usage.Datum).pivot(\"Rounded Hour\").count().orderBy(\"Datum\")\n    df_usage = df_usage.na.fill(0)\n    df_usage = df_usage.alias(\"t\").select('t.*', unix_timestamp('Datum', \"dd/MM/yyyy\").cast(TimestampType()).alias(\"Timestamp\")).orderBy(\"Timestamp\")\n    df_usage = df_usage.drop(\"Timestamp\", \"null\")\n    df_usage = df_usage.withColumn('Datum',F.to_date(\"Datum\", 'dd/MM/yyyy'))\n    df_usage = rename_cols(df_usage, \"usage_\")\n    liste = []\n    \n    ##Past \n    if x > 24:\n        #time delta\n        y = 0\n        #column counter\n        i = 24\n        for j in range(x):\n            liste = []\n            print(\"Past \", j)\n            df_usage = df_usage.withColumn('Yesterday'+`j`, F.date_add(df_usage['Datum'], y))\n            if (x > 24):\n                x -= 24\n                if (b == True):\n                    for k in range(24):\n                        liste.append(\"usage_\"+`i`)\n                        i += 1\n                    df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Yesterday'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n                b = True\n            else:\n                for k in range(x):\n                    liste.append(\"usage_\"+`i`)\n                    i += 1\n                df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Yesterday'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n                x = 0\n                break\n            y -= 1\n    \n    else:\n        for i in range(24, x, -1):\n            rm_cols.append(\"usage_\"+`i`)\n            rm_cols.append(\"Yesterday\"+`i`) #disable this for previous date\n        df_usage = df_usage.drop(*rm_cols)\n    \n    \n    df_usage = df_usage.orderBy(\"Datum\")\n    return df_usage\n\ndef generateFuture(f, df_usage):\n    \"\"\"Generate future hourly data\"\"\"\n    ##Future\n    y = 0\n    i = 0\n    rm_cols = []\n    for j in range(f):\n        print(\"Future \", j)\n        liste = []\n        y += 1\n        df_usage = df_usage.withColumn('Tomorrow'+`j`, F.date_add(df_usage['Datum'], y))\n        if f > 24:\n            f -= 24\n            for k in range(24):\n                liste.append(\"future_\"+`i`)\n                i += 1\n            df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Tomorrow'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+ \"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n        else:\n            for k in range(f):\n                liste.append(\"future_\"+`i`)\n                i += 1\n            df_usage = df_usage.alias('l').join(df_usage.alias('r'), col('l.Tomorrow'+`j`) == col('r.Datum'), how='left').select([col('l.*')] + [col('r.'+\"usage_\"+`a`).alias(`b`) for a, b in enumerate(liste)])\n            break\n        \n    df_usage = df_usage.orderBy(\"Datum\")\n    return df_usage\n    \ndef rename_cols(rename_df, z):\n    \"\"\"Rename all frequency columns\"\"\"\n    for column in rename_df.columns[1:]:\n        new_column = z+column \n        rename_df = rename_df.withColumnRenamed(column, new_column)\n    return rename_df\n    \ndef cleansingDF(df_rental, df_weather):\n    \"\"\"Cleaning null values, merge weather and usage\"\"\"\n    df_final = df_rental.alias('l').join(df_weather.alias('r'), col('l.Datum') == col('r.Start Date'), how='left').select(col('l.*'), col('r.*'))\n    df_final = df_final.orderBy(\"Datum\")\n    return df_final\n\n\n\n######Jumping Window Solution######\n######For using, just comment out######\n######Note: some functions of this script is used for sliding window approach below#######\n\n####Init\n#Set value x = hours to the past, station = station number, all_f = true all stations, f = zielgröße (future)\nx = 24\nstation = 14 #Kings cross\n#all_f = True\nf = 24\n\n####Generate Weather DF \n#df_weather = generateWeatherDF(x)\n#df_weather.show()\n\n####Generate Rental DF\n#df_rental = generateRentalDF(station)\n#df_rental.show()\n\n####Add Past hours\n#df_rental = generatePast(x, df_rental)\n#df_rental.show()\n\n####Add Future hours\n#df_rental = generateFuture(f, df_rental)\n#df_rental.show()\n\n####Merge both\n#df_final = cleansingDF(df_rental, df_weather)\n#df_final.printSchema()\n#df_final.show()\n\n####Save back as csv\n\n#df_final.coalesce(1).write.csv(\"/user/hadoop/preparedDataframe09\", header=True)\n\n#df_weather.coalesce(1).write.csv(\"/user/hadoop/WeatherData2\", header=True)\n\n","user":"admin","dateUpdated":"2019-05-30T23:12:56+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558311757445_353444766","id":"20190520-022237_1689603982","dateCreated":"2019-05-20T02:22:37+0200","dateStarted":"2019-05-30T03:57:36+0200","dateFinished":"2019-05-30T03:57:36+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7991"},{"text":"%md\n#### Testing area ####\n<sup>Just for some tests...</sup>","user":"admin","dateUpdated":"2019-05-30T19:53:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Testing area</h4>\n<p><sup>Just for some tests&hellip;</sup></p>\n"}]},"apps":[],"jobName":"paragraph_1559181026812_-531700604","id":"20190530-035026_630194382","dateCreated":"2019-05-30T03:50:26+0200","dateStarted":"2019-05-30T19:53:21+0200","dateFinished":"2019-05-30T19:53:21+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7992"},{"text":"%spark2.pyspark\n#This code is to compute a moving/rolling average over a DataFrame using Spark.\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as func\n\n#function to calculate number of seconds from number of days: thanks Bob Swain\ndays = lambda i: i * 86400\n\n\ndf = spark.createDataFrame([(17.00, \"2018-03-10T15:27:18+00:00\"), # The first six days are sequential\n  (13.00, \"2018-03-11T12:27:18+00:00\"),  # included ...  \n  (25.00, \"2018-03-12T11:27:18+00:00\"),  # included ...\n  (20.00, \"2018-03-13T15:27:18+00:00\"),  # included ...\n  (56.00, \"2018-03-14T12:27:18+00:00\"),  # included...\n  (99.00, \"2018-03-15T11:27:18+00:00\"),  # This one will be included with the next window\n  (156.00, \"2018-03-22T11:27:18+00:00\"), # This one is inside the 7 day window of the previous\n  (122.00, \"2018-03-31T11:27:18+00:00\"), # This one is a new window, outside the 7 day window of any previous...\n  (7000.00, \"2018-04-15T11:27:18+00:00\"),# This starts a * brand new window * with the next entry included next\n  (9999.00, \"2018-04-16T11:27:18+00:00\") # This should be part of the previous entry\n  ],\n  [\"dollars\", \"timestampGMT\"])\n\n# we need this timestampGMT as seconds for our Window time frame\n\ndf = df.withColumn('timestampGMT', df.timestampGMT.cast('timestamp'))\ndf.show()","user":"admin","dateUpdated":"2019-05-30T20:39:14+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-------------------+\n|dollars|       timestampGMT|\n+-------+-------------------+\n|   17.0|2018-03-10 16:27:18|\n|   13.0|2018-03-11 13:27:18|\n|   25.0|2018-03-12 12:27:18|\n|   20.0|2018-03-13 16:27:18|\n|   56.0|2018-03-14 13:27:18|\n|   99.0|2018-03-15 12:27:18|\n|  156.0|2018-03-22 12:27:18|\n|  122.0|2018-03-31 13:27:18|\n| 7000.0|2018-04-15 13:27:18|\n| 9999.0|2018-04-16 13:27:18|\n+-------+-------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=366","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=367"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558479051149_-1336600193","id":"20190522-005051_1167726115","dateCreated":"2019-05-22T00:50:51+0200","dateStarted":"2019-05-30T20:39:15+0200","dateFinished":"2019-05-30T20:39:15+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7993"},{"text":"%spark2.pyspark\n# Create a \"seven day\" Window from seven days previous to the current day (row), using previous casting of timestamp to long (number of seconds).\n# remember a start value of \"-1\" means one off before the current row, and we are taking the timestamp as a long and comparing it to the rangeBetween amount of time.\n# remember an end value of 0 is the current row.\n\nwindowSpec = Window.orderBy(func.col(\"timestampGMT\").cast('long')).rangeBetween(-days(7), 0)","user":"admin","dateUpdated":"2019-05-30T20:39:17+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558633295545_-1981875225","id":"20190523-194135_37507681","dateCreated":"2019-05-23T19:41:35+0200","dateStarted":"2019-05-30T20:39:18+0200","dateFinished":"2019-05-30T20:39:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7994"},{"text":"%spark2.pyspark\n# Note the OVER clause added to AVG(), to define a windowing column.\ndf2 = df.withColumn('rolling_seven_day_average', func.avg(\"dollars\").over(windowSpec)) ","user":"admin","dateUpdated":"2019-05-30T20:39:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1558633331034_1684729744","id":"20190523-194211_549915740","dateCreated":"2019-05-23T19:42:11+0200","dateStarted":"2019-05-30T20:39:21+0200","dateFinished":"2019-05-30T20:39:21+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7995"},{"text":"%spark2.pyspark\ndf2.show()","user":"admin","dateUpdated":"2019-05-30T20:39:24+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+-------------------+-------------------------+\n|dollars|       timestampGMT|rolling_seven_day_average|\n+-------+-------------------+-------------------------+\n|   17.0|2018-03-10 16:27:18|                     17.0|\n|   13.0|2018-03-11 13:27:18|                     15.0|\n|   25.0|2018-03-12 12:27:18|       18.333333333333332|\n|   20.0|2018-03-13 16:27:18|                    18.75|\n|   56.0|2018-03-14 13:27:18|                     26.2|\n|   99.0|2018-03-15 12:27:18|       38.333333333333336|\n|  156.0|2018-03-22 12:27:18|                    127.5|\n|  122.0|2018-03-31 13:27:18|                    122.0|\n| 7000.0|2018-04-15 13:27:18|                   7000.0|\n| 9999.0|2018-04-16 13:27:18|                   8499.5|\n+-------+-------------------+-------------------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=368"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558633345894_1546749903","id":"20190523-194225_705750735","dateCreated":"2019-05-23T19:42:25+0200","dateStarted":"2019-05-30T20:39:24+0200","dateFinished":"2019-05-30T20:39:24+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7996"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\n\ndf = sc.parallelize([(4, 9.0), (3, 7.0), (2, 3.0), (1, 5.0)]).toDF([\"id\", \"num\"]).orderBy(\"id\")\ndf.show()\nw = Window().partitionBy().orderBy(col(\"id\"))\n#Sliding into the past\ndf.select(\"*\", lag(\"num\", 1).over(w).alias(\"usage00\")).show()\n#Sliding into the future\ndf.select(\"*\", lead(\"num\", 1).over(w).alias(\"future00\")).show()","user":"admin","dateUpdated":"2019-06-01T00:56:00+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+---+\n| id|num|\n+---+---+\n|  1|5.0|\n|  2|3.0|\n|  3|7.0|\n|  4|9.0|\n+---+---+\n\n+---+---+-------+\n| id|num|usage00|\n+---+---+-------+\n|  1|5.0|   null|\n|  2|3.0|    5.0|\n|  3|7.0|    3.0|\n|  4|9.0|    7.0|\n+---+---+-------+\n\n+---+---+--------+\n| id|num|future00|\n+---+---+--------+\n|  1|5.0|     3.0|\n|  2|3.0|     7.0|\n|  3|7.0|     9.0|\n|  4|9.0|    null|\n+---+---+--------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=577","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=578","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=579","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=580","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=581","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=582"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559249585195_1713955707","id":"20190530-225305_1762597926","dateCreated":"2019-05-30T22:53:05+0200","dateStarted":"2019-06-01T00:56:00+0200","dateFinished":"2019-06-01T00:56:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7997"},{"text":"%md\n#### Interpolation ####\n> <sup>When a row has less than 4 empty values than use linear interpolation otherwise use last valid value.</sup>\n","user":"admin","dateUpdated":"2019-05-30T03:51:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Interpolation</h4>\n<blockquote><p><sup>When a row has less than 4 empty values than use linear interpolation otherwise use last valid value.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559180917959_-573575428","id":"20190530-034837_360110099","dateCreated":"2019-05-30T03:48:37+0200","dateStarted":"2019-05-30T03:51:59+0200","dateFinished":"2019-05-30T03:51:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7998"},{"text":"%spark2.pyspark\nimport pandas as pd \npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\n\n#Set x for past\nx = 24\n\n#generate weather data frame\ndf_weather = generateWeatherDF(x)\ndf_weather = df_weather.drop(\"Hourly Weather\")\n# Since only 1562 rows, we can convert into a pandas df\ndf = df_weather.toPandas()  \n\ndef replacer(p):\n    \"\"\"Replacing NaN\"\"\"\n    # interpolation does not work good for larger gaps\n    if (p.isnull().sum(axis=0) >= 4):\n        p.fillna(method=\"ffill\", inplace=True) #propagate last valid observation forward to next valid \n    return p\n    \ndf = df.apply(lambda x: replacer(x), axis=1)\n\n#linear interpolation of remaining NaNs and round up\ndf = df.round(2)\ndf.interpolate(inplace=True)\ndf.iloc[1535]\n#df[df.isnull().any(axis=1)]\n\n#convert back to pyspark df \ndf_weather = spark.createDataFrame(df)\n\ndf_weather.count()","user":"admin","dateUpdated":"2019-06-01T16:37:15+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1562\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=689","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=690","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=691"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1558633351709_758859663","id":"20190523-194231_491425071","dateCreated":"2019-05-23T19:42:31+0200","dateStarted":"2019-06-01T16:37:16+0200","dateFinished":"2019-06-01T16:37:21+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7999"},{"text":"%md\n#### Each hour one line ####\n> <sup>Transform the `df_usage` dataframe so that each row represents one rounded hour.</sup>","user":"admin","dateUpdated":"2019-05-30T19:51:32+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Each hour one line</h4>\n<blockquote><p><sup>Transform the <code>df_usage</code> dataframe so that each row represents one rounded hour.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559181048483_-79285113","id":"20190530-035048_1967219093","dateCreated":"2019-05-30T03:50:48+0200","dateStarted":"2019-05-30T19:51:32+0200","dateFinished":"2019-05-30T19:51:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8000"},{"text":"%spark2.pyspark\n#Previous solution with pivot tables\ntest = generateRentalDF(14)\ntest = test.groupby(test.Datum).pivot(\"Rounded Hour\").count().orderBy(\"Datum\")\ntest = test.na.fill(0)\ntest = test.alias(\"t\").select('t.*', unix_timestamp('Datum', \"dd/MM/yyyy\").cast(TimestampType()).alias(\"Timestamp\")).orderBy(\"Timestamp\")\ntest.show()","user":"admin","dateUpdated":"2019-06-01T02:01:21+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\n|     Datum|null|  0|  1|  2|  3|  4|  5|  6|  7|  8|  9| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23|          Timestamp|\n+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\n|04/01/2015|   0|  3|  0|  0|  0|  0|  0|  0|  0|  1|  0|  3|  0|  0|  2|  4|  2|  4|  2|  2|  3|  4|  1|  1|  2|2015-01-04 00:00:00|\n|05/01/2015|   0|  0|  0|  0|  0|  0|  1|  1| 36|137| 65|  1|  8|  2|  4|  0|  3|  2| 11|  2|  6|  5|  2|  0|  0|2015-01-05 00:00:00|\n|06/01/2015|   0|  0|  1|  0|  0|  0|  1|  6| 32|118| 88|  6|  1|  2|  2|  2|  6|  4|  3|  5|  4|  1|  3|  1|  0|2015-01-06 00:00:00|\n|07/01/2015|   0|  1|  0|  0|  0|  0|  0|  5| 39|128| 65|  3|  3|  7|  1|  4|  1|  4|  3|  4|  6|  1|  3|  1|  2|2015-01-07 00:00:00|\n|08/01/2015|   0|  1|  0|  0|  0|  0|  0|  4| 17| 57| 36|  5|  3|  6|  3|  5|  3|  3|  3|  3|  3|  3|  3|  2|  4|2015-01-08 00:00:00|\n|09/01/2015|   0|  1|  1|  0|  0|  0|  0|  4| 22|111| 87| 14|  2|  3|  3|  4|  3|  3|  3|  6|  4|  3|  6|  0|  3|2015-01-09 00:00:00|\n|10/01/2015|   0|  1|  2|  0|  2|  0|  0|  1|  1|  1|  3|  4|  1|  8|  8|  5|  4|  2| 10|  4|  3|  6|  3|  0|  0|2015-01-10 00:00:00|\n|11/01/2015|   0|  3|  1|  0|  3|  0|  0|  0|  0|  1|  3|  7|  0|  4|  4|  2|  6|  3|  6|  0|  3|  2|  2|  2|  3|2015-01-11 00:00:00|\n|12/01/2015|   0|  1|  0|  0|  0|  0|  1|  4| 20|138| 60|  6|  7|  3|  5|  0|  3|  2|  0|  1|  1|  4|  1|  1|  0|2015-01-12 00:00:00|\n|13/01/2015|   0|  0|  0|  0|  0|  0|  0|  4| 32|131| 77| 20|  5|  7|  8|  4|  3|  2|  2| 10|  2|  3|  1|  6|  2|2015-01-13 00:00:00|\n|14/01/2015|   0|  1|  0|  0|  0|  0|  0|  4| 31|128| 85| 17|  6|  2|  8|  4|  6|  4|  2|  5|  5|  5|  2|  0|  1|2015-01-14 00:00:00|\n|15/01/2015|   0|  0|  0|  0|  0|  0|  0|  4| 32|115| 71|  2|  2|  3|  4|  7|  3|  2|  4|  5| 10|  1|  2|  2|  0|2015-01-15 00:00:00|\n|16/01/2015|   0|  0|  1|  0|  0|  1|  0|  5| 30|137| 77|  5| 12|  4|  7|  7|  4|  2|  3|  5|  3|  5|  1|  3|  1|2015-01-16 00:00:00|\n|17/01/2015|   0|  4|  3|  0|  4|  0|  1|  1|  0|  3|  1|  4|  6|  0|  4|  4|  1|  2|  2|  2|  7|  9|  0|  4|  0|2015-01-17 00:00:00|\n|18/01/2015|   0|  4|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  1|  1|  4|  4|  6|  3|  5|  3|  3|  0|  5|  1|  3|2015-01-18 00:00:00|\n|19/01/2015|   0|  1|  1|  1|  0|  0|  0|  5| 34|125| 70|  9|  8|  3|  4|  8|  5|  4|  3|  9|  5|  0|  3|  2|  2|2015-01-19 00:00:00|\n|20/01/2015|   0|  0|  0|  1|  0|  0|  0|  4| 28|122| 80| 13|  2|  2|  8|  5|  2| 10|  3|  5|  9|  3|  1|  2|  2|2015-01-20 00:00:00|\n|21/01/2015|   0|  1|  0|  0|  0|  4|  0|  5| 22|121| 79| 20|  0|  7|  1|  1|  7|  4|  8|  5|  3|  3|  3|  4|  0|2015-01-21 00:00:00|\n|22/01/2015|   0|  1|  2|  0|  0|  0|  0|  4| 31|134| 58|  9|  7|  0|  5|  3|  3|  2|  0|  6|  3|  3|  2|  3|  1|2015-01-22 00:00:00|\n|23/01/2015|   0|  2|  1|  0|  0|  0|  0|  3| 30|114| 65| 18| 13|  2|  1|  2|  2|  7|  5|  9|  2|  1|  0|  4|  1|2015-01-23 00:00:00|\n+----------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=604","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=605","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=606","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=607","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=608","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=609"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559181312974_-1224841101","id":"20190530-035512_256580383","dateCreated":"2019-05-30T03:55:12+0200","dateStarted":"2019-06-01T02:01:21+0200","dateFinished":"2019-06-01T02:04:37+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8001"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import concat, col, lit\n\n#Give me all station records from x\ntest2 = generateRentalDF(14)\ntest2 = test2.dropna()\n#Concatenate daily date column with rounded hour column\ntest2 = test2.withColumn(\"New Datum\", concat(col(\"Datum\"), lit(\" \"), col(\"Rounded Hour\")))\ntest2 = test2.alias(\"t\").select('t.*', unix_timestamp('New Datum', \"dd/MM/yyyy HH\").cast(TimestampType()).alias(\"New Date\")).orderBy(\"New Date\") #TODO:// zu viele order bys, später noch entfernen\ntest2.show()","user":"admin","dateUpdated":"2019-06-01T02:01:26+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\n|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|           New Date|\n+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\n|04/01/2015 23:50|             14|2015-01-04 23:50:00|           0|04/01/2015|23:50| 04/01/2015 0|2015-01-04 00:00:00|\n|04/01/2015 23:40|             14|2015-01-04 23:40:00|           0|04/01/2015|23:40| 04/01/2015 0|2015-01-04 00:00:00|\n|04/01/2015 23:40|             14|2015-01-04 23:40:00|           0|04/01/2015|23:40| 04/01/2015 0|2015-01-04 00:00:00|\n|04/01/2015 07:44|             14|2015-01-04 07:44:00|           8|04/01/2015|07:44| 04/01/2015 8|2015-01-04 08:00:00|\n|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 10:04|             14|2015-01-04 10:04:00|          10|04/01/2015|10:04|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|2015-01-04 10:00:00|\n|04/01/2015 13:09|             14|2015-01-04 13:09:00|          13|04/01/2015|13:09|04/01/2015 13|2015-01-04 13:00:00|\n|04/01/2015 12:50|             14|2015-01-04 12:50:00|          13|04/01/2015|12:50|04/01/2015 13|2015-01-04 13:00:00|\n|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 14:09|             14|2015-01-04 14:09:00|          14|04/01/2015|14:09|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 13:42|             14|2015-01-04 13:42:00|          14|04/01/2015|13:42|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 14:17|             14|2015-01-04 14:17:00|          14|04/01/2015|14:17|04/01/2015 14|2015-01-04 14:00:00|\n|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:06|             14|2015-01-04 15:06:00|          15|04/01/2015|15:06|04/01/2015 15|2015-01-04 15:00:00|\n|04/01/2015 15:41|             14|2015-01-04 15:41:00|          16|04/01/2015|15:41|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 15:37|             14|2015-01-04 15:37:00|          16|04/01/2015|15:37|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 15:48|             14|2015-01-04 15:48:00|          16|04/01/2015|15:48|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 15:45|             14|2015-01-04 15:45:00|          16|04/01/2015|15:45|04/01/2015 16|2015-01-04 16:00:00|\n|04/01/2015 16:32|             14|2015-01-04 16:32:00|          17|04/01/2015|16:32|04/01/2015 17|2015-01-04 17:00:00|\n+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=610","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=611","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=612","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=613"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229521905_1962831237","id":"20190530-171841_2118270594","dateCreated":"2019-05-30T17:18:41+0200","dateStarted":"2019-06-01T02:01:26+0200","dateFinished":"2019-06-01T02:06:09+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8002"},{"text":"%spark2.pyspark\n#count usage per hour and join back\ntest3 = test2.groupBy(\"New Date\").count().withColumnRenamed('count', 'Current Usage')\ntest2 = test2.join(test3, [\"New Date\"], \"leftouter\")\ntest2 = test2.drop_duplicates([\"New Date\"]).orderBy(\"New Date\")\ntest2.printSchema()\ntest2.show()\n","user":"admin","dateUpdated":"2019-06-01T02:02:01+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- New Date: timestamp (nullable = true)\n |-- Start Date: string (nullable = true)\n |-- StartStation Id: integer (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Rounded Hour: integer (nullable = true)\n |-- Datum: string (nullable = true)\n |-- Hour: string (nullable = true)\n |-- New Datum: string (nullable = true)\n |-- Current Usage: long (nullable = true)\n\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|           New Date|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|Current Usage|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|2015-01-04 00:00:00|04/01/2015 23:40|             14|2015-01-04 23:40:00|           0|04/01/2015|23:40| 04/01/2015 0|            3|\n|2015-01-04 08:00:00|04/01/2015 07:44|             14|2015-01-04 07:44:00|           8|04/01/2015|07:44| 04/01/2015 8|            1|\n|2015-01-04 10:00:00|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|            3|\n|2015-01-04 13:00:00|04/01/2015 12:50|             14|2015-01-04 12:50:00|          13|04/01/2015|12:50|04/01/2015 13|            2|\n|2015-01-04 14:00:00|04/01/2015 13:42|             14|2015-01-04 13:42:00|          14|04/01/2015|13:42|04/01/2015 14|            4|\n|2015-01-04 15:00:00|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|            2|\n|2015-01-04 16:00:00|04/01/2015 15:45|             14|2015-01-04 15:45:00|          16|04/01/2015|15:45|04/01/2015 16|            4|\n|2015-01-04 17:00:00|04/01/2015 16:32|             14|2015-01-04 16:32:00|          17|04/01/2015|16:32|04/01/2015 17|            2|\n|2015-01-04 18:00:00|04/01/2015 17:59|             14|2015-01-04 17:59:00|          18|04/01/2015|17:59|04/01/2015 18|            2|\n|2015-01-04 19:00:00|04/01/2015 19:22|             14|2015-01-04 19:22:00|          19|04/01/2015|19:22|04/01/2015 19|            3|\n|2015-01-04 20:00:00|04/01/2015 19:45|             14|2015-01-04 19:45:00|          20|04/01/2015|19:45|04/01/2015 20|            4|\n|2015-01-04 21:00:00|04/01/2015 20:31|             14|2015-01-04 20:31:00|          21|04/01/2015|20:31|04/01/2015 21|            1|\n|2015-01-04 22:00:00|04/01/2015 21:52|             14|2015-01-04 21:52:00|          22|04/01/2015|21:52|04/01/2015 22|            1|\n|2015-01-04 23:00:00|04/01/2015 22:49|             14|2015-01-04 22:49:00|          23|04/01/2015|22:49|04/01/2015 23|            2|\n|2015-01-05 05:00:00|05/01/2015 05:06|             14|2015-01-05 05:06:00|           5|05/01/2015|05:06| 05/01/2015 5|            1|\n|2015-01-05 06:00:00|05/01/2015 06:18|             14|2015-01-05 06:18:00|           6|05/01/2015|06:18| 05/01/2015 6|            1|\n|2015-01-05 07:00:00|05/01/2015 06:37|             14|2015-01-05 06:37:00|           7|05/01/2015|06:37| 05/01/2015 7|           36|\n|2015-01-05 08:00:00|05/01/2015 07:31|             14|2015-01-05 07:31:00|           8|05/01/2015|07:31| 05/01/2015 8|          137|\n|2015-01-05 09:00:00|05/01/2015 08:31|             14|2015-01-05 08:31:00|           9|05/01/2015|08:31| 05/01/2015 9|           65|\n|2015-01-05 10:00:00|05/01/2015 09:58|             14|2015-01-05 09:58:00|          10|05/01/2015|09:58|05/01/2015 10|            1|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=614","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=615","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=616"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229458509_228380051","id":"20190530-171738_373187699","dateCreated":"2019-05-30T17:17:38+0200","dateStarted":"2019-06-01T02:04:37+0200","dateFinished":"2019-06-01T02:12:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8003"},{"text":"%spark2.pyspark\n#fill empty or missing hours with 0, supplement missing hours (e.g. 1, 8 -> supplement 2,3,4...)\nfrom pyspark.sql.functions import col, min as min_, max as max_\n\nstep = 60 * 60\n\nminp, maxp = test2.select(\n    min_(\"New Date\").cast(\"long\"), max_(\"New Date\").cast(\"long\")\n).first()\n\nreference = spark.range(\n    (minp / step) * step, ((maxp / step) + 1) * step, step\n).select(col(\"id\").cast(\"timestamp\").alias(\"New Date\"))\n\nreference = reference.join(test2, [\"New Date\"], \"leftouter\").orderBy(\"New Date\")\nreference.show()","user":"admin","dateUpdated":"2019-06-01T02:02:27+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|           New Date|      Start Date|StartStation Id|          Timestamp|Rounded Hour|     Datum| Hour|    New Datum|Current Usage|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\n|2015-01-04 00:00:00|04/01/2015 23:40|             14|2015-01-04 23:40:00|           0|04/01/2015|23:40| 04/01/2015 0|            3|\n|2015-01-04 01:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 02:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 03:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 04:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 05:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 06:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 07:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 08:00:00|04/01/2015 07:44|             14|2015-01-04 07:44:00|           8|04/01/2015|07:44| 04/01/2015 8|            1|\n|2015-01-04 09:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 10:00:00|04/01/2015 10:01|             14|2015-01-04 10:01:00|          10|04/01/2015|10:01|04/01/2015 10|            3|\n|2015-01-04 11:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 12:00:00|            null|           null|               null|        null|      null| null|         null|         null|\n|2015-01-04 13:00:00|04/01/2015 12:50|             14|2015-01-04 12:50:00|          13|04/01/2015|12:50|04/01/2015 13|            2|\n|2015-01-04 14:00:00|04/01/2015 13:42|             14|2015-01-04 13:42:00|          14|04/01/2015|13:42|04/01/2015 14|            4|\n|2015-01-04 15:00:00|04/01/2015 15:17|             14|2015-01-04 15:17:00|          15|04/01/2015|15:17|04/01/2015 15|            2|\n|2015-01-04 16:00:00|04/01/2015 15:45|             14|2015-01-04 15:45:00|          16|04/01/2015|15:45|04/01/2015 16|            4|\n|2015-01-04 17:00:00|04/01/2015 16:32|             14|2015-01-04 16:32:00|          17|04/01/2015|16:32|04/01/2015 17|            2|\n|2015-01-04 18:00:00|04/01/2015 17:59|             14|2015-01-04 17:59:00|          18|04/01/2015|17:59|04/01/2015 18|            2|\n|2015-01-04 19:00:00|04/01/2015 19:22|             14|2015-01-04 19:22:00|          19|04/01/2015|19:22|04/01/2015 19|            3|\n+-------------------+----------------+---------------+-------------------+------------+----------+-----+-------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=617","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=618","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=619","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=620","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=621","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=622","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=623"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559229827411_1189361632","id":"20190530-172347_798841573","dateCreated":"2019-05-30T17:23:47+0200","dateStarted":"2019-06-01T02:06:09+0200","dateFinished":"2019-06-01T02:21:54+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8004"},{"text":"%spark2.pyspark\n# Remove unused columns\ncolumns_to_drop = [\"Start Date\", \n                   \"Timestamp\",\n                   \"Rounded Hour\",\n                   \"Datum\",\n                   \"Hour\",\n                   \"New Datum\"]\n                   \nreference = reference.drop(*columns_to_drop)\n# fill null values\nreference = reference.na.fill(14, subset=['StartStation Id'])\nreference = reference.na.fill(0, subset=['Current Usage'])\nreference.show()","user":"admin","dateUpdated":"2019-06-01T02:02:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+---------------+-------------+\n|           New Date|StartStation Id|Current Usage|\n+-------------------+---------------+-------------+\n|2015-01-04 00:00:00|             14|            3|\n|2015-01-04 01:00:00|             14|            0|\n|2015-01-04 02:00:00|             14|            0|\n|2015-01-04 03:00:00|             14|            0|\n|2015-01-04 04:00:00|             14|            0|\n|2015-01-04 05:00:00|             14|            0|\n|2015-01-04 06:00:00|             14|            0|\n|2015-01-04 07:00:00|             14|            0|\n|2015-01-04 08:00:00|             14|            1|\n|2015-01-04 09:00:00|             14|            0|\n|2015-01-04 10:00:00|             14|            3|\n|2015-01-04 11:00:00|             14|            0|\n|2015-01-04 12:00:00|             14|            0|\n|2015-01-04 13:00:00|             14|            2|\n|2015-01-04 14:00:00|             14|            4|\n|2015-01-04 15:00:00|             14|            2|\n|2015-01-04 16:00:00|             14|            4|\n|2015-01-04 17:00:00|             14|            2|\n|2015-01-04 18:00:00|             14|            2|\n|2015-01-04 19:00:00|             14|            3|\n+-------------------+---------------+-------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=624","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=625","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=626","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=627"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559237800485_1829515935","id":"20190530-193640_1766444645","dateCreated":"2019-05-30T19:36:40+0200","dateStarted":"2019-06-01T02:12:11+0200","dateFinished":"2019-06-01T02:28:09+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8005"},{"text":"%md\n#### Sliding Window ####\n> <sup>Use a sliding window on a hourly base with dynamic size to go from current hour to `past` and to `predict` the next 24 hours from the current hour.</sup>","user":"admin","dateUpdated":"2019-06-01T16:44:52+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Sliding Window</h4>\n<blockquote><p><sup>Use a sliding window on a hourly base with dynamic size to go from current hour to <code>past</code> and to <code>predict</code> the next 24 hours from the current hour.</sup></p>\n</blockquote>\n"}]},"apps":[],"jobName":"paragraph_1559236207640_-633690624","id":"20190530-191007_2070186282","dateCreated":"2019-05-30T19:10:07+0200","dateStarted":"2019-06-01T16:44:52+0200","dateFinished":"2019-06-01T16:44:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8006"},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\n\n#set past\nx = 24 \n#set future\nf = 24 \n\n# go x hours back\nfor i in range(x):\n    w = Window().partitionBy().orderBy(col(\"New Date\"))\n    reference = reference.select(\"*\", lag(\"Current Usage\", i+1).over(w).alias(\"Usage\"+`i`))\n\n# go f hours future\nfor i in range(f):\n    w = Window().partitionBy().orderBy(col(\"New Date\"))\n    reference = reference.select(\"*\", lead(\"Current Usage\", i+1).over(w).alias(\"Future\"+`i`))\n\nreference.show()","user":"admin","dateUpdated":"2019-06-01T02:03:05+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|           New Date|StartStation Id|Current Usage|Usage0|Usage1|Usage2|Usage3|Usage4|Usage5|Usage6|Usage7|Usage8|Usage9|Usage10|Usage11|Usage12|Usage13|Usage14|Usage15|Usage16|Usage17|Usage18|Usage19|Usage20|Usage21|Usage22|Usage23|Future0|Future1|Future2|Future3|Future4|Future5|Future6|Future7|Future8|Future9|Future10|Future11|Future12|Future13|Future14|Future15|Future16|Future17|Future18|Future19|Future20|Future21|Future22|Future23|\n+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n|2015-01-04 00:00:00|             14|            3|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      0|      0|      0|      0|      0|      1|      0|      3|       0|       0|       2|       4|       2|       4|       2|       2|       3|       4|       1|       1|       2|       0|\n|2015-01-04 01:00:00|             14|            0|     3|  null|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      0|      0|      0|      0|      1|      0|      3|      0|       0|       2|       4|       2|       4|       2|       2|       3|       4|       1|       1|       2|       0|       0|\n|2015-01-04 02:00:00|             14|            0|     0|     3|  null|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      0|      0|      0|      1|      0|      3|      0|      0|       2|       4|       2|       4|       2|       2|       3|       4|       1|       1|       2|       0|       0|       0|\n|2015-01-04 03:00:00|             14|            0|     0|     0|     3|  null|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      0|      0|      1|      0|      3|      0|      0|      2|       4|       2|       4|       2|       2|       3|       4|       1|       1|       2|       0|       0|       0|       0|\n|2015-01-04 04:00:00|             14|            0|     0|     0|     0|     3|  null|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      0|      1|      0|      3|      0|      0|      2|      4|       2|       4|       2|       2|       3|       4|       1|       1|       2|       0|       0|       0|       0|       0|\n|2015-01-04 05:00:00|             14|            0|     0|     0|     0|     0|     3|  null|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      1|      0|      3|      0|      0|      2|      4|      2|       4|       2|       2|       3|       4|       1|       1|       2|       0|       0|       0|       0|       0|       1|\n|2015-01-04 06:00:00|             14|            0|     0|     0|     0|     0|     0|     3|  null|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      1|      0|      3|      0|      0|      2|      4|      2|      4|       2|       2|       3|       4|       1|       1|       2|       0|       0|       0|       0|       0|       1|       1|\n|2015-01-04 07:00:00|             14|            0|     0|     0|     0|     0|     0|     0|     3|  null|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      1|      0|      3|      0|      0|      2|      4|      2|      4|      2|       2|       3|       4|       1|       1|       2|       0|       0|       0|       0|       0|       1|       1|      36|\n|2015-01-04 08:00:00|             14|            1|     0|     0|     0|     0|     0|     0|     0|     3|  null|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      3|      0|      0|      2|      4|      2|      4|      2|      2|       3|       4|       1|       1|       2|       0|       0|       0|       0|       0|       1|       1|      36|     137|\n|2015-01-04 09:00:00|             14|            0|     1|     0|     0|     0|     0|     0|     0|     0|     3|  null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      3|      0|      0|      2|      4|      2|      4|      2|      2|      3|       4|       1|       1|       2|       0|       0|       0|       0|       0|       1|       1|      36|     137|      65|\n|2015-01-04 10:00:00|             14|            3|     0|     1|     0|     0|     0|     0|     0|     0|     0|     3|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      0|      2|      4|      2|      4|      2|      2|      3|      4|       1|       1|       2|       0|       0|       0|       0|       0|       1|       1|      36|     137|      65|       1|\n|2015-01-04 11:00:00|             14|            0|     3|     0|     1|     0|     0|     0|     0|     0|     0|     0|      3|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      0|      2|      4|      2|      4|      2|      2|      3|      4|      1|       1|       2|       0|       0|       0|       0|       0|       1|       1|      36|     137|      65|       1|       8|\n|2015-01-04 12:00:00|             14|            0|     0|     3|     0|     1|     0|     0|     0|     0|     0|     0|      0|      3|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      2|      4|      2|      4|      2|      2|      3|      4|      1|      1|       2|       0|       0|       0|       0|       0|       1|       1|      36|     137|      65|       1|       8|       2|\n|2015-01-04 13:00:00|             14|            2|     0|     0|     3|     0|     1|     0|     0|     0|     0|     0|      0|      0|      3|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      4|      2|      4|      2|      2|      3|      4|      1|      1|      2|       0|       0|       0|       0|       0|       1|       1|      36|     137|      65|       1|       8|       2|       4|\n|2015-01-04 14:00:00|             14|            4|     2|     0|     0|     3|     0|     1|     0|     0|     0|     0|      0|      0|      0|      3|   null|   null|   null|   null|   null|   null|   null|   null|   null|   null|      2|      4|      2|      2|      3|      4|      1|      1|      2|      0|       0|       0|       0|       0|       1|       1|      36|     137|      65|       1|       8|       2|       4|       0|\n|2015-01-04 15:00:00|             14|            2|     4|     2|     0|     0|     3|     0|     1|     0|     0|     0|      0|      0|      0|      0|      3|   null|   null|   null|   null|   null|   null|   null|   null|   null|      4|      2|      2|      3|      4|      1|      1|      2|      0|      0|       0|       0|       0|       1|       1|      36|     137|      65|       1|       8|       2|       4|       0|       3|\n|2015-01-04 16:00:00|             14|            4|     2|     4|     2|     0|     0|     3|     0|     1|     0|     0|      0|      0|      0|      0|      0|      3|   null|   null|   null|   null|   null|   null|   null|   null|      2|      2|      3|      4|      1|      1|      2|      0|      0|      0|       0|       0|       1|       1|      36|     137|      65|       1|       8|       2|       4|       0|       3|       2|\n|2015-01-04 17:00:00|             14|            2|     4|     2|     4|     2|     0|     0|     3|     0|     1|     0|      0|      0|      0|      0|      0|      0|      3|   null|   null|   null|   null|   null|   null|   null|      2|      3|      4|      1|      1|      2|      0|      0|      0|      0|       0|       1|       1|      36|     137|      65|       1|       8|       2|       4|       0|       3|       2|      11|\n|2015-01-04 18:00:00|             14|            2|     2|     4|     2|     4|     2|     0|     0|     3|     0|     1|      0|      0|      0|      0|      0|      0|      0|      3|   null|   null|   null|   null|   null|   null|      3|      4|      1|      1|      2|      0|      0|      0|      0|      0|       1|       1|      36|     137|      65|       1|       8|       2|       4|       0|       3|       2|      11|       2|\n|2015-01-04 19:00:00|             14|            3|     2|     2|     4|     2|     4|     2|     0|     0|     3|     0|      1|      0|      0|      0|      0|      0|      0|      0|      3|   null|   null|   null|   null|   null|      4|      1|      1|      2|      0|      0|      0|      0|      0|      1|       1|      36|     137|      65|       1|       8|       2|       4|       0|       3|       2|      11|       2|       6|\n+-------------------+---------------+-------------+------+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=628","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=629","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=630","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=631","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=632"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559245155625_-582948985","id":"20190530-213915_977149296","dateCreated":"2019-05-30T21:39:15+0200","dateStarted":"2019-06-01T02:21:54+0200","dateFinished":"2019-06-01T02:34:36+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8007"},{"text":"%md\nProblem: Weather dataframe has totally different structure.\nIdea: Use the power of `explode` function in PySpark.","user":"admin","dateUpdated":"2019-06-01T16:42:51+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559400109899_704791655","id":"20190601-164149_110811586","dateCreated":"2019-06-01T16:41:49+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11103","dateFinished":"2019-06-01T16:42:51+0200","dateStarted":"2019-06-01T16:42:51+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Problem: Weather dataframe has totally different structure.\n<br  />Idea: Use the power of <code>explode</code> function in PySpark.</p>\n"}]}},{"text":"%spark2.pyspark\nweather = generateWeatherDF(2)\nweather = weather.drop(*[\"Apparent Temperature (Avg)\", \"Daily Weather\", \"Hourly Weather\", \"Humidity\", \"Windspeed\"])\nweather.show()","user":"admin","dateUpdated":"2019-06-01T16:14:59+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+-----+-----+-------------------+-------------------+------+------+------+------+\n|Start Date|hum00|hum01|              sum00|              sum01|temp00|temp01|wind00|wind01|\n+----------+-----+-----+-------------------+-------------------+------+------+------+------+\n|2015-01-04| 0.93| 0.94|partly-cloudy-night|partly-cloudy-night| 37.24| 36.35|  0.94|  0.56|\n|2015-01-05| 0.94| 0.95|partly-cloudy-night|             cloudy| 37.62| 37.97|  0.37|  0.18|\n|2015-01-06| 0.82| 0.83|             cloudy|             cloudy| 45.99| 45.87|  1.44|  1.58|\n|2015-01-07|  0.9| 0.91|        clear-night|        clear-night| 35.64| 35.42|  0.64|  0.64|\n|2015-01-08|  0.9|  0.9|partly-cloudy-night|partly-cloudy-night| 47.21|  50.5|  6.27|  5.51|\n|2015-01-09| 0.81| 0.79|partly-cloudy-night|partly-cloudy-night| 44.75| 46.14|  6.13|  5.73|\n|2015-01-10| 0.82| 0.85|partly-cloudy-night|             cloudy| 56.73| 55.95|  7.88|  8.55|\n|2015-01-11| 0.76| 0.79|        clear-night|        clear-night| 35.06| 35.14|  4.56|  4.27|\n|2015-01-12| 0.78| 0.74|partly-cloudy-night|partly-cloudy-night| 43.09| 43.76|  6.61|  6.71|\n|2015-01-13| 0.92| 0.92|               rain|               rain| 52.35| 52.47|  5.45|  6.51|\n|2015-01-14| 0.79| 0.74|partly-cloudy-night|partly-cloudy-night| 35.57| 35.51|  4.32|  6.12|\n|2015-01-15| 0.89| 0.88|partly-cloudy-night|partly-cloudy-night| 44.16| 45.37|  9.22|  9.56|\n|2015-01-16| 0.67| 0.65|        clear-night|        clear-night| 38.98| 39.24|  5.17|  5.45|\n|2015-01-17| 0.89| 0.89|        clear-night|        clear-night| 31.72| 31.44|  0.63|  1.15|\n|2015-01-18| 0.86| 0.86|        clear-night|partly-cloudy-night| 38.46| 38.16|  0.35|  0.32|\n|2015-01-19| 0.93| 0.93|partly-cloudy-night|partly-cloudy-night| 30.99| 31.57|  0.59|  0.66|\n|2015-01-20|  0.9|  0.9|        clear-night|        clear-night| 29.39| 28.92|  0.03|  0.03|\n|2015-01-21|  0.8| 0.83|partly-cloudy-night|partly-cloudy-night| 33.45| 32.72|  5.25|  5.17|\n|2015-01-22| 0.89|  0.9|partly-cloudy-night|partly-cloudy-night| 34.62| 34.05|  0.23|  0.34|\n|2015-01-23| 0.89| 0.89|        clear-night|        clear-night| 27.81| 27.44|  0.04|  0.05|\n+----------+-----+-----+-------------------+-------------------+------+------+------+------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=652","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=653"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559340647166_-2019231479","id":"20190601-001047_2064169923","dateCreated":"2019-06-01T00:10:47+0200","dateStarted":"2019-06-01T16:14:59+0200","dateFinished":"2019-06-01T16:14:59+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8008"},{"text":"%spark2.pyspark\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import lag, col, lead, first\nfrom pyspark.sql.window import Window\n\ndef _combine(x,y):\n    \"\"\"creates hourly interval for weather df\"\"\"\n    print(y)\n    d = str(x) + ' {}:00:00'.format(y)\n    return d\n\ndef transformWeatherDF(df, past):\n    \"\"\"add hours on key column and transform columoriented hours to roworiented hours\"\"\"\n    \n    #Remove daily columns\n    df = df.drop(*[\"Apparent Temperature (Avg)\", \"Daily Weather\", \"Hourly Weather\", \"Humidity\", \"Windspeed\"])\n    \n    #use udf to generate hours on the dates\n    combine = F.udf(lambda x,y: _combine(x,y))\n\n    #fetch relevant columns and keep column datatype\n    cols_temp, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"temp\")))\n    cols_hum, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"hum\")))\n    cols_sum, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"sum\")))\n    cols_wind, dtypes2 = zip(*((c, t) for (c, t) in df.dtypes if c not in ['Start Date'] and c.startswith(\"wind\")))\n    \n    ##temperature\n    #explode function for changing structure of dataframe (temperature)\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_temp],\n          )).alias(\"kvs\")\n\n    df_temp = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Temperature')])\\\n       .withColumn('key', F.regexp_replace('key', 'temp', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n\n    # go x hours back (temperature)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df_temp = df_temp.select(\"*\", lag(\"Current Temperature\", i+1).over(w).alias(\"temp\"+`i`))\n       \n    ##humidity\n    #explode humidity columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_hum],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Humidity')])\\\n       .withColumn('key', F.regexp_replace('key', 'hum', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (humidity)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Humidity\", i+1).over(w).alias(\"hum\"+`i`))\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n    \n    ##summary\n    #explode summary columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_sum],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Summary')])\\\n       .withColumn('key', F.regexp_replace('key', 'sum', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (summary)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Summary\", i+1).over(w).alias(\"sum\"+`i`))\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n    \n    ##summary\n    #explode windspeed columns\n    kvs = F.explode(F.array([\n          F.struct(F.lit(c).alias(\"key\"), F.col(c).alias(\"val\")) for c in cols_wind],\n          )).alias(\"kvs\")\n      \n    df2 = df.select(['Start Date'] + [kvs])\\\n       .select(['Start Date'] + [\"kvs.key\", F.col(\"kvs.val\").alias('Current Windspeed')])\\\n       .withColumn('key', F.regexp_replace('key', 'wind', ''))\\\n       .withColumn('Start Date', combine('Start Date','key').cast('timestamp'))\\\n       .drop('key')\n       \n    # go x hours back (windspeed)\n    for i in range(past):\n        w = Window().partitionBy().orderBy(col(\"Start Date\"))\n        df2 = df2.select(\"*\", lag(\"Current Windspeed\", i+1).over(w).alias(\"wind\"+`i`))\n    \n    df_temp = df_temp.join(df2, \"Start Date\", \"leftouter\").orderBy(\"Start Date\")\n\n    return df_temp\n\ntransformWeatherDF(df_weather, 3).show()\ndf_weather = transformWeatherDF(df_weather, 24)","user":"admin","dateUpdated":"2019-06-01T16:47:49+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+-------------------+-------------------+-------------------+-------------------+-----------------+-----+-----+-----+\n|         Start Date|Current Temperature|temp0|temp1|temp2|Current Humidity|hum0|hum1|hum2|    Current Summary|               sum0|               sum1|               sum2|Current Windspeed|wind0|wind1|wind2|\n+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+-------------------+-------------------+-------------------+-------------------+-----------------+-----+-----+-----+\n|2015-01-04 00:00:00|              37.24| null| null| null|            0.93|null|null|null|partly-cloudy-night|               null|               null|               null|             0.94| null| null| null|\n|2015-01-04 01:00:00|              36.35|37.24| null| null|            0.94|0.93|null|null|partly-cloudy-night|partly-cloudy-night|               null|               null|             0.56| 0.94| null| null|\n|2015-01-04 02:00:00|              35.61|36.35|37.24| null|            0.94|0.94|0.93|null|                fog|partly-cloudy-night|partly-cloudy-night|               null|             0.34| 0.56| 0.94| null|\n|2015-01-04 03:00:00|              34.36|35.61|36.35|37.24|            0.95|0.94|0.94|0.93|        clear-night|                fog|partly-cloudy-night|partly-cloudy-night|             0.05| 0.34| 0.56| 0.94|\n|2015-01-04 04:00:00|              33.43|34.36|35.61|36.35|            0.94|0.95|0.94|0.94|                fog|        clear-night|                fog|partly-cloudy-night|             0.08| 0.05| 0.34| 0.56|\n|2015-01-04 05:00:00|              33.14|33.43|34.36|35.61|            0.94|0.94|0.95|0.94|                fog|                fog|        clear-night|                fog|             0.18| 0.08| 0.05| 0.34|\n|2015-01-04 06:00:00|              32.86|33.14|33.43|34.36|            0.94|0.94|0.94|0.95|                fog|                fog|                fog|        clear-night|             0.28| 0.18| 0.08| 0.05|\n|2015-01-04 07:00:00|              32.98|32.86|33.14|33.43|            0.96|0.94|0.94|0.94|                fog|                fog|                fog|                fog|             0.03| 0.28| 0.18| 0.08|\n|2015-01-04 08:00:00|              33.09|32.98|32.86|33.14|            0.95|0.96|0.94|0.94|                fog|                fog|                fog|                fog|             0.14| 0.03| 0.28| 0.18|\n|2015-01-04 09:00:00|              33.65|33.09|32.98|32.86|            0.94|0.95|0.96|0.94|                fog|                fog|                fog|                fog|             0.34| 0.14| 0.03| 0.28|\n|2015-01-04 10:00:00|              34.03|33.65|33.09|32.98|            0.94|0.94|0.95|0.96|                fog|                fog|                fog|                fog|             1.04| 0.34| 0.14| 0.03|\n|2015-01-04 11:00:00|              34.91|34.03|33.65|33.09|            0.94|0.94|0.94|0.95|                fog|                fog|                fog|                fog|              1.3| 1.04| 0.34| 0.14|\n|2015-01-04 12:00:00|              35.65|34.91|34.03|33.65|            0.94|0.94|0.94|0.94|                fog|                fog|                fog|                fog|             1.02|  1.3| 1.04| 0.34|\n|2015-01-04 13:00:00|              36.23|35.65|34.91|34.03|            0.94|0.94|0.94|0.94|                fog|                fog|                fog|                fog|             1.53| 1.02|  1.3| 1.04|\n|2015-01-04 14:00:00|              36.49|36.23|35.65|34.91|            0.94|0.94|0.94|0.94|  partly-cloudy-day|                fog|                fog|                fog|             1.36| 1.53| 1.02|  1.3|\n|2015-01-04 15:00:00|              36.57|36.49|36.23|35.65|            0.94|0.94|0.94|0.94|  partly-cloudy-day|  partly-cloudy-day|                fog|                fog|              0.8| 1.36| 1.53| 1.02|\n|2015-01-04 16:00:00|              36.18|36.57|36.49|36.23|            0.94|0.94|0.94|0.94|partly-cloudy-night|  partly-cloudy-day|  partly-cloudy-day|                fog|              1.6|  0.8| 1.36| 1.53|\n|2015-01-04 17:00:00|              36.31|36.18|36.57|36.49|            0.94|0.94|0.94|0.94|                fog|partly-cloudy-night|  partly-cloudy-day|  partly-cloudy-day|             0.84|  1.6|  0.8| 1.36|\n|2015-01-04 18:00:00|              36.42|36.31|36.18|36.57|            0.94|0.94|0.94|0.94|                fog|                fog|partly-cloudy-night|  partly-cloudy-day|             0.94| 0.84|  1.6|  0.8|\n|2015-01-04 19:00:00|              36.32|36.42|36.31|36.18|            0.94|0.94|0.94|0.94|                fog|                fog|                fog|partly-cloudy-night|             1.04| 0.94| 0.84|  1.6|\n+-------------------+-------------------+-----+-----+-----+----------------+----+----+----+-------------------+-------------------+-------------------+-------------------+-----------------+-----+-----+-----+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=694"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1559339728262_-1967668579","id":"20190531-235528_1485729994","dateCreated":"2019-05-31T23:55:28+0200","dateStarted":"2019-06-01T16:47:49+0200","dateFinished":"2019-06-01T16:48:04+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8009"},{"text":"%spark2.pyspark\ndf_weather.printSchema()\nreference.printSchema()","user":"admin","dateUpdated":"2019-06-01T16:48:36+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559400421007_-787111632","id":"20190601-164701_235173671","dateCreated":"2019-06-01T16:47:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11596","dateFinished":"2019-06-01T16:48:36+0200","dateStarted":"2019-06-01T16:48:36+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- Start Date: timestamp (nullable = true)\n |-- Current Temperature: double (nullable = true)\n |-- temp0: double (nullable = true)\n |-- temp1: double (nullable = true)\n |-- temp2: double (nullable = true)\n |-- temp3: double (nullable = true)\n |-- temp4: double (nullable = true)\n |-- temp5: double (nullable = true)\n |-- temp6: double (nullable = true)\n |-- temp7: double (nullable = true)\n |-- temp8: double (nullable = true)\n |-- temp9: double (nullable = true)\n |-- temp10: double (nullable = true)\n |-- temp11: double (nullable = true)\n |-- temp12: double (nullable = true)\n |-- temp13: double (nullable = true)\n |-- temp14: double (nullable = true)\n |-- temp15: double (nullable = true)\n |-- temp16: double (nullable = true)\n |-- temp17: double (nullable = true)\n |-- temp18: double (nullable = true)\n |-- temp19: double (nullable = true)\n |-- temp20: double (nullable = true)\n |-- temp21: double (nullable = true)\n |-- temp22: double (nullable = true)\n |-- temp23: double (nullable = true)\n |-- Current Humidity: double (nullable = true)\n |-- hum0: double (nullable = true)\n |-- hum1: double (nullable = true)\n |-- hum2: double (nullable = true)\n |-- hum3: double (nullable = true)\n |-- hum4: double (nullable = true)\n |-- hum5: double (nullable = true)\n |-- hum6: double (nullable = true)\n |-- hum7: double (nullable = true)\n |-- hum8: double (nullable = true)\n |-- hum9: double (nullable = true)\n |-- hum10: double (nullable = true)\n |-- hum11: double (nullable = true)\n |-- hum12: double (nullable = true)\n |-- hum13: double (nullable = true)\n |-- hum14: double (nullable = true)\n |-- hum15: double (nullable = true)\n |-- hum16: double (nullable = true)\n |-- hum17: double (nullable = true)\n |-- hum18: double (nullable = true)\n |-- hum19: double (nullable = true)\n |-- hum20: double (nullable = true)\n |-- hum21: double (nullable = true)\n |-- hum22: double (nullable = true)\n |-- hum23: double (nullable = true)\n |-- Current Summary: string (nullable = true)\n |-- sum0: string (nullable = true)\n |-- sum1: string (nullable = true)\n |-- sum2: string (nullable = true)\n |-- sum3: string (nullable = true)\n |-- sum4: string (nullable = true)\n |-- sum5: string (nullable = true)\n |-- sum6: string (nullable = true)\n |-- sum7: string (nullable = true)\n |-- sum8: string (nullable = true)\n |-- sum9: string (nullable = true)\n |-- sum10: string (nullable = true)\n |-- sum11: string (nullable = true)\n |-- sum12: string (nullable = true)\n |-- sum13: string (nullable = true)\n |-- sum14: string (nullable = true)\n |-- sum15: string (nullable = true)\n |-- sum16: string (nullable = true)\n |-- sum17: string (nullable = true)\n |-- sum18: string (nullable = true)\n |-- sum19: string (nullable = true)\n |-- sum20: string (nullable = true)\n |-- sum21: string (nullable = true)\n |-- sum22: string (nullable = true)\n |-- sum23: string (nullable = true)\n |-- Current Windspeed: double (nullable = true)\n |-- wind0: double (nullable = true)\n |-- wind1: double (nullable = true)\n |-- wind2: double (nullable = true)\n |-- wind3: double (nullable = true)\n |-- wind4: double (nullable = true)\n |-- wind5: double (nullable = true)\n |-- wind6: double (nullable = true)\n |-- wind7: double (nullable = true)\n |-- wind8: double (nullable = true)\n |-- wind9: double (nullable = true)\n |-- wind10: double (nullable = true)\n |-- wind11: double (nullable = true)\n |-- wind12: double (nullable = true)\n |-- wind13: double (nullable = true)\n |-- wind14: double (nullable = true)\n |-- wind15: double (nullable = true)\n |-- wind16: double (nullable = true)\n |-- wind17: double (nullable = true)\n |-- wind18: double (nullable = true)\n |-- wind19: double (nullable = true)\n |-- wind20: double (nullable = true)\n |-- wind21: double (nullable = true)\n |-- wind22: double (nullable = true)\n |-- wind23: double (nullable = true)\n\nroot\n |-- New Date: timestamp (nullable = false)\n |-- StartStation Id: integer (nullable = true)\n |-- Current Usage: long (nullable = true)\n |-- Usage0: long (nullable = true)\n |-- Usage1: long (nullable = true)\n |-- Usage2: long (nullable = true)\n |-- Usage3: long (nullable = true)\n |-- Usage4: long (nullable = true)\n |-- Usage5: long (nullable = true)\n |-- Usage6: long (nullable = true)\n |-- Usage7: long (nullable = true)\n |-- Usage8: long (nullable = true)\n |-- Usage9: long (nullable = true)\n |-- Usage10: long (nullable = true)\n |-- Usage11: long (nullable = true)\n |-- Usage12: long (nullable = true)\n |-- Usage13: long (nullable = true)\n |-- Usage14: long (nullable = true)\n |-- Usage15: long (nullable = true)\n |-- Usage16: long (nullable = true)\n |-- Usage17: long (nullable = true)\n |-- Usage18: long (nullable = true)\n |-- Usage19: long (nullable = true)\n |-- Usage20: long (nullable = true)\n |-- Usage21: long (nullable = true)\n |-- Usage22: long (nullable = true)\n |-- Usage23: long (nullable = true)\n |-- Future0: long (nullable = true)\n |-- Future1: long (nullable = true)\n |-- Future2: long (nullable = true)\n |-- Future3: long (nullable = true)\n |-- Future4: long (nullable = true)\n |-- Future5: long (nullable = true)\n |-- Future6: long (nullable = true)\n |-- Future7: long (nullable = true)\n |-- Future8: long (nullable = true)\n |-- Future9: long (nullable = true)\n |-- Future10: long (nullable = true)\n |-- Future11: long (nullable = true)\n |-- Future12: long (nullable = true)\n |-- Future13: long (nullable = true)\n |-- Future14: long (nullable = true)\n |-- Future15: long (nullable = true)\n |-- Future16: long (nullable = true)\n |-- Future17: long (nullable = true)\n |-- Future18: long (nullable = true)\n |-- Future19: long (nullable = true)\n |-- Future20: long (nullable = true)\n |-- Future21: long (nullable = true)\n |-- Future22: long (nullable = true)\n |-- Future23: long (nullable = true)\n\n"}]}},{"text":"%spark2.pyspark\nfrom pyspark.sql.functions import to_date, col\n\n##Merge prepared usage frame with prepared weather dataframe\n#reference = reference.withColumn('Join Date', func.to_date(func.col(\"New Date\")))\ndf_final = reference.alias('l').join(df_weather.alias('r'), col('l.New Date') == col('r.Start Date'), how='left').select(col('l.*'), col('r.*'))\ndf_final = df_final.drop(*[\"Start Date\"])\ndf_final = df_final.orderBy(\"New Date\")\ndf_final.printSchema()","user":"admin","dateUpdated":"2019-06-01T16:50:57+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559249631058_1551676240","id":"20190530-225351_364238333","dateCreated":"2019-05-30T22:53:51+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8013","dateFinished":"2019-06-01T16:50:57+0200","dateStarted":"2019-06-01T16:50:57+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- New Date: timestamp (nullable = false)\n |-- StartStation Id: integer (nullable = true)\n |-- Current Usage: long (nullable = true)\n |-- Usage0: long (nullable = true)\n |-- Usage1: long (nullable = true)\n |-- Usage2: long (nullable = true)\n |-- Usage3: long (nullable = true)\n |-- Usage4: long (nullable = true)\n |-- Usage5: long (nullable = true)\n |-- Usage6: long (nullable = true)\n |-- Usage7: long (nullable = true)\n |-- Usage8: long (nullable = true)\n |-- Usage9: long (nullable = true)\n |-- Usage10: long (nullable = true)\n |-- Usage11: long (nullable = true)\n |-- Usage12: long (nullable = true)\n |-- Usage13: long (nullable = true)\n |-- Usage14: long (nullable = true)\n |-- Usage15: long (nullable = true)\n |-- Usage16: long (nullable = true)\n |-- Usage17: long (nullable = true)\n |-- Usage18: long (nullable = true)\n |-- Usage19: long (nullable = true)\n |-- Usage20: long (nullable = true)\n |-- Usage21: long (nullable = true)\n |-- Usage22: long (nullable = true)\n |-- Usage23: long (nullable = true)\n |-- Future0: long (nullable = true)\n |-- Future1: long (nullable = true)\n |-- Future2: long (nullable = true)\n |-- Future3: long (nullable = true)\n |-- Future4: long (nullable = true)\n |-- Future5: long (nullable = true)\n |-- Future6: long (nullable = true)\n |-- Future7: long (nullable = true)\n |-- Future8: long (nullable = true)\n |-- Future9: long (nullable = true)\n |-- Future10: long (nullable = true)\n |-- Future11: long (nullable = true)\n |-- Future12: long (nullable = true)\n |-- Future13: long (nullable = true)\n |-- Future14: long (nullable = true)\n |-- Future15: long (nullable = true)\n |-- Future16: long (nullable = true)\n |-- Future17: long (nullable = true)\n |-- Future18: long (nullable = true)\n |-- Future19: long (nullable = true)\n |-- Future20: long (nullable = true)\n |-- Future21: long (nullable = true)\n |-- Future22: long (nullable = true)\n |-- Future23: long (nullable = true)\n |-- Current Temperature: double (nullable = true)\n |-- temp0: double (nullable = true)\n |-- temp1: double (nullable = true)\n |-- temp2: double (nullable = true)\n |-- temp3: double (nullable = true)\n |-- temp4: double (nullable = true)\n |-- temp5: double (nullable = true)\n |-- temp6: double (nullable = true)\n |-- temp7: double (nullable = true)\n |-- temp8: double (nullable = true)\n |-- temp9: double (nullable = true)\n |-- temp10: double (nullable = true)\n |-- temp11: double (nullable = true)\n |-- temp12: double (nullable = true)\n |-- temp13: double (nullable = true)\n |-- temp14: double (nullable = true)\n |-- temp15: double (nullable = true)\n |-- temp16: double (nullable = true)\n |-- temp17: double (nullable = true)\n |-- temp18: double (nullable = true)\n |-- temp19: double (nullable = true)\n |-- temp20: double (nullable = true)\n |-- temp21: double (nullable = true)\n |-- temp22: double (nullable = true)\n |-- temp23: double (nullable = true)\n |-- Current Humidity: double (nullable = true)\n |-- hum0: double (nullable = true)\n |-- hum1: double (nullable = true)\n |-- hum2: double (nullable = true)\n |-- hum3: double (nullable = true)\n |-- hum4: double (nullable = true)\n |-- hum5: double (nullable = true)\n |-- hum6: double (nullable = true)\n |-- hum7: double (nullable = true)\n |-- hum8: double (nullable = true)\n |-- hum9: double (nullable = true)\n |-- hum10: double (nullable = true)\n |-- hum11: double (nullable = true)\n |-- hum12: double (nullable = true)\n |-- hum13: double (nullable = true)\n |-- hum14: double (nullable = true)\n |-- hum15: double (nullable = true)\n |-- hum16: double (nullable = true)\n |-- hum17: double (nullable = true)\n |-- hum18: double (nullable = true)\n |-- hum19: double (nullable = true)\n |-- hum20: double (nullable = true)\n |-- hum21: double (nullable = true)\n |-- hum22: double (nullable = true)\n |-- hum23: double (nullable = true)\n |-- Current Summary: string (nullable = true)\n |-- sum0: string (nullable = true)\n |-- sum1: string (nullable = true)\n |-- sum2: string (nullable = true)\n |-- sum3: string (nullable = true)\n |-- sum4: string (nullable = true)\n |-- sum5: string (nullable = true)\n |-- sum6: string (nullable = true)\n |-- sum7: string (nullable = true)\n |-- sum8: string (nullable = true)\n |-- sum9: string (nullable = true)\n |-- sum10: string (nullable = true)\n |-- sum11: string (nullable = true)\n |-- sum12: string (nullable = true)\n |-- sum13: string (nullable = true)\n |-- sum14: string (nullable = true)\n |-- sum15: string (nullable = true)\n |-- sum16: string (nullable = true)\n |-- sum17: string (nullable = true)\n |-- sum18: string (nullable = true)\n |-- sum19: string (nullable = true)\n |-- sum20: string (nullable = true)\n |-- sum21: string (nullable = true)\n |-- sum22: string (nullable = true)\n |-- sum23: string (nullable = true)\n |-- Current Windspeed: double (nullable = true)\n |-- wind0: double (nullable = true)\n |-- wind1: double (nullable = true)\n |-- wind2: double (nullable = true)\n |-- wind3: double (nullable = true)\n |-- wind4: double (nullable = true)\n |-- wind5: double (nullable = true)\n |-- wind6: double (nullable = true)\n |-- wind7: double (nullable = true)\n |-- wind8: double (nullable = true)\n |-- wind9: double (nullable = true)\n |-- wind10: double (nullable = true)\n |-- wind11: double (nullable = true)\n |-- wind12: double (nullable = true)\n |-- wind13: double (nullable = true)\n |-- wind14: double (nullable = true)\n |-- wind15: double (nullable = true)\n |-- wind16: double (nullable = true)\n |-- wind17: double (nullable = true)\n |-- wind18: double (nullable = true)\n |-- wind19: double (nullable = true)\n |-- wind20: double (nullable = true)\n |-- wind21: double (nullable = true)\n |-- wind22: double (nullable = true)\n |-- wind23: double (nullable = true)\n\n"}]}},{"text":"%spark2.pyspark\n#save back\ndf_final.coalesce(1).write.csv(\"/user/hadoop/preparedDataframe010\", header=True)\n","user":"admin","dateUpdated":"2019-06-01T16:52:04+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559400371895_1123902302","id":"20190601-164611_12466624","dateCreated":"2019-06-01T16:46:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11524","dateFinished":"2019-06-01T16:58:58+0200","dateStarted":"2019-06-01T16:52:04+0200","results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=695","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=696","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=697","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=698","http://i-hadoop-01.informatik.hs-ulm.de:4040/jobs/job?id=699"],"interpreterSettingId":"spark2"}}},{"text":"%md\n#### Aggregation of >24 Hours ####\n> <sup>As an extension, hourly date older than one day should be aggregated.</sup>","user":"admin","dateUpdated":"2019-06-01T16:45:31+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559251634063_-181212485","id":"20190530-232714_1655968322","dateCreated":"2019-05-30T23:27:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8017","dateFinished":"2019-06-01T16:45:31+0200","dateStarted":"2019-06-01T16:45:31+0200","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h4>Aggregation of >24 Hours</h4>\n<blockquote><p><sup>As an extension, hourly date older than one day should be aggregated.</sup></p>\n</blockquote>\n"}]}},{"text":"%md\n","user":"admin","dateUpdated":"2019-06-01T16:44:35+0200","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1559400275738_529434027","id":"20190601-164435_1387575916","dateCreated":"2019-06-01T16:44:35+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11358"}],"name":"DFGeneration","id":"2EE43ZRBW","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}